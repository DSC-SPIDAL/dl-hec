{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_qNSzzyaCbD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors and Geoffrey Fox 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmjh290raIky",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title #### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1fLYj-KBAjF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Describe Dataset components"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json, copy\n",
    "\n",
    "NamesList = [\"RunName\", \"RunComment\", \"LocalRunName\", \"LocalRunComment\",\n",
    "             \"UseLSTMModel\", \"UseScienceTransformerModel\", \"UseTFTModel\", \"UseModel\", \"Directoryaddon\",\n",
    "  \"SymbolicWindows\", \"Tseq\", \"LocationBasedValidation\", \"RestartLocationBasedValidation\", \"LocationValidationFraction\", \n",
    "  \"RestartValidationSetRunName\", \"UseFutures\", \"CustomLoss\", \"Npropperseq\", \"NpropperseqTOT\", \"Npredperseq\", \"NpredperseqTOT\", \n",
    "  \"PropertyNameIndex\", \"PredictionNameIndex\", \"PropertyAverageValuesPointer\",\n",
    "\n",
    "  \"PredictionAverageValuesPointer\", \"Predictionwgt\",\n",
    "\n",
    "  \"InputSource\", \"InputSourceNumber\", \"PredSource\", \"PredSourceNumber\", \"FuturedPred \",\n",
    "\n",
    "  \"SpaceTimeEncodingPropTypes\", \"SpaceTimeEncodingPropValues\", \"SpaceTimeEncodingPredTypes\", \"SpaceTimeEncodingPredValues \",\n",
    "\n",
    "  \"Nloc\", \"ListofTrainingLocs\", \"ListofValidationLocs\", \"MappingtoTraining\", \"MappingtoValidation\", \"TrainingNloc\", \"ValidationNloc\",\n",
    "\n",
    "  \"RawInputSequences\", \"RawInputPredictions\", \"RawInputSequencesTOT\",  \"RawInputPredictionsTOT\",    \n",
    "\n",
    "  \"LSTMepochs\",\n",
    "  \"LSTMvalidationfrac\", \"LSTMbatch_size\", \"LSTMoptimizer\", \"LSTMactivationvalue\", \"LSTMdropout1\", \"LSTMrecurrent_dropout1\", \n",
    "  \"LSTMdropout2\", \"LSTMrecurrent_dropout2\", \"LSTMSkipInitial\", \"number_LSTMnodes\", \"LSTMThirdLayer\", \"LSTMInitialMLP\", \n",
    "  \"LSTMFinalMLP\", \"LSTMverbose\", \"LSTMlearning_rate \", \"LSTMvalidationfrac\", \"UsedLSTMvalidationfrac\",\n",
    "\n",
    "  \"SpacetimeforMask \", \"GlobalSpacetime\",\n",
    "\n",
    "  \"IncreaseNloc_sample\", \"DecreaseNloc_sample\", \"Transformerepochs\",\n",
    "\n",
    "  \"SkipDL2\", \"SkipDL2B\", \"SkipDL2D\", \"SkipDL2E\",\n",
    "\n",
    "  \"SampleSize\", \"PredictionTraining\", \"FullSetValidation\",\n",
    "\n",
    "  \"MaskingOption\", \"ActivateAttention\", \"DoubleQKV\", \"TimeShufflingOnly\", \"Transformerbatch_size\", \n",
    "  \"Transformervalidationfrac\", \"UsedTransformervalidationfrac\", \"Transformeroptimizer\", \"Transformerverbose\", \n",
    "  \"TransformerOnlyFullAttention\", \"SpacewiseSecondAttention\", \"SeparateHeads\", \"d_model\", \"d_Attention\", \n",
    "  \"d_qkl\", \"d_intermediateqk\", \"num_heads\", \"num_Encoderlayers\", \"EncoderDropout\", \"EncoderActivation\", \n",
    "  \"d_EncoderLayer\", \"d_merge\", \"d_ffn\", \"oldencoderversion\", \"ReuseInputinEncoder\", \"UseMappedinput\", \"Takevasinput\",\n",
    "\n",
    "  \"ChopupMatrix\", \"ChopupNumber\",\n",
    "\n",
    "  \"Plotrealnumbers\", \"JournalSimplePrint\", \"UseRealDatesonplots\", \"PlotinDL2F\", \"SkipDL2F\", \"CalculateNNSE\", \"PlotPredictions\", \"SeparateValandTrainingPlots\", \"PlotsOnlyinTestFIPS\", \"ListofTestFIPS\",\n",
    "\n",
    "  \"Dailyunit\", \"StartDate\",\n",
    "\n",
    "  \"Num_Seq\", \"GlobalTimeMask \",\n",
    "\n",
    "  \"AnalysisOnly\", \"Dumpoutkeyplotsaspics\", \"Restorefromcheckpoint\", \"Checkpointfinalstate \", \n",
    "  \"inputRunName\", \"inputCheckpointpostfix\", \"standaloneLSTMrun\", \"ClassLSTMrun\", \"CustomTraining\", \n",
    "  \"SuccessLimit\", \"FailureLimit\"]\n"
   ],
   "metadata": {
    "id": "l4ACgNYdM6G9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "  def SetDictvalues(Dict, UpdateList):\n",
    "    for varname in UpdateList:\n",
    "      if varname in Dict:\n",
    "        exec('Dict[\"' + varname +'\"] =' + varname)\n",
    "        print(varname + \" updated in Dict \" + str(Dict[varname]))\n",
    "      else:\n",
    "        printexit(\"Error as \" + varname + \" not in dictionary\")\n",
    "    \n",
    "  def SetupModelOver():\n",
    "    global UseModel, UseLSTMModel, UseScienceTransformerModel, UseTFTModel\n",
    "    UseLSTMModel = False\n",
    "    UseScienceTransformerModel = False\n",
    "    UseTFTModel = False\n",
    "    if UseModel == 0:\n",
    "      UseLSTMModel = True\n",
    "    elif UseModel == 1:\n",
    "      UseScienceTransformerModel = True\n",
    "    elif UseModel == 2:\n",
    "      UseTFTModel = True"
   ],
   "metadata": {
    "id": "rFQnIaJ7cTYY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EV1qWhrmI1nF",
    "outputId": "0a2906c7-d185-4344-ea59-ff0a774ef68e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your runtime has 54.8 gigabytes of available RAM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "from psutil import virtual_memory\n",
    "\n",
    "# Avoids scroll-in-the-scroll in the entire Notebook\n",
    "def resize_colab_cell():\n",
    "  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 20000})'))\n",
    "get_ipython().events.register('pre_run_cell', resize_colab_cell)\n",
    "\n",
    "# Set Global Parameters for all components\n",
    "# Can set defaults for components in Overalljstring\n",
    "Overalljstring =  '{ \"RunName\" : \"EARTHQ-EMA1LR8\" , \"RunComment\" : \"EARTHQ LSTM and 1 EMA LR*0.1 Earlier Stop\" }'\n",
    "FFFFOverallDict = dict.fromkeys(NamesList)\n",
    "TempDict = json.loads(Overalljstring)\n",
    "FFFFOverallDict.update(TempDict)\n",
    "RunName = FFFFOverallDict[\"RunName\"]\n",
    "RunComment = FFFFOverallDict[\"RunComment\"]\n",
    "FFFFOverallDict[\"LocalRunName\"] = ''\n",
    "FFFFOverallDict[\"LocalRunComment\"] = ''\n",
    "CurrentDataset = FFFFOverallDict\n",
    "NumberofDatasets = 1\n",
    "PrintTitle('Start Job')\n",
    "\n",
    "def PrintTitle(extrawords):\n",
    "  current_time = timenow()\n",
    "\n",
    "  LR = CurrentDataset[\"LocalRunName\"] \n",
    "  if LR != '':\n",
    "    if NumberofDatasets > 1:\n",
    "      LR = str(NumberofDatasets) + ': ' + LR\n",
    "    LR = ' ' + LR\n",
    "  LC = CurrentDataset[\"LocalRunComment\"]\n",
    "  if LC != '':\n",
    "    if NumberofDatasets > 1:\n",
    "      LC = str(NumberofDatasets) + ': ' + LC\n",
    "    LC = ' ' + LC\n",
    "   \n",
    "  line =  CurrentDataset[\"RunName\"] + LR + ' ' + CurrentDataset[\"RunComment\"] + LC\n",
    "  beginwords = ''\n",
    "  if extrawords != '':\n",
    "    beginwords = extrawords + ' '\n",
    "  print(wraptotext(startbold + startred + beginwords + current_time + ' '  + line + resetfonts))\n",
    "  ram_gb = virtual_memory().total / 1e9\n",
    "  print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7vFGEcfNvVS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set Run Name etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWNb05uZ7V9I",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initial System Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "w4cXSlPV7hNG",
    "outputId": "d72285d0-9c4e-44c9-f7c4-b6954273a58c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 20000})"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Architecture:        x86_64\n",
      "CPU op-mode(s):      32-bit, 64-bit\n",
      "Byte Order:          Little Endian\n",
      "CPU(s):              8\n",
      "On-line CPU(s) list: 0-7\n",
      "Thread(s) per core:  2\n",
      "Core(s) per socket:  4\n",
      "Socket(s):           1\n",
      "NUMA node(s):        1\n",
      "Vendor ID:           GenuineIntel\n",
      "CPU family:          6\n",
      "Model:               79\n",
      "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "Stepping:            0\n",
      "CPU MHz:             2199.998\n",
      "BogoMIPS:            4399.99\n",
      "Hypervisor vendor:   KVM\n",
      "Virtualization type: full\n",
      "L1d cache:           32K\n",
      "L1i cache:           32K\n",
      "L2 cache:            256K\n",
      "L3 cache:            56320K\n",
      "NUMA node0 CPU(s):   0-7\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, \n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "startbold = \"\\033[1m\"\n",
    "resetfonts = \"\\033[0m\"\n",
    "startred = '\\033[31m'\n",
    "\n",
    "startpurple = '\\033[35m'\n",
    "startyellowbkg = '\\033[43m'\n",
    "\n",
    "!lscpu\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformer model for science data based on original for language understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpGoE2T-YXS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/transformer\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/transformer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA3Lx2aY1xeg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Science Data Parameters and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMY9LokXwa9K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-------\n",
    "Here is structure of science time series module. We will need several arrays that will need to be flattened at times. Note Python defaults to row major i.e. final index describes contiguous positions in memory\n",
    "\n",
    "\n",
    "At highest level data is labeled by Time and Location\n",
    "\n",
    "*   Ttot is total number of time steps\n",
    "*   Tseq is length of each sequence in time steps\n",
    "*   Num_Seq is number of sequences in time: Num_Seq = Ttot-Tseq + 1\n",
    "*   Nloc is Number of locations. The locations could be a 1D list or have array structure such as an image.\n",
    "*   Nsample is number of data samples Nloc * Num_Seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Input data is at each location\n",
    "*   Nprop time independent properties describing the location\n",
    "*   Nforcing is number of time dependent forcing features INPUT at each time value\n",
    "\n",
    "\n",
    "Output (predicted) data at each location and for each time sequence is\n",
    "*   Npred predicted time dependent values defined at every time step\n",
    "*   Recorded at Nforecast time values measured wrt final time value of sequence\n",
    "*   ForecastDay is an array of length Nforecast defining how many days into future prediction is. Typically ForecastDay[0] = 1 and Nforecast is often 1\n",
    "*   There is also a class of science problems that are more similar to classic Seq2Seq. Here Nforecast = Tseq and ForecastDay = [-Tseq+1 ... 0]\n",
    "*   We also support Nwishful predictions of events in future such probability of an earthquake of magnitude 6 in next 3 years. These are defined by araays EventType and Timestart, TimeInterval of length Nwishful. EventType is user defined and Timestart, TimeInterval is measured in time steps\n",
    "*   Any missing output values should be set to NaN and Loss function must ensure that these points are ignored in derivative calculation and value calculation\n",
    "\n",
    "We have an input module that supports either LSTM or Transformer (multi-head attention) models\n",
    "\n",
    "Example Problem AICov\n",
    "\n",
    "*   Ttot = 114\n",
    "*   Tseq = 9\n",
    "*   Num_Seq = 106\n",
    "*   Nloc = 110\n",
    "\n",
    "\n",
    "*   Nprop = 35\n",
    "*   Nforcing = 5 including infections, fatalities, plus 3 temporal position variables (last 3 not in current version)\n",
    " \n",
    " \n",
    "*   Npred = 2 (predicted infections and fatalities). Could be 5 if predicted temporal position of output)\n",
    "*   Nforecast= 15\n",
    "*   ForecastDay = [1, 2, .......14, 15]\n",
    "*   Nwishful = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UlOJMJ31SoG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Science Data Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdszPs9on5gk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Typical Arrays\n",
    "\n",
    "\n",
    "[ time, Location ] as Pandas array with label [name of time-dependent variable] as an array or just name of Pandas array\n",
    "\n",
    "time labels rows indexed by datetime or the difference datetime - start\n",
    "\n",
    "Non windowed data is stored with propert name as row index and location as column index\n",
    "[ static property, Location]\n",
    "\n",
    "Covid Input is\n",
    "[Sequence number 0..Num_Seq-1 ] [ Location 0..Nloc-1 ] [position in time sequence Tseq]  [ Input Features]\n",
    "\n",
    "Covid Output is \n",
    "[Sequence number Num_Seq ] [ Location Nloc ]  [ Output Features] \n",
    "\n",
    "Output Features are [ ipred = 0 ..Npred-1 ] [ iforecast = 0 ..Nforecast-1 ]\n",
    "\n",
    "Input Features are static fields followed by if present by dynamic system fields (cos-theta sin-theta linear) chosen followed by cases, deaths. In fact this is user chosen as they set static and dynamic system properties to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-iizX9OKmI3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will have various numpy and pandas arrays where we designate label\n",
    "\n",
    "[Ttot] is all time values \n",
    "\n",
    "[Num_Seq]  is all sequences of window size ***Tseq***\n",
    "\n",
    "We can select time values or sequences [Ttot-reason] [Num_Seq-reason] for a given \"reason\"\n",
    "\n",
    "[Num_Seq][Tseq] is all time values in all sequences\n",
    "\n",
    "[Nloc] is all locations while [Nloc-reason] is subset of locations for given \"reason\"\n",
    "\n",
    "[Model1] is initial embedding of each data point\n",
    "\n",
    "[Model1+TrPosEnc] is initial embedding of each data point with Transformer style positional encoding\n",
    "\n",
    "[Nforcing] is time dependent input parameters and [Nprop] static properties while [ExPosEnc] are explicit positional (temporal) encoding.\n",
    "\n",
    "[Nforcing+ExPosEnc+Nprop] are all possible inputs\n",
    "\n",
    "[Npred] is predicted values with [Npred+ExPosEnc] as predictions plus encodings with actually used [Predvals] = [Npred+ExPosEnc-Selout] \n",
    "\n",
    "[Predtimes] = [Forecast time range] are times forecasted with \"time range\" separately defined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "id": "JjJJyJTZYebt",
    "outputId": "b0ac79d7-392b-4a92-8003-b59bd3d51bd6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 20000})"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting cloudmesh-common\n",
      "  Downloading cloudmesh_common-4.3.161-py2.py3-none-any.whl (96 kB)\n",
      "\u001B[K     |████████████████████████████████| 96 kB 1.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (2022.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (2.23.0)\n",
      "Collecting simplejson\n",
      "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
      "\u001B[K     |████████████████████████████████| 130 kB 11.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (2.8.2)\n",
      "Collecting pyfiglet\n",
      "  Downloading pyfiglet-0.8.post1-py2.py3-none-any.whl (865 kB)\n",
      "\u001B[K     |████████████████████████████████| 865 kB 30.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (0.5.1)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (0.8.10)\n",
      "Collecting python-hostlist\n",
      "  Downloading python-hostlist-1.21.tar.gz (35 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (4.64.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from cloudmesh-common) (5.4.8)\n",
      "Collecting oyaml\n",
      "  Downloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from oyaml->cloudmesh-common) (6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->cloudmesh-common) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->cloudmesh-common) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->cloudmesh-common) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->cloudmesh-common) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->cloudmesh-common) (3.0.4)\n",
      "Building wheels for collected packages: python-hostlist\n",
      "  Building wheel for python-hostlist (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for python-hostlist: filename=python_hostlist-1.21-py3-none-any.whl size=38943 sha256=a024df7f8ee821cd77bf8f025f9a63f502277d74ded74cc502d265c570c82e43\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/31/d6/c2ea1dd468ff9d67b94bf63a4fb4590337ac6af531b1d04aae\n",
      "Successfully built python-hostlist\n",
      "Installing collected packages: simplejson, python-hostlist, pyfiglet, oyaml, colorama, cloudmesh-common\n",
      "Successfully installed cloudmesh-common-4.3.161 colorama-0.4.5 oyaml-1.0 pyfiglet-0.8.post1 python-hostlist-1.21 simplejson-3.17.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tqdm import tnrange, notebook, tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from csv import reader\n",
    "from csv import writer\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "import pandas as pd\n",
    "import io as io\n",
    "import string\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta,date,datetime\n",
    "\n",
    "!pip install cloudmesh-common -U\n",
    "from cloudmesh.common.StopWatch import StopWatch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdH4W3OJTLyj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Define Basic Control parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###General Functions for all Components"
   ],
   "metadata": {
    "id": "rUIxsTsTbXkY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def wraptotext(textinput,size=None):\n",
    "  if size is None:\n",
    "    size = 120\n",
    "  textlist = wrap(textinput,size)\n",
    "  textresult = textlist[0]\n",
    "  for itext in range(1,len(textlist)):\n",
    "    textresult += '\\n'+textlist[itext]\n",
    "  return textresult\n",
    "\n",
    "def timenow():\n",
    "  now = datetime.now()\n",
    "  return now.strftime(\"%m/%d/%Y, %H:%M:%S\") + \" UTC\"\n",
    "\n",
    "def float32fromstrwithNaN(instr):\n",
    "  if instr == 'NaN':\n",
    "    return NaN\n",
    "  return np.float32(instr)\n",
    "\n",
    "def printexit(exitmessage):\n",
    "  print(exitmessage)\n",
    "  sys.exit()\n",
    "\n",
    "def strrnd(value):\n",
    "  return str(round(value,4))\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "NaN = np.float32(\"NaN\")\n",
    "PLOTNUMBER =0 # Count Output Plots"
   ],
   "metadata": {
    "id": "qXTANCYZbg32",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "outputId": "f445ad71-dd5e-4c75-dbc0-6b7b13e420a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 20000})"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Set Global Variables to None"
   ],
   "metadata": {
    "id": "CMfCf74YfttE",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ReadMay2022Covid = None\n",
    "Read7dayCovid = None\n",
    "ScaleProperties = None\n",
    "ConvertDynamicPredictedQuantity = None\n",
    "ConvertDynamicProperties = None\n",
    "GenerateFutures = None\n",
    "GenerateSequences = None\n",
    "PredictionsfromInputs = None\n",
    "RereadMay2020 = None\n",
    "UseOLDCovariates = None\n",
    "Dropearlydata = None\n",
    "NIHCovariates = None\n",
    "UseFutures = None\n",
    "Usedaystart = None\n",
    "PopulationNorm = None\n",
    "SymbolicWindows = None\n",
    "Hydrology = None\n",
    "Earthquake = None\n",
    "\n",
    "CDSpecial = None\n",
    "RootCasesDeaths = None\n",
    "NumpredbasicperTime = None\n",
    "NumpredFuturedperTime = None\n",
    "NumTimeSeriesCalculated = None\n",
    "Dailyunit = None\n",
    "TimeIntervalUnitName = None\n",
    "InitialDate = None\n",
    "NumberofTimeunits = None\n",
    "Num_Time = None\n",
    "FinalDate = None\n",
    "GlobalTrainingLoss = None\n",
    "GlobalValidationLoss = None\n",
    "\n",
    "# Type of Testing\n",
    "LocationBasedValidation = None\n",
    "LocationValidationFraction = None\n",
    "LocationTrainingfraction = None\n",
    "RestartLocationBasedValidation = None\n",
    "\n",
    "#Plotting\n",
    "SeparateValandTrainingPlots = None\n",
    "Plotsplitsize = None\n",
    "Plotrealnumbers = None\n",
    "ListofTestFIPS = None\n",
    "PlotsOnlyinTestFIPS = None\n",
    "EarthquakeImagePlots = None\n",
    "AddSpecialstoSummedplots = None\n",
    "UseRealDatesonplots = None\n",
    "Dumpoutkeyplotsaspics = None\n",
    "OutputNetworkPictures = None\n",
    "JournalSimplePrint = None\n",
    "PlotinDL2F = None\n",
    "FONTSIZE = None\n",
    "\n",
    "GarbageCollect = None\n",
    "GarbageCollectionLimit = None"
   ],
   "metadata": {
    "id": "u14S6jjaf4sC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Start New Dataset"
   ],
   "metadata": {
    "id": "nJK0qD5oy2G9",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "hH6D2TmcBE4u",
    "outputId": "b34ca417-7356-417b-a61a-3371b9b625fe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 20000})"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2 3]\n",
      "LocalRunName updated in Dict \n",
      "LocalRunComment updated in Dict \n",
      "UseLSTMModel updated in Dict True\n",
      "UseScienceTransformerModel updated in Dict False\n",
      "UseTFTModel updated in Dict False\n",
      "UseModel updated in Dict 0\n",
      "Directoryaddon updated in Dict \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "ignored",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 22\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "UpdateList = [\"LocalRunName\", \"LocalRunComment\", \"UseLSTMModel\", \"UseScienceTransformerModel\", \"UseTFTModel\", \"UseModel\", \"Directoryaddon\"]\n",
    "CountDatasets = 0\n",
    "\n",
    "ComponentRunsJSON = [\"\"]\n",
    "ComponentRuns =[]\n",
    "NumberofDatasets = len(ComponentRunsJSON)\n",
    "while CountDatasets < NumberofDatasets:\n",
    "  CountDatasets += 1\n",
    "  UseLSTMModel = False\n",
    "  UseScienceTransformerModel = False\n",
    "  UseTFTModel = False\n",
    "  UseModel = 0\n",
    "  SetupModelOver()\n",
    "  Directoryaddon = \"\"\n",
    "  LocalRunName = \"\"\n",
    "  LocalRunComment = \"\"\n",
    "  CurrentDataset = copy.deepcopy(FFFFOverallDict)\n",
    " \n",
    "  SetDictvalues(CurrentDataset, UpdateList)\n",
    "  ComponentRuns.append(CurrentDataset)\n",
    "  if Earthquake:\n",
    "    ReadEarthquakeData()\n",
    "  \n",
    "\n",
    "sys.exit(22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZAL5yNsC_UK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SetupScience():\n",
    "\n",
    "  global ReadJuly2020Covid, ReadAugust2020Covid, ReadJan2021Covid, ReadApril2021Covid, ReadNov2021Covid, ReadMay2022Covid, Read7dayCovid,\n",
    "    ScaleProperties, ConvertDynamicPredictedQuantity, ConvertDynamicProperties, GenerateFutures, GenerateSequences, PredictionsfromInputs,\n",
    "    RereadMay2020, UseOLDCovariates, Dropearlydata, NIHCovariates, UseFutures, Usedaystart, PopulationNorm, SymbolicWindows, Hydrology, Earthquake,\n",
    "    CDSpecial, RootCasesDeaths, NumpredbasicperTime, NumpredFuturedperTime, NumTimeSeriesCalculated, Dailyunit, TimeIntervalUnitName, InitialDate,\n",
    "    NumberofTimeunits, Num_Time, FinalDate, GlobalTrainingLoss, GlobalValidationLoss, LocationBasedValidation, LocationValidationFraction,\n",
    "    LocationTrainingfraction, RestartLocationBasedValidation, SeparateValandTrainingPlots, Plotsplitsize, Plotrealnumbers, ListofTestFIPS,\n",
    "    PlotsOnlyinTestFIPS, EarthquakeImagePlots, AddSpecialstoSummedplots, UseRealDatesonplots, Dumpoutkeyplotsaspics, OutputNetworkPictures,\n",
    "    JournalSimplePrint, PlotinDL2F, FONTSIZE, GarbageCollect, GarbageCollectionLimit\n",
    "    \n",
    "  ReadJuly2020Covid = False\n",
    "  ReadAugust2020Covid = False\n",
    "  ReadJan2021Covid = False\n",
    "  ReadApril2021Covid = False\n",
    "  ReadNov2021Covid = False\n",
    "  ReadMay2022Covid = False\n",
    "  Read7dayCovid = False\n",
    "  ScaleProperties = False\n",
    "  ConvertDynamicPredictedQuantity = False\n",
    "  ConvertDynamicProperties = True\n",
    "  GenerateFutures = False\n",
    "  GenerateSequences = False\n",
    "  PredictionsfromInputs = False\n",
    "  RereadMay2020 = False\n",
    "  UseOLDCovariates = False\n",
    "  Dropearlydata = 0\n",
    "  NIHCovariates = False \n",
    "  UseFutures = True\n",
    "  Usedaystart = False \n",
    "  PopulationNorm = False\n",
    "  SymbolicWindows = False\n",
    "  Hydrology = False\n",
    "  Earthquake = False\n",
    "\n",
    "  CDSpecial = False\n",
    "  RootCasesDeaths = True\n",
    "  NumpredbasicperTime = 2\n",
    "  NumpredFuturedperTime = 2\n",
    "  NumTimeSeriesCalculated = 0\n",
    "  Dailyunit = 1\n",
    "  TimeIntervalUnitName = 'Day'\n",
    "  InitialDate = datetime(2000,1,1)\n",
    "  NumberofTimeunits = 0\n",
    "  Num_Time =0\n",
    "  FinalDate = datetime(2000,1,1)\n",
    "  GlobalTrainingLoss = 0.0\n",
    "  GlobalValidationLoss = 0.0\n",
    "\n",
    "  # Type of Testing\n",
    "  LocationBasedValidation = False\n",
    "  LocationValidationFraction = 0.0\n",
    "  LocationTrainingfraction = 1.0\n",
    "  RestartLocationBasedValidation = False\n",
    "\n",
    "  #Plotting\n",
    "  SeparateValandTrainingPlots = True\n",
    "  Plotsplitsize = -1 # if > 1 split time in plots\n",
    "  Plotrealnumbers = True\n",
    "  ListofTestFIPS = []\n",
    "  PlotsOnlyinTestFIPS = True\n",
    "  EarthquakeImagePlots = False\n",
    "  AddSpecialstoSummedplots = False\n",
    "  UseRealDatesonplots = False\n",
    "  Dumpoutkeyplotsaspics = False\n",
    "  OutputNetworkPictures = False\n",
    "  JournalSimplePrint = False\n",
    "  PlotinDL2F = False\n",
    "  FONTSIZE = 20\n",
    "\n",
    "  GarbageCollect = True\n",
    "  GarbageCollectionLimit = 5000000\n",
    "\n",
    "  PrintTitle('Start Dataset')\n",
    "\n",
    "  SubName = RunName[0:6]\n",
    "  if SubName == 'BEST14' or SubName == 'BEST15' or SubName == 'BEST16':\n",
    "    UseOLDCovariates = False\n",
    "    ReadAugust2020Covid = True\n",
    "    ScaleProperties = True\n",
    "    ConvertDynamicPredictedQuantity = True\n",
    "    GenerateFutures = True\n",
    "    GenerateSequences = True\n",
    "    PredictionsfromInputs = True\n",
    "    NIHCovariates = True\n",
    "    ConvertDynamicProperties = True\n",
    "    Dropearlydata = 37\n",
    "    CDSpecial = True\n",
    "\n",
    "  if SubName == 'CovidA' or SubName == 'CovidN' or SubName == 'CovidM' or SubName == 'Covid7':\n",
    "    UseOLDCovariates = False\n",
    "    ReadApril2021Covid = True\n",
    "    ScaleProperties = True\n",
    "    ConvertDynamicPredictedQuantity = True\n",
    "    GenerateFutures = True\n",
    "    UseFutures = True\n",
    "    GenerateSequences = True\n",
    "    PredictionsfromInputs = True\n",
    "    NIHCovariates = True\n",
    "    ConvertDynamicProperties = True\n",
    "    CDSpecial = True\n",
    "    if SubName == 'CovidN':\n",
    "      ReadNov2021Covid = True\n",
    "    if SubName == 'CovidM':\n",
    "      ReadMay2022Covid = True\n",
    "    if SubName == 'Covid7':\n",
    "      ReadMay2022Covid = True\n",
    "      Read7dayCovid = True\n",
    "\n",
    "  if SubName == 'C2021A' or SubName == 'C2021B':\n",
    "    UseOLDCovariates = False\n",
    "    ReadJan2021Covid = True\n",
    "    ScaleProperties = True\n",
    "    ConvertDynamicPredictedQuantity = True\n",
    "    GenerateFutures = True\n",
    "    GenerateSequences = True\n",
    "    PredictionsfromInputs = True\n",
    "    NIHCovariates = True\n",
    "    ConvertDynamicProperties = True\n",
    "    Dropearlydata = 0\n",
    "    CDSpecial = True\n",
    "\n",
    "  if SubName == 'Hydrol':\n",
    "    Hydrology = True\n",
    "\n",
    "  if SubName == 'EARTHQ':\n",
    "    Earthquake = True\n",
    "\n",
    "  if RunName == 'BEST10' or RunName == 'BEST13-10D' or RunName == 'BEST12-10' or RunName == 'BEST12-Test' or RunName == 'BEST13' or RunName == 'BEST13-10' or RunName == 'BEST13-10A' or RunName == 'BEST13-10C':\n",
    "    UseOLDCovariates = False\n",
    "    ReadAugust2020Covid = True\n",
    "    ScaleProperties = True\n",
    "    ConvertDynamicPredictedQuantity = True\n",
    "    GenerateFutures = True\n",
    "    GenerateSequences = True\n",
    "    PredictionsfromInputs = True\n",
    "    CDSpecial = True\n",
    "\n",
    "  if RunName == 'BEST11' or RunName == 'BEST11A':\n",
    "    UseOLDCovariates = True\n",
    "    ReadAugust2020Covid = True\n",
    "    ScaleProperties = True\n",
    "    ConvertDynamicPredictedQuantity = True\n",
    "    GenerateFutures = True\n",
    "    GenerateSequences = True\n",
    "    PredictionsfromInputs = True\n",
    "    CDSpecial = True\n",
    "\n",
    "  if RunName == 'BEST12':\n",
    "    UseOLDCovariates = True\n",
    "    RereadMay2020 = True\n",
    "    ReadAugust2020Covid = False\n",
    "    ScaleProperties = True\n",
    "    ConvertDynamicPredictedQuantity = True\n",
    "    GenerateFutures = True\n",
    "    GenerateSequences = True\n",
    "    PredictionsfromInputs = True\n",
    "    CDSpecial = True\n",
    "\n",
    "  if RunName == 'BEST8' or RunName == 'BEST8A' or RunName == 'BEST12-LSTM-8':\n",
    "    ReadJuly2020Covid = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DefDaYecDhIM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define input structure\n",
    "\n",
    "Read in data and set it up for Tensorflow with training and validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kj1DvDTneDZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set train_examples, val_examples as science training and validatioon set.\n",
    "\n",
    "The shuffling of Science Data needs some care. We have ***Tseq*** * size of {[Num_Seq][Nloc]} locations in each sample. In simplease case the last is just a decomposition over location; not over time. Let's Nloc-sel be number of locations per sample. It will be helpful if Nloc-sel is divisable by 2. \n",
    "\n",
    "Perhaps Nloc-sel = 2 6 or 10 is reasonable.\n",
    "\n",
    "Then you shuffle locations every epoch and divide them into groups of size Nloc-sel with 50% overlap so you get locations\n",
    "\n",
    "0 1 2 3 4 5; \n",
    "\n",
    "3 4 5 6 7 8; \n",
    "\n",
    "6 7 8 9 10 11 etc.\n",
    "\n",
    "Every locations appears twice in an epoch (for each time value). You need to randomly add locations at end of sequence so it is divisiuble by Nloc-sel e.g add 4 random positions to the end if Nloc=110 and Nloc-sel = 6. Note last group of 6 has members 112 113 114 0 1 2\n",
    "\n",
    "After spatial structure set up, randomly shuffle in Num_Seq where there is an argument to do all locations for a partcular time value together.\n",
    "\n",
    "For validation, it is probably best to select validation location before chopping them into groups of size Nloc-sel\n",
    "\n",
    "How one groups locations for inference is not clear. One idea is to take trained network and use it to find for each location which other locations have the most attention with it. Use those locations in  prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKm_MgRMdcTT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More general input. \n",
    "NaN allowed value\n",
    "\n",
    "* Number time values\n",
    "* Number locations\n",
    "* Number driving values\n",
    "* Number predicted values\n",
    "\n",
    "For COVID driving same as predicted\n",
    "\n",
    "* a) Clean up >=0 daily\n",
    "* b) Normalize\n",
    "* c) Add Futures\n",
    "* d) Add time/location encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KJIxYoMDZOu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup File Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShbYhXJbKCDT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in science data \n",
    "COLABROOTDIR=\"/content/gdrive/My Drive/Colab Datasets\"\n",
    "os.environ[\"COLABROOTDIR\"] = COLABROOTDIR\n",
    "\n",
    "if Hydrology:\n",
    "  APPLDIR=os.path.join(COLABROOTDIR, \"Hydrology\")\n",
    "elif Earthquake:\n",
    "  APPLDIR=os.path.join(COLABROOTDIR, \"EarthquakeDec2020\")\n",
    "else:\n",
    "  APPLDIR=os.path.join(COLABROOTDIR, \"COVIDJuly2020\")\n",
    "\n",
    "# Set up Checkpoints\n",
    "CHECKPOINTDIR = APPLDIR + \"/checkpoints/\" + RunName + \"dir/\"\n",
    "\n",
    "if Directoryaddon != \"\"):\n",
    "  APPLDIR += \"/\" + Directoryaddon\n",
    "  \n",
    "try: \n",
    "    if not os.path.exists(CHECKPOINTDIR):\n",
    "      os.mkdir(CHECKPOINTDIR) \n",
    "except OSError as error: \n",
    "    print(error)\n",
    "print('Checkpoint set up in directory ' + CHECKPOINTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##General Routines"
   ],
   "metadata": {
    "id": "IY0DkrSZcKDW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def mysavefig(label):\n",
    "  global PLOTNUMBER\n",
    "  if label == \"\":\n",
    "    label = RunName+str(PLOTNUMBER)\n",
    "    PLOTNUMBER+=1\n",
    "  print(\"SavedPlot \"+ label)\n",
    "  plt.savefig(APPLDIR +'/Outputs/' + label + '.pdf',format='pdf',dpi=300)\n",
    "  plt.savefig(APPLDIR +'/Outputs/' + label + '.png',format='png',dpi=300)\n",
    "  return\n",
    "\n",
    "def makeadateplot(plotfigure,plotpointer, Dateaxis=None, datemin=None, datemax=None, Yearly=True, majoraxis = 5):\n",
    "  if not Yearly:\n",
    "    sys.exit('Only yearly supported')\n",
    "  plt.rcParams.update({'font.size': 9})\n",
    "  years5 = mdates.YearLocator(majoraxis)   # every 5 years\n",
    "  years_fmt = mdates.DateFormatter('%Y')\n",
    "  plotpointer.xaxis.set_major_locator(years5)\n",
    "  plotpointer.xaxis.set_major_formatter(years_fmt)\n",
    "  if datemin is None:\n",
    "    datemin = np.datetime64(Dateaxis[0], 'Y')\n",
    "  if datemax is None:\n",
    "    datemax = np.datetime64(Dateaxis[-1], 'Y') + np.timedelta64(1, 'Y')\n",
    "  plotpointer.set_xlim(datemin, datemax)\n",
    "  plotfigure.autofmt_xdate()\n",
    "  return datemin, datemax\n",
    "\n",
    "def makeasmalldateplot(figure,ax, Dateaxis):\n",
    "  plt.rcParams.update({'font.size': 9})\n",
    "  months = mdates.MonthLocator(interval=2)   # every month\n",
    "  datemin = np.datetime64(Dateaxis[0], 'M')\n",
    "  datemax = np.datetime64(Dateaxis[-1], 'M') + np.timedelta64(1, 'M')\n",
    "  ax.set_xlim(datemin, datemax)\n",
    "  \n",
    "  months_fmt = mdates.DateFormatter('%y-%b')\n",
    "  locator = mdates.AutoDateLocator()\n",
    "  locator.intervald['MONTHLY'] = [2]\n",
    "  formatter = mdates.ConciseDateFormatter(locator)\n",
    "#  ax.xaxis.set_major_locator(locator)\n",
    "#  ax.xaxis.set_major_formatter(formatter)\n",
    "  ax.xaxis.set_major_locator(months)\n",
    "  ax.xaxis.set_major_formatter(months_fmt)\n",
    "\n",
    "  figure.autofmt_xdate()\n",
    "  return datemin, datemax\n",
    "\n"
   ],
   "metadata": {
    "id": "4SMteltxeliW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Earthquake Routines"
   ],
   "metadata": {
    "id": "sccd4Ax2b1w0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def printeq():\n",
    "    qsort = np.argsort(Specialdate)\n",
    "    for jquake in range(0,numberspecialeqs):\n",
    "      iquake = qsort[jquake]\n",
    "      print(str(iquake) + ' ' +str(Specialdate[iquake]) + ' '  +  str(round(Specialmags[iquake],1)) + ' ' + Specialeqname[iquake])\n",
    "\n",
    "\n",
    "def Addfixedearthquakes(plotpointer,graphmin, graphmax, ylogscale = False, quakecolor = None, Dateplot = True, vetoquake = None):\n",
    "  if vetoquake is None: # Vetoquake = True means do not plot this quake\n",
    "    vetoquake = np.full(numberspecialeqs, False, dtype = np.bool)\n",
    "  if quakecolor is None: # Color of plot\n",
    "    quakecolor = 'black'\n",
    "  Place =np.arange(numberspecialeqs, dtype =np.int)\n",
    "  Place[8] = 11\n",
    "  Place[10] = 3\n",
    "  Place[12] = 16\n",
    "  Place[7] = 4\n",
    "  Place[2] = 5\n",
    "  Place[4] = 14\n",
    "  Place[11] = 18\n",
    "\n",
    "  ymin, ymax = plotpointer.get_ylim() # Or work with transform=ax.transAxes\n",
    "  qindex =1\n",
    "  fudge = 0.03\n",
    "  qsort = np.argsort(Specialdate)\n",
    "  for jquake in range(0,numberspecialeqs):\n",
    "    iquake = qsort[jquake]\n",
    "\n",
    "    if qindex == 16:\n",
    "      continue\n",
    "# This is the x position for the vertical line\n",
    "    if Dateplot:\n",
    "      x_line_annotation = Specialdate[iquake] # numpy date format\n",
    "    else:\n",
    "      x_line_annotation = Numericaldate[iquake] # Float where each interval 1 and start is 0\n",
    "\n",
    "\n",
    "    # This is the x position for the label\n",
    "    if Dateplot:\n",
    "      x_text_annotation = x_line_annotation - np.timedelta64(2*Dailyunit,'D')\n",
    "    else:\n",
    "      x_text_annotation = x_line_annotation -2.0\n",
    "\n",
    "    if Specialuse[iquake]:\n",
    "\n",
    "      # Draw a text \n",
    "      ascii = str(round(Specialmags[iquake],1)) + '\\n' + Specialeqname[iquake]\n",
    "      ascii = 'EQ' +str(qindex)\n",
    "      qindex += 1\n",
    "      fudge =0.09-fudge\n",
    "\n",
    "      if (x_line_annotation < graphmin) or (x_line_annotation > graphmax):\n",
    "        continue\n",
    "\n",
    "      if vetoquake[iquake]:\n",
    "        continue\n",
    "      \n",
    "      # Draw a line at the position\n",
    "      ydelta = 0.15\n",
    "      plotpointer.axvline(x=x_line_annotation, linestyle='solid', alpha=1.0, linewidth = 2.0, color='black', ymin = 1.0-ydelta)\n",
    "\n",
    "      acfudge = fudge      \n",
    "      if qindex == 13:\n",
    "        acfudge = 0.09\n",
    "      if ylogscale:\n",
    "        yminl = max(0.01*ymax,ymin)\n",
    "        yminl = math.log(yminl,10)\n",
    "        ymaxl = math.log(ymax,10)\n",
    "        logyplot = yminl + (0.1 + 0.8*(float(Place[iquake])/float(numberspecialeqs-1)))*(ymaxl-yminl)\n",
    "        yplot = pow(10, logyplot)\n",
    "        yplot = ymax - (ydelta + acfudge)*(ymax-ymin)\n",
    "      else:\n",
    "        yplot = ymax - (0.1 + 0.8*(float(Place[iquake])/float(numberspecialeqs-1)))*(ymax-ymin)\n",
    "        yplot = ymax - (ydelta + acfudge)*(ymax-ymin)\n",
    "      if Dateplot:\n",
    "        if x_text_annotation > graphmax - np.timedelta64(1200, 'D'):\n",
    "          x_text_annotation = graphmax - np.timedelta64(1200, 'D')\n",
    "        x_text_annotation = max(x_text_annotation, graphmin + np.timedelta64(400, 'D') )\n",
    "      else:\n",
    "        if x_text_annotation > graphmax - 60:\n",
    "          x_text_annotation = graphmax - 60\n",
    "        x_text_annotation = max(x_text_annotation, graphmin+20)\n",
    "#      print(str(yplot) + \" \" + str(ymin) + \" \" + str(ymax) + \" \" + str(x_text_annotation) + \" \" + str(x_line_annotation)) + \" \" + ascii\n",
    "#      print(str(qindex-1) + ' ' + str(iquake) + ' ' +str(Specialdate[iquake]) + ' ' + str(x_line_annotation) + ' ' + str(x_text_annotation) + ' ' + ascii + ' ' +  str(round(Specialmags[iquake],1)) + ' ' + Specialeqname[iquake])\n",
    "      plotpointer.text(x=x_text_annotation, y=yplot, s=wraptotext(ascii,size=10), alpha=1.0, color='black', fontsize = 10)\n",
    "\n",
    "\n",
    "\n",
    "def quakesearch(iquake, iloc):\n",
    "# see if top earthquake iquake llies near location iloc\n",
    "# result = 0 NO; =1 YES Primary: locations match exactly; = -1 Secondary: locations near\n",
    "# iloc is location before mapping\n",
    "  xloc = iloc%60\n",
    "  yloc = (iloc - xloc)/60\n",
    "  if (xloc == Specialxpos[iquake]) and (yloc == Specialypos[iquake]):\n",
    "    return 1\n",
    "  if (abs(xloc - Specialxpos[iquake]) <= 1) and (abs(yloc - Specialypos[iquake]) <= 1):\n",
    "    return -1\n",
    "  return 0\n",
    "\n",
    "# Read Earthquake Data\n",
    "def log_sum_exp10(ns, sumaxis =0):  \n",
    "    max_v = np.max(ns, axis=None)\n",
    "    ds = ns - max_v\n",
    "    sum_of_exp = np.power(10, ds).sum(axis=sumaxis)\n",
    "    return max_v + np.log10(sum_of_exp)\n",
    "\n",
    "def log_energyweightedsum(nvalue, ns, sumaxis = 0): \n",
    "    max_v = np.max(ns, axis=None)\n",
    "    ds = ns - max_v\n",
    "    ds = np.power(10, 1.5*ds)\n",
    "    dvalue = (np.multiply(nvalue,ds)).sum(axis=sumaxis)\n",
    "    ds  = ds.sum(axis=0)\n",
    "    return np.divide(dvalue,ds)\n",
    "\n",
    "# Set summed magnitude as log summed energy = 10^(1.5 magnitude)\n",
    "def log_energy(mag, sumaxis =0):\n",
    "    return log_sum_exp10(1.5 * mag, sumaxis = sumaxis) / 1.5\n",
    "    \n",
    "def AggregateEarthquakes(itime, DaysDelay, DaysinInterval, Nloc, Eqdata, Approach, weighting = None):\n",
    "  if (itime + DaysinInterval + DaysDelay) > NumberofTimeunits:\n",
    "    return np.full([Nloc],NaN,dtype = np.float32)\n",
    "  if Approach == 0: # Magnitudes\n",
    "    if MagnitudeMethod == 0:\n",
    "      TotalMagnitude = log_energy(Eqdata[itime +DaysDelay:itime+DaysinInterval+DaysDelay])\n",
    "    else:\n",
    "      TotalMagnitude = Eqdata[itime +DaysDelay:itime+DaysinInterval+DaysDelay,:].sum(axis=0)\n",
    "    return TotalMagnitude\n",
    "  if Approach == 1: # Depth -- energy weighted\n",
    "    WeightedResult = log_energyweightedsum(Eqdata[itime +DaysDelay:itime+DaysinInterval+DaysDelay],\n",
    "                                        weighting[itime +DaysDelay:itime+DaysinInterval+DaysDelay])\n",
    "    return WeightedResult\n",
    "  if Approach == 2: # Multiplicity -- summed\n",
    "    SimpleSum = Eqdata[itime +DaysDelay:itime+DaysinInterval+DaysDelay,:].sum(axis=0)\n",
    "    return SimpleSum\n",
    "\n",
    "# MagnitudeMethodTransform = 0 No Transform\n",
    "# MagnitudeMethodTransform = 1 E^0.25\n",
    "# MagnitudeMethodTransform = 2 E^0.5\n",
    "def TransformMagnitude(mag):\n",
    "  if MagnitudeMethod == 0:\n",
    "    return mag\n",
    "  if MagnitudeMethod == 1:\n",
    "    return np.power(10, 0.375*(mag-3.29))\n",
    "  return np.power(10, 0.75*(mag-3.29))\n",
    "\n",
    "# Change Daily Unit\n",
    "# Accumulate data in Dailyunit chunks. \n",
    "# This changes data so it looks like daily data bu really collections of chunked data.\n",
    "# For earthquakes, the aggregations uses energy averaging for depth and magnitude. It just adds for multiplicity\n",
    "def GatherUpData(OldInputTimeSeries):\n",
    "    Skipped = NumberofTimeunits%Dailyunit\n",
    "    NewInitialDate = InitialDate + timedelta(days=Skipped)\n",
    "    NewNum_Time = int(Num_Time/Dailyunit)\n",
    "    NewFinalDate = NewInitialDate + Dailyunit * timedelta(days=NewNum_Time-1)\n",
    "    print(' Daily Unit ' +str(Dailyunit) + ' number of ' + TimeIntervalUnitName + ' Units ' + str(NewNum_Time)+ ' ' + \n",
    "         NewInitialDate.strftime(\"%d/%m/%Y\") + ' To ' + NewFinalDate.strftime(\"%d/%m/%Y\"))\n",
    "    NewInputTimeSeries = np.empty([NewNum_Time,Nloc,NpropperTimeDynamicInput],dtype = np.float32)\n",
    "    for itime in range(0,NewNum_Time):\n",
    "      NewInputTimeSeries[itime,:,0] = AggregateEarthquakes(Skipped + itime*Dailyunit,0,Dailyunit, Nloc, \n",
    "                                                           BasicInputTimeSeries[:,:,0], 0)\n",
    "      NewInputTimeSeries[itime,:,1] = AggregateEarthquakes(Skipped + itime*Dailyunit,0,Dailyunit, Nloc, \n",
    "                                                           BasicInputTimeSeries[:,:,1], 1, \n",
    "                                                           weighting = BasicInputTimeSeries[:,:,0])\n",
    "      NewInputTimeSeries[itime,:,2] = AggregateEarthquakes(Skipped + itime*Dailyunit,0,Dailyunit, Nloc, \n",
    "                                                           BasicInputTimeSeries[:,:,2], 2)\n",
    "      NewInputTimeSeries[itime,:,3] = AggregateEarthquakes(Skipped + itime*Dailyunit,0,Dailyunit, Nloc, \n",
    "                                                           BasicInputTimeSeries[:,:,3], 2)\n",
    "    return NewInputTimeSeries, NewNum_Time, NewNum_Time, NewInitialDate, NewFinalDate\n",
    "\n",
    "# make numpy array not a list\n",
    "# Return Exponential Moving Average\n",
    "def MakeEMA(TimeSeries, Nsteps):\n",
    "  datasize = len(TimeSeries)\n",
    "  EMASeries = np.empty(datasize,dtype = np.float32)\n",
    "  WeightedSum = 0.0\n",
    "  WeightedCount = 0.0\n",
    "  beta = (Nsteps - 1.0)/(Nsteps + 1.0)\n",
    "  for i in range(0,datasize):\n",
    "    WeightedSum = TimeSeries[i] + beta*WeightedSum\n",
    "    WeightedCount = 1.0 + beta*WeightedCount\n",
    "    EMASeries[i] = WeightedSum/WeightedCount\n",
    "  return EMASeries\n",
    "\n",
    "# Insist on a minimum count at each time value and then find Exponential Moving Average\n",
    "def MakeEMAMinCT(TimeSeries, Nsteps, Lambda):\n",
    "  Mincount = np.mean(TimeSeries) * Lambda\n",
    "  Temporaryarray = np.maximum(TimeSeries, Mincount)\n",
    "  EMASeries = MakeEMA(Temporaryarray, Nsteps)\n",
    "  return EMASeries"
   ],
   "metadata": {
    "id": "Dk_Y2peDcV-c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX4_pGSonAyz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Space Filling Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-FrNiAY0ALF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "outputId": "db21a444-4655-45f5-f33f-178dbe50ef12",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-f94d0e920d66>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0mcolor_map\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"rainbow\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m     \u001B[0mfigsize\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTuple\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 83\u001B[0;31m     \u001B[0mlinewidth\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     84\u001B[0m ) -> None:\n\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Optional, List, Union, Callable\n",
    "import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "# import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.path import Path\n",
    "\n",
    "def cal_gilbert2d(width: int, height: int) -> List[Tuple[int, int]]:\n",
    "    coordinates: List[Tuple[int, int]] = []\n",
    "\n",
    "    def sgn(x: int) -> int:\n",
    "        return (x > 0) - (x < 0)\n",
    "\n",
    "    def gilbert2d(x: int, y: int, ax: int, ay: int, bx: int, by: int):\n",
    "        \"\"\"\n",
    "        Generalized Hilbert ('gilbert') space-filling curve for arbitrary-sized\n",
    "        2D rectangular grids.\n",
    "        \"\"\"\n",
    "\n",
    "        w = abs(ax + ay)\n",
    "        h = abs(bx + by)\n",
    "\n",
    "        (dax, day) = (sgn(ax), sgn(ay))  # unit major direction\n",
    "        (dbx, dby) = (sgn(bx), sgn(by))  # unit orthogonal direction\n",
    "\n",
    "        if h == 1:\n",
    "            # trivial row fill\n",
    "            for i in range(0, w):\n",
    "                coordinates.append((x, y))\n",
    "                (x, y) = (x + dax, y + day)\n",
    "            return\n",
    "\n",
    "        if w == 1:\n",
    "            # trivial column fill\n",
    "            for i in range(0, h):\n",
    "                coordinates.append((x, y))\n",
    "                (x, y) = (x + dbx, y + dby)\n",
    "            return\n",
    "\n",
    "        (ax2, ay2) = (ax // 2, ay // 2)\n",
    "        (bx2, by2) = (bx // 2, by // 2)\n",
    "\n",
    "        w2 = abs(ax2 + ay2)\n",
    "        h2 = abs(bx2 + by2)\n",
    "\n",
    "        if 2 * w > 3 * h:\n",
    "            if (w2 % 2) and (w > 2):\n",
    "                # prefer even steps\n",
    "                (ax2, ay2) = (ax2 + dax, ay2 + day)\n",
    "\n",
    "            # long case: split in two parts only\n",
    "            gilbert2d(x, y, ax2, ay2, bx, by)\n",
    "            gilbert2d(x + ax2, y + ay2, ax - ax2, ay - ay2, bx, by)\n",
    "\n",
    "        else:\n",
    "            if (h2 % 2) and (h > 2):\n",
    "                # prefer even steps\n",
    "                (bx2, by2) = (bx2 + dbx, by2 + dby)\n",
    "\n",
    "            # standard case: one step up, one long horizontal, one step down\n",
    "            gilbert2d(x, y, bx2, by2, ax2, ay2)\n",
    "            gilbert2d(x + bx2, y + by2, ax, ay, bx - bx2, by - by2)\n",
    "            gilbert2d(x + (ax - dax) + (bx2 - dbx), y + (ay - day) + (by2 - dby), -bx2, -by2, -(ax - ax2), -(ay - ay2))\n",
    "\n",
    "    if width >= height:\n",
    "        gilbert2d(0, 0, width, 0, 0, height)\n",
    "    else:\n",
    "        gilbert2d(0, 0, 0, height, width, 0)\n",
    "    return coordinates\n",
    "\n",
    "def lookup_color(unique_colors, color_value: float) -> int:\n",
    "      ids = np.where(unique_colors == color_value)\n",
    "      color_id = ids[0][0]\n",
    "      return color_id\n",
    "\n",
    "def plot_gilbert2d_space_filling(\n",
    "    vertices: List[Tuple[int, int]],\n",
    "    width: int,\n",
    "    height: int,\n",
    "    filling_color: Optional[np.ndarray] = None,\n",
    "    color_map: str = \"rainbow\",\n",
    "    figsize: Tuple[int, int] = (12, 8),\n",
    "    linewidth: int = 1,\n",
    ") -> None:\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    patch_list: List = []\n",
    "\n",
    "    if filling_color is None:\n",
    "        cmap = matplotlib.cm.get_cmap(color_map, len(vertices))\n",
    "        for i in range(len(vertices) - 1):\n",
    "            path = Path([vertices[i], vertices[i + 1]], [Path.MOVETO, Path.LINETO])\n",
    "            patch = patches.PathPatch(path, fill=False, edgecolor=cmap(i), lw=linewidth)\n",
    "            patch_list.append(patch)\n",
    "        ax.set_xlim(-1, width)\n",
    "        ax.set_ylim(-1, height)\n",
    "\n",
    "    else:\n",
    "        unique_colors = np.unique(filling_color)\n",
    "#        np.random.shuffle(unique_colors)\n",
    "        cmap = matplotlib.cm.get_cmap(color_map, len(unique_colors))\n",
    "\n",
    "        for i in range(len(vertices) - 1):\n",
    "            x, y = vertices[i]\n",
    "            fi, fj = x, height - 1 - y\n",
    "            color_value = filling_color[fj, fi]\n",
    "            color_id = lookup_color(unique_colors, color_value)\n",
    "            path = Path(\n",
    "                [rescale_xy(x, y), rescale_xy(vertices[i + 1][0], vertices[i + 1][1])], [Path.MOVETO, Path.LINETO]\n",
    "            )\n",
    "            # path = Path([vertices[i], vertices[i + 1]], [Path.MOVETO, Path.LINETO])\n",
    "            patch = patches.PathPatch(path, fill=False, edgecolor=cmap(color_id), lw=linewidth)\n",
    "            patch_list.append(patch)\n",
    "        ax.set_xlim(-120 - 0.1, width / 10 - 120)\n",
    "        ax.set_ylim(32 - 0.1, height / 10 + 32)\n",
    "\n",
    "    collection = matplotlib.collections.PatchCollection(patch_list, match_original=True)\n",
    "    # collection.set_array()\n",
    "    # plt.colorbar(collection)\n",
    "    ax.add_collection(collection)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "def rescale_xy(x: int, y: int) -> Tuple[float, float]:\n",
    "    return x / 10 - 120, y / 10 + 32\n",
    "\n",
    "def remapfaults(InputFaultNumbers, Numxlocations, Numylocations, SpaceFillingCurve):\n",
    "  TotalLocations = Numxlocations*Numylocations\n",
    "  OutputFaultNumbers = np.full_like(InputFaultNumbers, -1, dtype=np.int)\n",
    "  MaxOldNumber = np.amax(InputFaultNumbers)\n",
    "  mapping = np.full(MaxOldNumber+1, -1,dtype=np.int)\n",
    "  newlabel=-1\n",
    "  for sfloc in range(0, TotalLocations):\n",
    "    [x,y] = SpaceFillingCurve[sfloc]\n",
    "    pixellocation = y*Numxlocations + x\n",
    "    pixellocation1 = y*Numxlocations + x\n",
    "    oldfaultnumber = InputFaultNumbers[pixellocation1]\n",
    "    if mapping[oldfaultnumber] < 0:\n",
    "      newlabel += 1\n",
    "      mapping[oldfaultnumber] = newlabel\n",
    "    OutputFaultNumbers[pixellocation] = mapping[oldfaultnumber]\n",
    "  MinNewNumber = np.amin(OutputFaultNumbers)\n",
    "  if MinNewNumber < 0:\n",
    "    printexit('Incorrect Fault Mapping')\n",
    "  print('new Fault Labels generated 0 through ' + str(newlabel))\n",
    "  plot_gilbert2d_space_filling(SpaceFillingCurve,Numxlocations, Numylocations, filling_color = np.reshape(OutputFaultNumbers,(40,60)), color_map=\"gist_ncar\")\n",
    "  return OutputFaultNumbers\n",
    "\n",
    "def annotate_faults_ndarray(pix_faults: np.ndarray, figsize=(10, 8), color_map=\"rainbow\"):\n",
    "    matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "    plt.rcParams.update({\"font.size\": 12})\n",
    "    unique_colors = np.unique(pix_faults)\n",
    "    np.random.shuffle(unique_colors)\n",
    "    cmap = matplotlib.cm.get_cmap(color_map, len(unique_colors))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    height, width = pix_faults.shape\n",
    "    for j in range(height):\n",
    "        for i in range(width):\n",
    "            x, y = i / 10 - 120, (height - j - 1) / 10 + 32\n",
    "            ax.annotate(str(pix_faults[j, i]), (x + 0.05, y + 0.05), ha=\"center\", va=\"center\")\n",
    "            color_id = lookup_color(unique_colors, pix_faults[j, i])\n",
    "            ax.add_patch(patches.Rectangle((x, y), 0.1, 0.1, color=cmap(color_id), alpha=0.5))\n",
    "    ax.set_xlim(-120, width / 10 - 120)\n",
    "    ax.set_ylim(32, height / 10 + 32)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpGXkJ04G865",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Read Earthquake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WchUjkEnMxD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ReadEarthquakeData():\n",
    "  read1950 = True\n",
    "  Eigenvectors = 2\n",
    "  UseEarthquakeEigenSystems = False\n",
    "  Dailyunit = 14\n",
    "  addwobblingposition = False\n",
    "  !ls /content/gdrive/'My Drive'/'Colab Datasets'/EarthquakeDec2020\n",
    "  if read1950:\n",
    "    MagnitudeDataFile = APPLDIR + '/1950start/SC_1950-2019.freq-D-25567x2400-log_eng.multi.csv'\n",
    "    DepthDataFile = APPLDIR + '/1950start/SC_1950-2019.freq-D-25567x2400-w_depth.multi.csv'\n",
    "    MultiplicityDataFile = APPLDIR + '/1950start/SC_1950-2019.freq-D-25567x2400-n_shock.multi.csv'\n",
    "    RundleMultiplicityDataFile = APPLDIR + '/1950start/SC_1950-2019.freq-D-25567x2400-n_shock-mag-3.29.multi.csv'\n",
    "    NumberofTimeunits = 25567\n",
    "    InitialDate = datetime(1950,1,1)\n",
    "  else:\n",
    "    MagnitudeDataFile = APPLDIR + '/SC_1990-2019.freq-D-10759x2400.csv'\n",
    "    DepthDataFile = APPLDIR + '/SC_1990-2019.freq-D-w_depth-10759x2400.multi.csv'\n",
    "    MultiplicityDataFile = APPLDIR + '/SC_1990-2019.freq-D-num_evts-10759x2400.csv'\n",
    "    RundleMultiplicityDataFile = APPLDIR + '/SC_1990-2019.freq-D-10755x2400-n_shock-mag-3.29.multi.csv'\n",
    "    \n",
    "    NumberofTimeunits = 10759\n",
    "    InitialDate = datetime(1990,1,1)\n",
    "  Topearthquakesfile = APPLDIR + '/topearthquakes_20.csv'\n",
    "\n",
    "  FaultLabelDataFile = APPLDIR + '/pix_faults_SmallJan21.csv'\n",
    "  MagnitudeMethod = 0\n",
    "  ReadFaultMethod = 2 # one set of x values for each input row\n",
    "  Numberxpixels = 60\n",
    "  Numberypixels = 40\n",
    "  Numberpixels = Numberxpixels*Numberypixels\n",
    "  Nloc = Numberpixels\n",
    "  Nlocdimension = 2\n",
    "  Nlocaxislengths = np.array((Numberxpixels,Numberypixels), ndmin = 1, dtype=int) # First row is top (north)\n",
    "  vertices = cal_gilbert2d(Numberxpixels,Numberypixels)\n",
    "#    print(vertices[0], vertices[1],vertices[2399], vertices[1198], vertices[1199],vertices[1200], vertices[1201])\n",
    "  sfcurvelist = vertices\n",
    "  plot_gilbert2d_space_filling(sfcurvelist, Numberxpixels, Numberypixels)\n",
    "  \n",
    "  Dropearlydata = 0\n",
    " \n",
    "  FinalDate = InitialDate + timedelta(days=NumberofTimeunits-1)\n",
    "  print(startbold + startred + InitialDate.strftime(\"%d/%m/%Y\") + ' To ' + FinalDate.strftime(\"%d/%m/%Y\") \n",
    "    + ' days ' + str(NumberofTimeunits) + resetfonts)\n",
    "  print( ' Pixels ' + str(Nloc) + ' x dimension '  + str(Nlocaxislengths[0]) + ' y dimension ' + str(Nlocaxislengths[1]) )\n",
    "  \n",
    " # Set up location information\n",
    "  Num_Time = NumberofTimeunits\n",
    "  NFIPS = Numberpixels\n",
    "  Locationname = [''] * NFIPS\n",
    "  Locationstate = [' '] * NFIPS\n",
    "  Locationpopulation = np.ones(NFIPS, dtype=int)\n",
    "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs\n",
    "  Locationcolumns = [] # String version of FIPS\n",
    "  FIPSintegerlookup = {}\n",
    "  FIPSstringlookup = {}\n",
    "  for iloc in range (0, Numberpixels):\n",
    "    localfips = iloc\n",
    "    xvalue = localfips%Nlocaxislengths[0]\n",
    "    yvalue = np.floor(localfips/Nlocaxislengths[0])\n",
    "    Stringfips = str(xvalue) + ',' + str(yvalue)\n",
    "    Locationcolumns.append(Stringfips)\n",
    "    Locationname[iloc] = Stringfips\n",
    "    Locationfips[iloc] = localfips\n",
    "    FIPSintegerlookup[localfips] = localfips\n",
    "    FIPSstringlookup[Stringfips] = localfips\n",
    "\n",
    "# TimeSeries 0 magnitude 1 depth 2 Multiplicity 3 Rundle Multiplicity\n",
    "  NpropperTimeDynamicInput = 4\n",
    "  BasicInputTimeSeries = np.empty([Num_Time,Nloc,NpropperTimeDynamicInput],dtype = np.float32)\n",
    "# StaticProps 0...NumFaultLabels-1 Fault Labels\n",
    "  NumFaultLabels = 4\n",
    "  BasicInputStaticProps = np.empty([Nloc,NumFaultLabels],dtype = np.float32)\n",
    "  RawFaultData = np.empty(Nloc,dtype = np.int)\n",
    "\n",
    "# Read in Magnitude Data into BasicInputTimeSeries\n",
    "  with open(MagnitudeDataFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != '':\n",
    "        printexit('EXIT: Wrong header on line 1 ' + Ftype + ' of ' + MagnitudeDataFile)\n",
    "\n",
    "      itime = 0\n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)!=Numberpixels + 1):\n",
    "          printexit('EXIT: Incorrect row length Magnitude ' + str(itime) + ' ' +str(len(nextrow)))\n",
    "        localtime = nextrow[0]\n",
    "        if (itime != int(localtime)):\n",
    "          printexit('EXIT: Unexpected Time in Magnitude ' + localtime + ' ' +str(itime))\n",
    "        for iloc in range(0, Numberpixels):\n",
    "          BasicInputTimeSeries[itime,iloc,0] = TransformMagnitude(float(nextrow[iloc + 1]))\n",
    "        itime += 1\n",
    "\n",
    "  if itime != Num_Time:\n",
    "    printexit('EXIT Inconsistent time lengths in Magnitude Data ' +str(itime) + ' ' + str(Num_Time))\n",
    "  print('Read Magnitude data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "# End Reading in Magnitude data\n",
    "\n",
    "# Read in Depth Data into BasicInputTimeSeries\n",
    "  with open(DepthDataFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != '':\n",
    "        printexit('EXIT: Wrong header on line 1 ' + Ftype + ' of ' + DepthDataFile)\n",
    "\n",
    "      itime = 0\n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)!=Numberpixels + 1):\n",
    "          printexit('EXIT: Incorrect row length Depth ' + str(itime) + ' ' +str(len(nextrow)))\n",
    "        localtime = nextrow[0]\n",
    "        if (itime != int(localtime)):\n",
    "          printexit('EXIT: Unexpected Time in Depth ' + localtime + ' ' +str(itime))\n",
    "        for iloc in range(0, Numberpixels):\n",
    "          BasicInputTimeSeries[itime,iloc,1] = nextrow[iloc + 1]\n",
    "        itime += 1\n",
    "\n",
    "  if itime != Num_Time:\n",
    "    printexit('EXIT Inconsistent time lengths in Depth Data ' +str(itime) + ' ' + str(Num_Time))\n",
    "  print('Read Depth data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "# End Reading in Depth data\n",
    "\n",
    "# Read in Multiplicity Data into BasicInputTimeSeries\n",
    "  with open(MultiplicityDataFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != '':\n",
    "        printexit('EXIT: Wrong header on line 1 ' + Ftype + ' of ' + MultiplicityDataFile)\n",
    "\n",
    "      itime = 0\n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)!=Numberpixels + 1):\n",
    "          printexit('EXIT: Incorrect row length Multiplicity ' + str(itime) + ' ' +str(len(nextrow)))\n",
    "        localtime = nextrow[0]\n",
    "        if (itime != int(localtime)):\n",
    "          printexit('EXIT: Unexpected Time in Multiplicity ' + localtime + ' ' +str(itime))\n",
    "        for iloc in range(0, Numberpixels):\n",
    "          BasicInputTimeSeries[itime,iloc,2] = nextrow[iloc + 1]\n",
    "        itime += 1\n",
    "\n",
    "  if itime != Num_Time:\n",
    "    printexit('EXIT Inconsistent time lengths in Multiplicity Data ' +str(itime) + ' ' + str(Num_Time))\n",
    "  print('Read Multiplicity data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "# End Reading in Multiplicity data\n",
    "\n",
    "# Read in Rundle Multiplicity Data into BasicInputTimeSeries\n",
    "  with open(RundleMultiplicityDataFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != '':\n",
    "        printexit('EXIT: Wrong header on line 1 ' + Ftype + ' of ' + RundleMultiplicityDataFile)\n",
    "\n",
    "      itime = 0\n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)!=Numberpixels + 1):\n",
    "          printexit('EXIT: Incorrect row length Rundle Multiplicity ' + str(itime) + ' ' +str(len(nextrow)))\n",
    "        localtime = nextrow[0]\n",
    "        if (itime != int(localtime)):\n",
    "          printexit('EXIT: Unexpected Time in Rundle Multiplicity ' + localtime + ' ' +str(itime))\n",
    "        for iloc in range(0, Numberpixels):\n",
    "          BasicInputTimeSeries[itime,iloc,3] = nextrow[iloc + 1]\n",
    "        itime += 1\n",
    "\n",
    "  if itime != Num_Time:\n",
    "    printexit('EXIT Inconsistent time lengths in Rundle Multiplicity Data ' +str(itime) + ' ' + str(Num_Time))\n",
    "  print('Read Rundle Multiplicity data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "# End Reading in Rundle Multiplicity data\n",
    "\n",
    "# Read in Top Earthquake Data\n",
    "  numberspecialeqs = 20\n",
    "  Specialuse = np.full(numberspecialeqs, True, dtype=bool)\n",
    "  Specialuse[14] = False\n",
    "  Specialuse[15] = False\n",
    "  Specialuse[18] = False\n",
    "  Specialuse[19] = False\n",
    "  Specialmags = np.empty(numberspecialeqs, dtype=np.float32)\n",
    "  Specialdepth = np.empty(numberspecialeqs, dtype=np.float32)\n",
    "  Speciallong = np.empty(numberspecialeqs, dtype=np.float32)\n",
    "  Speciallat = np.empty(numberspecialeqs, dtype=np.float32)\n",
    "  Specialdate = np.empty(numberspecialeqs, dtype = 'datetime64[D]')\n",
    "  Specialxpos = np.empty(numberspecialeqs, dtype=np.int32)\n",
    "  Specialypos = np.empty(numberspecialeqs, dtype=np.int32)\n",
    "  Specialeqname = []\n",
    "\n",
    "  with open(Topearthquakesfile, 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    header = next(csv_reader)\n",
    "    Ftype = header[0]\n",
    "    if Ftype != 'date':\n",
    "      printexit('EXIT: Wrong header on line 1 ' + Ftype + ' of ' + Topearthquakesfile)\n",
    "\n",
    "    iquake = 0\n",
    "    for nextrow in csv_reader:\n",
    "      if (len(nextrow)!=6):\n",
    "        printexit('EXIT: Incorrect row length Special Earthquakes ' + str(iquake) + ' ' +str(len(nextrow)))\n",
    "      Specialdate[iquake] = nextrow[0]\n",
    "      Speciallong[iquake] = nextrow[1]\n",
    "      Speciallat[iquake] = nextrow[2]\n",
    "      Specialmags[iquake] = nextrow[3]\n",
    "      Specialdepth[iquake] = nextrow[4]\n",
    "      Specialeqname.append(nextrow[5])\n",
    "      ixpos = math.floor((Speciallong[iquake]+120.0)*10.0)\n",
    "      ixpos = max(0,ixpos)\n",
    "      ixpos = min(59,ixpos)\n",
    "      iypos = math.floor((36.0-Speciallat[iquake])*10.0)\n",
    "      iypos = max(0,iypos)\n",
    "      iypos = min(39,iypos)\n",
    "      Specialxpos[iquake] = ixpos\n",
    "      Specialypos[iquake] = iypos\n",
    "      iquake += 1\n",
    "\n",
    "  for iquake in range(0,numberspecialeqs):\n",
    "    line = str(iquake) + ' mag ' + str(round(Specialmags[iquake],1)) + ' Lat/Long '\n",
    "    line += str(round(Speciallong[iquake],2)) + ' ' + str(round(Speciallong[iquake],2)) + ' ' + np.datetime_as_string(Specialdate[iquake])\n",
    "    line += Specialeqname[iquake]\n",
    "    print(line)\n",
    "  printeq()\n",
    "\n",
    "  # Possibly change Unit\n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + current_time + ' Data read in ' + RunName + ' ' + RunComment + resetfonts)\n",
    "  if Dailyunit != 1:\n",
    "    if Dailyunit == 14:\n",
    "      TimeIntervalUnitName = 'Fortnight'\n",
    "    if Dailyunit == 28:\n",
    "      TimeIntervalUnitName = 'LunarMonth'\n",
    "    BasicInputTimeSeries, NumberofTimeunits, Num_Time, InitialDate, FinalDate = GatherUpData(BasicInputTimeSeries)\n",
    "    current_time = timenow()\n",
    "    print(startbold + startred + current_time + ' Data unit changed ' +RunName + ' ' + RunComment + resetfonts)\n",
    "    Dateaxis = np.empty(Num_Time, dtype = 'datetime64[D]')\n",
    "    Dateaxis[0] = np.datetime64(InitialDate).astype('datetime64[D]')\n",
    "    for idate in range(1,Num_Time):\n",
    "      Dateaxis[idate] = Dateaxis[idate-1] + np.timedelta64(Dailyunit,'D')\n",
    "    for idate in range(0,Num_Time):\n",
    "      Dateaxis[idate] = Dateaxis[idate] + np.timedelta64(int(Dailyunit/2),'D')\n",
    "    print('Mid unit start time ' + np.datetime_as_string(Dateaxis[0]))\n",
    "\n",
    "    Totalmag = np.zeros(Num_Time,dtype = np.float32)\n",
    "    Totalefourthroot = np.zeros(Num_Time,dtype = np.float32)\n",
    "    Totalesquareroot = np.zeros(Num_Time,dtype = np.float32)\n",
    "    Totaleavgedmag = np.zeros(Num_Time,dtype = np.float32)\n",
    "    Totalmult = np.zeros(Num_Time,dtype = np.float32)\n",
    "\n",
    "    Totalmag[:] = BasicInputTimeSeries[:,:,0].sum(axis=1)\n",
    "    Totaleavgedmag = log_energy(BasicInputTimeSeries[:,:,0], sumaxis=1)\n",
    "    Totalmult[:] = BasicInputTimeSeries[:,:,3].sum(axis=1)\n",
    "    MagnitudeMethod = 1\n",
    "    Tempseries = TransformMagnitude(BasicInputTimeSeries[:,:,0])\n",
    "    Totalefourthroot = Tempseries.sum(axis=1)\n",
    "    MagnitudeMethod = 2\n",
    "    Tempseries = TransformMagnitude(BasicInputTimeSeries[:,:,0])\n",
    "    Totalesquareroot = Tempseries.sum(axis=1)\n",
    "    MagnitudeMethod = 0\n",
    "\n",
    "    basenorm = Totalmult.max(axis=0)\n",
    "    magnorm = Totalmag.max(axis=0)\n",
    "    eavgedmagnorm = Totaleavgedmag.max(axis=0)  \n",
    "    efourthrootnorm = Totalefourthroot.max(axis=0)\n",
    "    esquarerootnorm = Totalesquareroot.max(axis=0)\n",
    "    print('Maximum Mult ' + str(round(basenorm,2)) + ' Mag 0.15 ' + str(round(magnorm,2)) \n",
    "      + ' E-avg 0.5 ' + str(round(eavgedmagnorm,2)) + ' E^0.25 1.0 ' + str(round(efourthrootnorm,2))\n",
    "      + ' E^0.5 1.0 ' + str(round(esquarerootnorm,2)) )\n",
    "    Totalmag = np.multiply(Totalmag, 0.15*basenorm/magnorm)\n",
    "    Totaleavgedmag = np.multiply(Totaleavgedmag, 0.5*basenorm/eavgedmagnorm)\n",
    "    Totalefourthroot= np.multiply(Totalefourthroot, basenorm/efourthrootnorm)\n",
    "    Totalesquareroot= np.multiply(Totalesquareroot, basenorm/esquarerootnorm)\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = [16,8]\n",
    "    figure, ax = plt.subplots()\n",
    "    datemin, datemax = makeadateplot(figure, ax, Dateaxis)\n",
    "    ax.plot(Dateaxis, Totalmult, label='Multiplicity')\n",
    "    ax.plot(Dateaxis, Totalmag, label='Summed Magnitude')\n",
    "    ax.plot(Dateaxis, Totaleavgedmag, label='E-averaged Magnitude')\n",
    "    ax.plot(Dateaxis, Totalefourthroot, label='Summed E^0.25')\n",
    "    ax.plot(Dateaxis, Totalesquareroot, label='Summed E^0.5')\n",
    "    ax.set_title('Observables summed over space')\n",
    "    ax.set_xlabel(\"Years\")\n",
    "    ax.set_ylabel(\"Mult/Mag/Energy\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc='upper right')\n",
    "    Addfixedearthquakes(ax, datemin, datemax)\n",
    "    ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "    ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "    ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "    figure.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "  else:\n",
    "    print(' Data unit is the day and input this way')\n",
    "    Dateaxis = np.empty(Num_Time, dtype = 'datetime64[D]')\n",
    "    Dateaxis[0] = np.datetime64(InitialDate).astype('datetime64[D]')\n",
    "    for idate in range(1,Num_Time):\n",
    "      Dateaxis[idate] = Dateaxis[idate-1] + np.timedelta64(Dailyunit,'D')\n",
    "    for idate in range(0,Num_Time):\n",
    "      Dateaxis[idate] = Dateaxis[idate] + np.timedelta64(int(Dailyunit/2),'D')\n",
    "    print('Mid unit start time ' + np.datetime_as_string(Dateaxis[0]))\n",
    "\n",
    "# Read in Fault Label Data into BasicInputStaticProps\n",
    "# No header for data\n",
    "  with open(FaultLabelDataFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "\n",
    "      iloc = 0\n",
    "      if ReadFaultMethod ==1:\n",
    "        for nextrow in csv_reader:\n",
    "          if (len(nextrow)!=1):\n",
    "            printexit('EXIT: Incorrect row length Fault Label Data ' + str(iloc) + ' ' + str(len(nextrow)))\n",
    "          RawFaultData[iloc] = nextrow[0]\n",
    "          iloc += 1\n",
    "      else:\n",
    "        for nextrow in csv_reader:\n",
    "          if (len(nextrow)!=Numberxpixels):\n",
    "            printexit('EXIT: Incorrect row length Fault Label Data ' + str(iloc) + ' ' + str(len(nextrow)) + ' ' + str(Numberxpixels))\n",
    "          for jloc in range(0, len(nextrow)):\n",
    "            RawFaultData[iloc] = nextrow[jloc]\n",
    "            iloc += 1\n",
    "\n",
    "  if iloc != Nloc:\n",
    "    printexit('EXIT Inconsistent location lengths in Fault Label Data ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Fault Label data locations ' + str(Nloc))\n",
    "# End Reading in Fault Label data\n",
    "\n",
    "  if NumFaultLabels == 1:\n",
    "    BasicInputStaticProps[:,0] = RawFaultData.astype(np.float32)\n",
    "  else: # remap fault label more reasonably\n",
    "    unique, counts = np.unique(RawFaultData,  return_counts=True)\n",
    "    num = len(unique)\n",
    "    print('Number Fault Collections ' + str(num))\n",
    "#    for i in range(0,num):\n",
    "#      print(str(unique[i]) + ' ' + str(counts[i]))\n",
    "  \n",
    "    BasicInputStaticProps[:,0] = remapfaults(RawFaultData, Numberxpixels,Numberypixels, sfcurvelist).astype(np.float32)\n",
    "    pix_faults = np.reshape(BasicInputStaticProps[:,0],(40,60)).astype(np.int)\n",
    "    annotate_faults_ndarray(pix_faults,figsize=(24, 16))\n",
    "    sfcurvelist2 = []\n",
    "    for yloc in range(0, Numberypixels):\n",
    "      for xloc in range(0, Numberxpixels):\n",
    "          pixellocation = yloc*Numberxpixels + xloc\n",
    "          [x,y] = sfcurvelist[pixellocation]\n",
    "          sfcurvelist2.append([x,39-y])\n",
    "    BasicInputStaticProps[:,1] = remapfaults(RawFaultData, Numberxpixels,Numberypixels, sfcurvelist2).astype(np.float32)\n",
    "    sfcurvelist3 = []\n",
    "    for yloc in range(0, Numberypixels):\n",
    "      for xloc in range(0, Numberxpixels):\n",
    "          pixellocation = yloc*Numberxpixels + xloc\n",
    "          [x,y] = sfcurvelist[pixellocation]\n",
    "          sfcurvelist3.append([59-x,y])\n",
    "    BasicInputStaticProps[:,2] = remapfaults(RawFaultData, Numberxpixels,Numberypixels, sfcurvelist3).astype(np.float32)\n",
    "    sfcurvelist4 = []\n",
    "    for yloc in range(0, Numberypixels):\n",
    "      for xloc in range(0, Numberxpixels):\n",
    "          pixellocation = yloc*Numberxpixels + xloc\n",
    "          [x,y] = sfcurvelist[pixellocation]\n",
    "          sfcurvelist4.append([59-x,39-y])\n",
    "    BasicInputStaticProps[:,3] = remapfaults(RawFaultData, Numberxpixels,Numberypixels, sfcurvelist4).astype(np.float32)\n",
    "  \n",
    "  addRundleEMA = 1\n",
    "  if Dailyunit != 14:\n",
    "    addRundleEMA = 0\n",
    "  RundleLambda = [2.5]\n",
    "  RundleSteps = [144]\n",
    "  NpropperTimeDynamicCalculated = 11 + addRundleEMA\n",
    "  InputIndextogenerateEMA = 3\n",
    "  FirstEMAIndex = 15\n",
    "  NpropperTimeDynamic = NpropperTimeDynamicInput + NpropperTimeDynamicCalculated\n",
    "\n",
    "  NpropperTimeStatic = NumFaultLabels\n",
    "#  NumpredbasicperTime = NpropperTimeDynamic\n",
    "  NumpredbasicperTime = 1 # Can be 1 upto NpropperTimeDynamic\n",
    "  NumpredFuturedperTime = NumpredbasicperTime\n",
    "\n",
    "# Setup Transformed Data\n",
    "# MagnitudeMethodTransform = 0 No Transform\n",
    "# MagnitudeMethodTransform = 1 E^0.25\n",
    "# MagnitudeMethodTransform = 2 E^0.5\n",
    "  MagnitudeMethodTransform = 1\n",
    "  TransformName = 'E^0.25'\n",
    "\n",
    "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic   \n",
    "  InputPropertyNames = [' '] * NpropperTime\n",
    "  \n",
    "  DynamicNames = ['Magnitude Now', 'Depth Now', 'Multiplicity Now','Mult >3.29 Now','Mag 2/3 Month Back','Mag 1.5 Month Back','Mag 3 Months Back','Mag 6 Months Back',\n",
    "                  'Mag Year Back',TransformName + ' Now',TransformName+' 2/3 Month Back',TransformName+' 1.5 Month Back',TransformName+' 3 Months Back',TransformName+' 6 Months Back',TransformName+' Year Back']\n",
    "  if Dailyunit == 14:\n",
    "    DynamicNames = ['Magnitude 2 weeks Now', 'Depth 2 weeks Now', 'Multiplicity 2 weeks Now','Mult >3.29 2 weeks Now',\n",
    "                    'Mag 4 Weeks Back','Mag 2 Months Back','Mag 3 Months Back','Mag 6 Months Back','Mag Year Back',\n",
    "                    TransformName+ ' 2 weeks Back',TransformName+' 4 weeks Back',TransformName+' 2 Months Back',TransformName+' 3 Months Back',TransformName+' 6 Months Back',TransformName+' Year Back']\n",
    "    if addRundleEMA != 0:\n",
    "      for i in range(0,addRundleEMA):\n",
    "        DynamicNames.append('MultEMA' + str(RundleSteps[i]) + ' L' + str(round(RundleLambda[i],2)))\n",
    "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n",
    "  for iprop in range(0, NpropperTimeStatic):\n",
    "    InputPropertyNames[iprop] = 'Fault ' +str(iprop)\n",
    "  for iprop in range(0, NpropperTimeDynamic):\n",
    "    InputPropertyNames[iprop+NpropperTimeStatic] = DynamicNames[iprop]\n",
    "  Num_Extensive = 0\n",
    "\n",
    "  CDSpecial = False\n",
    "  ScaleProperties = True\n",
    "  GenerateFutures = False\n",
    "  GenerateSequences = True\n",
    "  PredictionsfromInputs = True\n",
    "  ConvertDynamicPredictedQuantity = False\n",
    "  AddSpecialstoSummedplots = True\n",
    "  UseRealDatesonplots = True\n",
    "  EarthquakeImagePlots = False\n",
    "  UseFutures = False\n",
    "  PopulationNorm = False \n",
    "  OriginalNloc = Nloc\n",
    "  MapLocation = False\n",
    "\n",
    "# Add summed magnitudes as properties to use in prediction and Calculated Properties for some\n",
    "# Calculated Properties are sums starting at given time and are set to NaN if necessary\n",
    "  NumTimeSeriesCalculatedBasic = 9\n",
    "  NumTimeSeriesCalculated = 2*NumTimeSeriesCalculatedBasic + 1\n",
    "  NamespredCalculated = ['Mag 2/3 Month Ahead','Mag 1.5 Month Ahead','Mag 3 Months Ahead','Mag 6 Months Ahead','Mag Year Ahead Ahead','Mag 2 Years Ahead','Mag 4 years Ahead','Mag Skip 1, Year ahead', 'Mag 2 years 2 ahead',\n",
    "    TransformName+' Daily Now',TransformName+' 2/3 Month Ahead',TransformName+' 1.5 Month Ahead',TransformName+' 3 Months Ahead',TransformName+' 6 Months Ahead',TransformName+' Year Ahead',\n",
    "    TransformName+' 2 Years Ahead',TransformName+' 4 years Ahead',TransformName+' Skip 1, Year ahead',TransformName+' 2 years 2 ahead']\n",
    "  Unitjumps = [ 23, 46, 92, 183, 365, 730, 1460, 365, 730]\n",
    "  Unitdelays = [ 0, 0, 0, 0, 0, 0, 0, 365, 730] \n",
    "  Plottingdelay = 1460\n",
    "  if Dailyunit == 14:\n",
    "    NumTimeSeriesCalculatedBasic = 9\n",
    "    NumTimeSeriesCalculated = 2*NumTimeSeriesCalculatedBasic + 1\n",
    "    NamespredCalculated = ['Mag 4 Weeks Ahead','Mag 2 Month Ahead','Mag 3 Months Ahead','Mag 6 Months Ahead','Mag Year Ahead','Mag 2 Years Ahead', 'Mag 4 years Ahead','Mag Skip 1, Year ahead', 'Mag 2 years 2 ahead',\n",
    "      TransformName+' 2 Weeks Now',TransformName+' 4 Weeks Ahead',TransformName+' 2 Months Ahead',TransformName+' 3 Months Ahead',TransformName+' 6 Months Ahead',\n",
    "      TransformName+' Year Ahead',TransformName+' 2 Years Ahead',TransformName+' 4 years Ahead',TransformName+' Skip 1, Year ahead',TransformName+' 2 years 2 ahead']\n",
    "    Unitjumps = [ 2, 4, 7, 13, 26, 52, 104, 26, 52]\n",
    "    Unitdelays = [ 0, 0, 0, 0, 0, 0, 0, 26, 52]\n",
    "    Plottingdelay = 104\n",
    "      \n",
    "  NumpredbasicperTime  += NumTimeSeriesCalculated\n",
    "  CalculatedTimeSeries = np.empty([Num_Time,Nloc,NumTimeSeriesCalculated],dtype = np.float32)\n",
    "  for icalc in range (0, NumTimeSeriesCalculatedBasic):\n",
    "    newicalc = icalc+1+NumTimeSeriesCalculatedBasic\n",
    "    for itime in range(0,Num_Time):\n",
    "      MagnitudeMethod = 0\n",
    "      CalculatedTimeSeries[itime,:,icalc] = AggregateEarthquakes(itime,Unitdelays[icalc],Unitjumps[icalc], Nloc, \n",
    "                                                               BasicInputTimeSeries[:,:,0], 0)\n",
    "      MagnitudeMethod = MagnitudeMethodTransform\n",
    "      CalculatedTimeSeries[itime,:,newicalc] = TransformMagnitude(CalculatedTimeSeries[itime,:,icalc])\n",
    "      MagnitudeMethod = 0\n",
    "    current_time = timenow()\n",
    "    print(startbold + startred + 'Earthquake ' + str(icalc) + ' ' + NamespredCalculated[icalc] + ' ' + current_time + ' ' +RunName + resetfonts)\n",
    "    print(startbold + startred + 'Earthquake ' + str(newicalc) + ' ' + NamespredCalculated[newicalc] + ' ' + current_time + ' ' +RunName + resetfonts)\n",
    "  MagnitudeMethod = MagnitudeMethodTransform\n",
    "  CalculatedTimeSeries[:,:,NumTimeSeriesCalculatedBasic] = TransformMagnitude(BasicInputTimeSeries[:,:,0])\n",
    "  MagnitudeMethod = 0\n",
    "  print(startbold + startred + 'Earthquake ' + str(NumTimeSeriesCalculatedBasic) + ' ' + NamespredCalculated[NumTimeSeriesCalculatedBasic] + ' ' + current_time + ' ' +RunName + resetfonts)\n",
    "\n",
    "  for iprop in range(0,NumTimeSeriesCalculated):\n",
    "    InputPropertyNames.append(NamespredCalculated[iprop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w6y73vmEleC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Earthquake Eigensystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogCpNPUMEtiK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if Earthquake:\n",
    "  if UseEarthquakeEigenSystems:\n",
    "    !pip install scipy -U\n",
    "    import scipy as sc\n",
    "    import scipy.linalg as solver\n",
    "    version = sc.version.version\n",
    "    print('SciPy version ' + str(version))\n",
    "    #x = np.array([[1,2.0],[2.0,0]])\n",
    "    #w, v = solver.eigh(x,  driver='evx')\n",
    "    #print(w)\n",
    "    #print(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KwljLkzTikB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Multiplicity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z86OVQYxTqwp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def histogrammultiplicity(Type, numbins, Data):\n",
    "  hitcounts = np.zeros(Nloc, dtype=np.int)\n",
    "  rawcounts = np.zeros(Nloc, dtype=np.int)\n",
    "  for iloc in range(0,Nloc):\n",
    "    rawcounts[iloc] = np.int(0.1+Data[:,iloc].sum(0))\n",
    "    hitcounts[iloc] = np.int(min(numbins, rawcounts[iloc]))\n",
    "  matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "  plt.rcParams.update({'font.size': 9})\n",
    "  plt.rcParams[\"figure.figsize\"] = [8,6]\n",
    "  plt.hist(hitcounts, numbins,  facecolor='b', alpha=0.75, log=True)\n",
    "  plt.title('\\n'.join(wrap(RunComment + ' ' + RunName + ' ' + Type + ' Earthquake Count per location ',70)))\n",
    "  plt.xlabel('Hit Counts')\n",
    "  plt.ylabel('Occurrences')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "  return rawcounts\n",
    "\n",
    "def threebythree(pixellocation,numxlocations,numylocations):\n",
    "  indices = np.empty([3,3], dtype=np.int)\n",
    "  \n",
    "  y = int(0.1 + pixellocation/numxlocations)\n",
    "  x = pixellocation - y*numxlocations\n",
    "  bottomx = max(0,x-1)\n",
    "  bottomx = min(bottomx,numxlocations-3)\n",
    "  bottomy = max(0,y-1)\n",
    "  bottomy = min(bottomy,numylocations-3)\n",
    "  for ix in range(0,3):\n",
    "    for iy in range(0,3):\n",
    "      x= bottomx+ix\n",
    "      y= bottomy+iy\n",
    "      pixellocation = y*numxlocations + x\n",
    "      indices[ix,iy] = pixellocation\n",
    "  return indices\n",
    "\n",
    "if Earthquake:\n",
    "  MappedLocations = np.arange(0,Nloc, dtype=np.int)\n",
    "  LookupLocations = np.arange(0,Nloc, dtype=np.int)\n",
    "  MappedNloc = Nloc\n",
    "  histogrammultiplicity('Basic', 100, BasicInputTimeSeries[:,:,2])\n",
    "  nbins = 10\n",
    "  if read1950:\n",
    "    nbins= 50\n",
    "  rawcounts1 = histogrammultiplicity('Rundle > 3.29', nbins, BasicInputTimeSeries[:,:,3])\n",
    "  TempTimeSeries = np.zeros([Num_Time,Nloc],dtype = np.float32)\n",
    "  for iloc in range (0,Nloc):\n",
    "    indices = threebythree(iloc,60,40)\n",
    "    for itime in range(0,Num_Time):\n",
    "      sum3by3 = 0.0\n",
    "      for ix in range(0,3):\n",
    "        for iy in range(0,3):\n",
    "          pixellocation = indices[ix,iy]\n",
    "          sum3by3 += BasicInputTimeSeries[itime,pixellocation,3]\n",
    "      TempTimeSeries[itime,iloc] = sum3by3\n",
    "  nbins =40\n",
    "  if read1950:\n",
    "    nbins= 150\n",
    "  rawcounts2 = histogrammultiplicity('3x3 Rundle > 3.29', nbins, TempTimeSeries)\n",
    "#\n",
    "# Define \"Interesting Locations\"\n",
    "  if read1950:\n",
    "    singleloccut = 25\n",
    "    groupedloccut = 110\n",
    "    singleloccut = 7.1\n",
    "    groupedloccut = 34.1\n",
    "\n",
    "#    groupedloccut = 1000000000\n",
    "  else:\n",
    "    singleloccut = 5.1\n",
    "    groupedloccut = 24.9\n",
    "  MappedLocations.fill(-1)\n",
    "  MappedNloc = 0\n",
    "  ct1 = 0\n",
    "  ct2 = 0\n",
    "  for iloc in range (0,Nloc):\n",
    "    if rawcounts1[iloc] >= singleloccut:\n",
    "      ct1 += 1\n",
    "    if rawcounts2[iloc] >= groupedloccut:\n",
    "      ct2 += 1\n",
    "    if rawcounts1[iloc] < singleloccut and rawcounts2[iloc] < groupedloccut:\n",
    "      continue\n",
    "    MappedLocations[iloc] = MappedNloc\n",
    "    MappedNloc += 1\n",
    "\n",
    "  LookupLocations = None\n",
    "  LookupLocations = np.empty(MappedNloc, dtype=np.int)\n",
    "  for iloc in range (0,Nloc):\n",
    "    jloc = MappedLocations[iloc]\n",
    "    if jloc >= 0:\n",
    "      LookupLocations[jloc] = iloc\n",
    "\n",
    "  TempTimeSeries = None\n",
    "  print('Total ' + str(MappedNloc) + ' Single location multiplicity cut ' + str(singleloccut) + \n",
    "  ' ' + str(ct1) + ' 3x3 ' + str(groupedloccut) + ' ' + str(ct2))\n",
    "\n",
    "  if UseEarthquakeEigenSystems:\n",
    "    if Eigenvectors > 0:\n",
    "      UseTopEigenTotal = 16\n",
    "      UseTopEigenLocal = 0\n",
    "      if Eigenvectors > 1:\n",
    "        UseTopEigenLocal = 4\n",
    "      Num_EigenProperties = UseTopEigenTotal + UseTopEigenLocal\n",
    "      EigenTimeSeries =  np.empty([Num_Time,MappedNloc],dtype = np.float32)\n",
    "      PsiTimeSeries =  np.empty([Num_Time,MappedNloc],dtype = np.float32)\n",
    "      FiTimeSeries =  np.empty([Num_Time,MappedNloc],dtype = np.float32)\n",
    "      EigenTimeSeries[:,:] = BasicInputTimeSeries[:,LookupLocations,3]\n",
    "      StoreEigenvectors = np.zeros([Num_Time,MappedNloc,MappedNloc],dtype = np.float32)\n",
    "      StoreEigencorrels = np.zeros([Num_Time,MappedNloc,MappedNloc],dtype = np.float32)\n",
    "      StoreNormingfactor = np.zeros([Num_Time],dtype = np.float32)\n",
    "      StoreNormingfactor1 = np.zeros([Num_Time],dtype = np.float32)\n",
    "      StoreNormingfactor2 = np.zeros([Num_Time],dtype = np.float32)\n",
    "      current_time = timenow()\n",
    "      print(startbold + startred + 'Start Eigen Earthquake ' \n",
    "       + current_time + ' ' +RunName + resetfonts)\n",
    "  \n",
    "      for itime in range (0,Num_Time):\n",
    "        imax = itime\n",
    "        imin = max(0, imax-25)\n",
    "        Result =  np.zeros(MappedNloc, dtype = np.float64)\n",
    "        Result = AggregateEarthquakes(imin,0,imax-imin+1, MappedNloc, EigenTimeSeries[:,:], 2)\n",
    "        PsiTimeSeries[itime,:] = Result\n",
    "        FiTimeSeries[itime,:] = EigenTimeSeries[itime,:]\n",
    "      \n",
    "      current_time = timenow()\n",
    "      print(startbold + startred + 'End Eigen Earthquake 1 ' \n",
    "       + current_time + ' ' +RunName + resetfonts)\n",
    "      Eigenvals = np.zeros([Num_Time,MappedNloc], dtype = np.float32)\n",
    "      Chi1 = np.zeros(Num_Time, dtype = np.float32)\n",
    "      Chi2 = np.zeros(Num_Time, dtype = np.float32)\n",
    "      Sumai = np.zeros(Num_Time, dtype = np.float32)\n",
    "      Bestindex = np.zeros(Num_Time, dtype = np.int)\n",
    "      Numbereigs = np.zeros(Num_Time, dtype = np.int)\n",
    "      Besttrailingindex = np.zeros(Num_Time, dtype = np.int)\n",
    "      Eig0coeff = np.zeros(Num_Time, dtype = np.float32)\n",
    "      meanmethod = 0\n",
    "      if meanmethod == 1:\n",
    "        Meanovertime = np.empty(MappedNloc, dtype = np.float32)\n",
    "        sigmaovertime = np.empty(MappedNloc, dtype = np.float32)\n",
    "        Meanovertime = FiTimeSeries.mean(axis=0)\n",
    "        Meanovertime = Meanovertime.reshape(1,MappedNloc)\n",
    "        sigmaovertime = FiTimeSeries.std(axis=0)\n",
    "        sigmaovertime = sigmaovertime.reshape(1,MappedNloc)\n",
    "      countbad = 0\n",
    "      OldActualNumberofLocationsUsed = -1\n",
    "      for itime in range (25,Num_Time):\n",
    "        LocationCounts = FiTimeSeries[0:itime,:].sum(axis=0)\n",
    "        NumLocsToday = np.count_nonzero(LocationCounts)\n",
    "        Nonzeromapping =  np.zeros(NumLocsToday, dtype = np.int)\n",
    "        ActualNumberofLocationsUsed = 0\n",
    "        for ipos in range (0,MappedNloc):\n",
    "          if LocationCounts[ipos] == 0:\n",
    "            continue\n",
    "          Nonzeromapping[ActualNumberofLocationsUsed] = ipos\n",
    "          ActualNumberofLocationsUsed +=1\n",
    "        if ActualNumberofLocationsUsed <= 1:\n",
    "          print(str(itime) + ' Abandoned ' + str(ActualNumberofLocationsUsed))\n",
    "          continue\n",
    "        FiHatTimeSeries = np.empty([itime+1,ActualNumberofLocationsUsed], dtype = np.float32)\n",
    "        if meanmethod == 1:\n",
    "          FiHatTimeSeries[:,:] = np.divide(np.subtract(FiTimeSeries[0:(itime+1),Nonzeromapping],Meanovertime[0,Nonzeromapping]),\n",
    "                                   sigmaovertime[0,Nonzeromapping])\n",
    "        else:\n",
    "          FiHatTimeSeries[:,:] = FiTimeSeries[0:(itime+1),Nonzeromapping]\n",
    "#          FiHatTimeSeries[:,:] = PsiTimeSeries[0:(itime+1),Nonzeromapping]\n",
    "        CorrelationMatrix = np.corrcoef(FiHatTimeSeries, rowvar =False)\n",
    "        bad = np.count_nonzero(np.isnan(CorrelationMatrix))\n",
    "        if bad > 0:\n",
    "          countbad += 1\n",
    "          continue\n",
    "        evalues, evectors = solver.eigh(CorrelationMatrix)\n",
    "        Newevector = evectors[:,ActualNumberofLocationsUsed-1] \n",
    "        Newevalue = evalues[ActualNumberofLocationsUsed-1]\n",
    "        debug = False\n",
    "        if debug:\n",
    "          if OldActualNumberofLocationsUsed == ActualNumberofLocationsUsed:\n",
    "            Mapdiff = np.where(np.not_equal(OldNonzeromapping,Nonzeromapping),1,0.).sum()\n",
    "            if Mapdiff > 0: \n",
    "              print(str(itime) + ' Change in mapping ' + str(ActualNumberofLocationsUsed) + ' Change ' + str(Mapdiff))\n",
    "            else:\n",
    "              Corrdiff =  np.absolute(np.subtract(OldCorrelationMatrix,CorrelationMatrix)).sum()\n",
    "              Corrorg = np.absolute(CorrelationMatrix).sum()\n",
    "              yummy = CorrelationMatrix.dot(Oldevector)\n",
    "              vTMv = yummy.dot(Oldevector)\n",
    "              Doubleyummy = CorrelationMatrix.dot(Newevector)\n",
    "              newvTMv = Doubleyummy.dot(Newevector)\n",
    "              print(str(itime) + ' Change in correlation ' + str(ActualNumberofLocationsUsed) + ' Change '\n",
    "              + str(Corrdiff) + ' original ' + str(Corrorg) + ' eval ' + str(Oldevalue) + ' new '\n",
    "                + str(Newevalue) + ' vTMv ' + str(vTMv) + ' New ' + str(newvTMv))   \n",
    "\n",
    "          else:\n",
    "            print(str(itime) + ' Change in size ' + str(OldActualNumberofLocationsUsed) + ' ' +\n",
    "                  str(ActualNumberofLocationsUsed))\n",
    "        \n",
    "        OldActualNumberofLocationsUsed = ActualNumberofLocationsUsed\n",
    "        OldNonzeromapping = Nonzeromapping\n",
    "        OldCorrelationMatrix = CorrelationMatrix\n",
    "        Oldevector = Newevector\n",
    "        Oldevalue = Newevalue\n",
    "        \n",
    "        normcoeff = 100.0/evalues.sum()\n",
    "        evalues = np.multiply(evalues, normcoeff)\n",
    "        Numbereigs[itime] = ActualNumberofLocationsUsed\n",
    " \n",
    "        for ieig in range(0,ActualNumberofLocationsUsed):\n",
    "          Eigenvals[itime, ieig] = evalues[ActualNumberofLocationsUsed-ieig-1]\n",
    "        chival = 0.0\n",
    "        sumaieig = 0.0\n",
    "        Checkvector = np.zeros(ActualNumberofLocationsUsed,dtype = np.float32)\n",
    "        largesteigcoeff = -1.0\n",
    "        largestindex = -1\n",
    "\n",
    "        Keepaisquared = np.zeros(ActualNumberofLocationsUsed, dtype=np.float32)\n",
    "        for ieig in range(0,ActualNumberofLocationsUsed):\n",
    "          aieig = 0.0\n",
    "          backwards = ActualNumberofLocationsUsed-ieig-1\n",
    "          for vectorindex in range(0,ActualNumberofLocationsUsed):\n",
    "            StoreEigenvectors[itime,backwards,Nonzeromapping[vectorindex]] = evectors[vectorindex,ieig]\n",
    "            aieig += evectors[vectorindex,ieig]*PsiTimeSeries[itime,Nonzeromapping[vectorindex]]\n",
    "          for vectorindex in range(0,ActualNumberofLocationsUsed):\n",
    "            Checkvector[vectorindex] += aieig*evectors[vectorindex, ieig]\n",
    "          aieig *= aieig\n",
    "          chival += aieig*evalues[ieig]\n",
    "          sumaieig += aieig\n",
    "          Keepaisquared[backwards] = aieig\n",
    "\n",
    "        for ieig in range(0,ActualNumberofLocationsUsed): \n",
    "          backwards = ActualNumberofLocationsUsed-ieig-1 \n",
    "          aieig = Keepaisquared[backwards]\n",
    "          aieig = aieig/sumaieig\n",
    "          if backwards == 0:\n",
    "            Eig0coeff[itime] = aieig\n",
    "          test = evalues[ieig]*aieig\n",
    "          if test > largesteigcoeff:\n",
    "            largesteigcoeff = test\n",
    "            largestindex = backwards\n",
    "        Bestindex[itime] = largestindex\n",
    "\n",
    "        discrep = 0.0\n",
    "        for vectorindex in range(0,ActualNumberofLocationsUsed):\n",
    "          discrep += pow(Checkvector[vectorindex] - PsiTimeSeries[itime,Nonzeromapping[vectorindex]], 2)\n",
    "        if discrep > 0.01:\n",
    "          print('Eigendecomposition Failure ' + str(itime) + ' ' + str(discrep))\n",
    "        Chi1[itime] = chival\n",
    "        Chi2[itime] = chival/sumaieig\n",
    "        Sumai[itime] = sumaieig\n",
    "\n",
    "        largesteigcoeff = -1.0\n",
    "        largestindex = -1\n",
    "        sumaieig = 0.0\n",
    "        Trailingtimeindex = itime-3\n",
    "        if itime > 40:\n",
    "          Trailinglimit = Numbereigs[Trailingtimeindex]\n",
    "          KeepTrailingaisquared = np.zeros(Trailinglimit, dtype=np.float32)\n",
    "          for ieig in range(0,Trailinglimit):\n",
    "            aieig = 0.0\n",
    "            for vectorindex in range(0,MappedNloc):\n",
    "#              aieig += StoreEigenvectors[Trailingtimeindex,ieig,vectorindex]*PsiTimeSeries[itime,vectorindex]\n",
    "              aieig += StoreEigenvectors[Trailingtimeindex,ieig,vectorindex]*StoreEigenvectors[itime,\n",
    "                                                                                               Bestindex[itime],vectorindex]\n",
    "            aieig *= aieig\n",
    "            sumaieig += aieig\n",
    "            KeepTrailingaisquared[ieig] = aieig\n",
    "\n",
    "          for ieig in range(0,Trailinglimit): \n",
    "            aieig = KeepTrailingaisquared[ieig]\n",
    "            aieig = aieig/sumaieig\n",
    "            test = Eigenvals[Trailingtimeindex, ieig]*aieig\n",
    "            if test > largesteigcoeff:\n",
    "              largesteigcoeff = test\n",
    "              largestindex = ieig\n",
    "          Besttrailingindex[itime] = largestindex\n",
    "\n",
    "\n",
    "\n",
    "        if itime >40: # Calculate eigenvector tracking\n",
    "          Leader = StoreEigenvectors[itime,:,:]\n",
    "          Trailer = StoreEigenvectors[itime-3,:,:]\n",
    "          StoreEigencorrels[itime,:,:] = np.tensordot(Leader,Trailer,((1),(1)))\n",
    "          StrippedDown = StoreEigencorrels[itime,Bestindex[itime],:]\n",
    "          Normingfactor =  np.multiply(StrippedDown,StrippedDown).sum()\n",
    "          Normingfactor1 =  np.multiply(StrippedDown[0:8],StrippedDown[0:8]).sum()\n",
    "          Normingfactor2 =  np.multiply(StrippedDown[0:30],StrippedDown[0:30]).sum()\n",
    "          StoreNormingfactor[itime] = Normingfactor\n",
    "          StoreNormingfactor1[itime] = Normingfactor1\n",
    "          StoreNormingfactor2[itime] = Normingfactor2\n",
    "\n",
    "      averagesumai = Sumai.mean()\n",
    "      Chi1 = np.divide(Chi1,averagesumai)\n",
    "      print('Bad Correlation Matrices ' + str(countbad))\n",
    "      print(startbold + startred + 'End Eigen Earthquake 2 ' \n",
    "       + current_time + ' ' +RunName + resetfonts)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG7_hFuDb0oM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def makeasmalldateplot(figure,ax, Dateaxis):\n",
    "  plt.rcParams.update({'font.size': 9})\n",
    "  months = mdates.MonthLocator(interval=2)   # every month\n",
    "  datemin = np.datetime64(Dateaxis[0], 'M')\n",
    "  datemax = np.datetime64(Dateaxis[-1], 'M') + np.timedelta64(1, 'M')\n",
    "  ax.set_xlim(datemin, datemax)\n",
    "  \n",
    "  months_fmt = mdates.DateFormatter('%y-%b')\n",
    "  locator = mdates.AutoDateLocator()\n",
    "  locator.intervald['MONTHLY'] = [2]\n",
    "  formatter = mdates.ConciseDateFormatter(locator)\n",
    "#  ax.xaxis.set_major_locator(locator)\n",
    "#  ax.xaxis.set_major_formatter(formatter)\n",
    "  ax.xaxis.set_major_locator(months)\n",
    "  ax.xaxis.set_major_formatter(months_fmt)\n",
    "\n",
    "  figure.autofmt_xdate()\n",
    "  return datemin, datemax\n",
    "  \n",
    "def plotquakeregions(HalfSize,xaxisdates, SetofPlots, Commontitle, ylabel, SetofColors, Startx, ncols):\n",
    "  numplotted = SetofPlots.shape[1]\n",
    "  totusedquakes = 0\n",
    "  for iquake in range(0,numberspecialeqs):\n",
    "    x_line_index = Numericaldate[iquake]\n",
    "    if (x_line_index <= Startx) or (x_line_index >= Num_Time-1):\n",
    "      continue \n",
    "    if Specialuse[iquake]:\n",
    "      totusedquakes +=1\n",
    "  nrows = math.ceil(totusedquakes/ncols)\n",
    "  sortedquakes = np.argsort(Numericaldate)\n",
    "\n",
    "  jplot = 0\n",
    "  kplot = -1\n",
    "  for jquake in range(0,numberspecialeqs):\n",
    "    iquake = sortedquakes[jquake]\n",
    "    if not Specialuse[iquake]:\n",
    "      continue\n",
    "    x_line_annotation = Specialdate[iquake]\n",
    "    x_line_index = Numericaldate[iquake]\n",
    "    if (x_line_index <= Startx) or (x_line_index >= Num_Time-1):\n",
    "      continue \n",
    "    \n",
    "    kplot +=1\n",
    "    if kplot == ncols:\n",
    "      \n",
    "      plt.savefig(APPLDIR +'/Outputs/QRegions' + str(jplot) +RunName + '.png ',format='png')\n",
    "      plt.show()\n",
    "      kplot = 0\n",
    "      jplot +=1\n",
    "    if kplot == 0:\n",
    "        plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "        figure, axs = plt.subplots(nrows=1, ncols=ncols, squeeze=False)\n",
    "\n",
    "    beginplotindex = x_line_index - HalfSize\n",
    "    beginplotindex = max(beginplotindex, Startx)\n",
    "    endplotindex = x_line_index  + HalfSize\n",
    "    endplotindex = min(endplotindex, Num_Time-1)\n",
    "\n",
    "    eachplt = axs[0,kplot]\n",
    "    ascii = ''      \n",
    "    if Specialuse[iquake]:\n",
    "      ascii = np.datetime_as_string(Specialdate[iquake]) + ' ' + str(round(Specialmags[iquake],1)) + ' ' + Specialeqname[iquake]\n",
    "    eachplt.set_title(str(iquake) + ' ' + RunName + ' Best Eigenvalue (Black) Trailing (Red) \\n' + ascii)\n",
    "    datemin, datemax = makeasmalldateplot(figure, eachplt, xaxisdates[beginplotindex:endplotindex+1])\n",
    "    for curves in range(0,numplotted):\n",
    "      eachplt.plot(xaxisdates[beginplotindex:endplotindex+1], SetofPlots[beginplotindex:endplotindex+1,curves], \n",
    "                    'o', color=SetofColors[curves], markersize =1) \n",
    "    \n",
    "    ymin, ymax = eachplt.get_ylim() \n",
    "    if ymax >= 79.9:\n",
    "      ymax = 82\n",
    "    eachplt.set_ylim(bottom=-1.0, top=max(ymax,20))\n",
    "    eachplt.set_ylabel(ylabel)\n",
    "    eachplt.set_xlabel('Time')\n",
    "    eachplt.grid(True)\n",
    "    eachplt.set_yscale(\"linear\")\n",
    "    eachplt.axvline(x=x_line_annotation, linestyle='dashed', alpha=1.0, linewidth = 2.0, color='red')\n",
    "    for kquake in range(0,numberspecialeqs):\n",
    "      if not Specialuse[kquake]:\n",
    "        continue\n",
    "      if kquake == iquake:\n",
    "        continue\n",
    "      anotherx_line_index = Numericaldate[kquake]\n",
    "      if (anotherx_line_index < beginplotindex) or (anotherx_line_index >= endplotindex):\n",
    "        continue\n",
    "      eachplt.axvline(x=Specialdate[kquake], linestyle='dashed', alpha=1.0, linewidth = 1.0, color='purple')\n",
    "    eachplt.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "\n",
    " \n",
    "  plt.savefig(APPLDIR +'/Outputs/QRegions' + str(jplot) +RunName + '.png ',format='png')\n",
    "  plt.show()\n",
    "\n",
    "EigenAnalysis = False\n",
    "if Earthquake and EigenAnalysis:\n",
    "\n",
    "  UseTopEigenTotal = 40\n",
    "  FirstTopEigenTotal = 10\n",
    "  PLTlabels = []\n",
    "  for ieig in range(0,UseTopEigenTotal):\n",
    "    PLTlabels.append('Eig-' + str(ieig))\n",
    "\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,10]\n",
    "  figure, ax = plt.subplots()\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,10]\n",
    "  for ieig in range(0,FirstTopEigenTotal):\n",
    "    ax.plot(Dateaxis[26:],np.maximum(Eigenvals[26:, ieig],0.1))\n",
    "  \n",
    "  ax.set_title(RunName + ' Multiplicity Eigenvalues')\n",
    "  ax.set_ylabel('Eigenvalue')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.set_yscale(\"log\")\n",
    "  ax.grid(True)\n",
    "  ax.legend(PLTlabels[0:FirstTopEigenTotal], loc='upper right')\n",
    "  Addfixedearthquakes(ax, datemin, datemax,ylogscale=True )\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  plt.show()\n",
    "\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,10]\n",
    "  figure, ax = plt.subplots()\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,10]\n",
    "  for ieig in range(FirstTopEigenTotal,UseTopEigenTotal):\n",
    "    ax.plot(Dateaxis[26:],np.maximum(Eigenvals[26:, ieig],0.1))\n",
    "  \n",
    "  ax.set_title(RunName + ' Multiplicity Eigenvalues')\n",
    "  ax.set_ylabel('Eigenvalue')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.set_yscale(\"linear\")\n",
    "  ax.grid(True)\n",
    "  ax.legend(PLTlabels[FirstTopEigenTotal:], loc='upper right')\n",
    "  Addfixedearthquakes(ax, datemin, datemax,ylogscale=False )\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  plt.show()\n",
    "\n",
    "  ShowEigencorrels = False\n",
    "  if ShowEigencorrels:\n",
    "    for mastereig in range(0, UseTopEigenTotal):\n",
    "      figure, ax = plt.subplots()\n",
    "      plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "      datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "      for ieig in range(0,UseTopEigenTotal):\n",
    "        alpha = 1.0\n",
    "        width = 3\n",
    "        if ieig == mastereig:\n",
    "          alpha=0.5\n",
    "          width = 1\n",
    "        ax.plot(Dateaxis[26:],np.power(StoreEigencorrels[26:,mastereig,ieig],2), alpha=alpha, linewidth = width)\n",
    "      ax.set_title(RunName + ' Eigenvalue ' + str(mastereig) + ' Current versus Past Total Correlation')\n",
    "      ax.set_ylabel('Norm')\n",
    "      ax.set_xlabel('Time')\n",
    "      ax.grid(True)\n",
    "      ax.legend(PLTlabels, loc='upper right')\n",
    "      Addfixedearthquakes(ax, datemin, datemax,ylogscale=False )\n",
    "      ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "      ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "      ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "      plt.show()\n",
    "\n",
    "  figure, ax = plt.subplots()\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  alpha = 1.0\n",
    "  width = 0.5\n",
    "  ax.plot(Dateaxis[26:],StoreNormingfactor[26:], alpha=alpha, linewidth = width)\n",
    "  ax.set_title(RunName + ' Eigenvalue Full Norming Factor with Past')\n",
    "  ax.set_ylabel('Norming Factor')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.grid(True)\n",
    "  Addfixedearthquakes(ax, datemin, datemax,ylogscale=False )\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  plt.show()\n",
    "\n",
    "  figure, ax = plt.subplots()\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  alpha = 1.0\n",
    "  width = 0.5\n",
    "  ax.plot(Dateaxis[26:],StoreNormingfactor1[26:], alpha=alpha, linewidth = width)\n",
    "  ax.set_title(RunName + ' Eigenvalue First 8 Norming Factor with Past')\n",
    "  ax.set_ylabel('Norming Factor')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.grid(True)\n",
    "  Addfixedearthquakes(ax, datemin, datemax,ylogscale=False )\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  plt.show()\n",
    "\n",
    "  figure, ax = plt.subplots()\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  alpha = 1.0\n",
    "  width = 0.5\n",
    "  ax.plot(Dateaxis[26:],StoreNormingfactor2[26:], alpha=alpha, linewidth = width)\n",
    "  ax.set_title(RunName + ' Eigenvalue First 30 Norming Factor with Past')\n",
    "  ax.set_ylabel('Norming Factor')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.grid(True)\n",
    "  Addfixedearthquakes(ax, datemin, datemax,ylogscale=False )\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  plt.show()\n",
    "\n",
    "  figure, ax = plt.subplots()\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "  ax.plot(Dateaxis[26:],Chi1[26:])\n",
    "\n",
    "  ax.set_title(RunName + ' Correlations Normalized on average over time')\n",
    "  ax.set_ylabel('Chi1')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.grid(True)\n",
    "  Addfixedearthquakes(ax, datemin, datemax)\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  ax.set_yscale(\"linear\")\n",
    "  plt.show()\n",
    "\n",
    "  figure, ax = plt.subplots()\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "  ax.plot(Dateaxis[26:],Chi2[26:])\n",
    "\n",
    "  ax.set_title(RunName + ' Correlations Normalized at each time')\n",
    "  ax.set_ylabel('Chi2')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.grid(True)\n",
    "  Addfixedearthquakes(ax, datemin, datemax)\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  ax.set_yscale(\"linear\")\n",
    "  plt.show()\n",
    "\n",
    "  figure, ax = plt.subplots()\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "  norm = np.amax(Chi1[26:])\n",
    "  Maxeig = 80\n",
    " # ax.plot(Dateaxis[26:],Chi1[26:]*Maxeig/norm)\n",
    "  ax.plot(Dateaxis[26:], 0.5 + np.minimum(Maxeig, Bestindex[26:]), 'o', color='black', markersize =1)\n",
    "  ax.plot(Dateaxis[26:], np.minimum(Maxeig, Besttrailingindex[26:]), 'o', color='red', markersize =1)\n",
    "  \n",
    "  ax.set_title(RunName + ' Best Eigenvalue (Black) Trailing (Red)')\n",
    "  ax.set_ylabel('Eig#')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.grid(True)\n",
    "  Addfixedearthquakes(ax, datemin, datemax)\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  ax.set_yscale(\"linear\")\n",
    "  plt.show()\n",
    "\n",
    "  SetofPlots = np.empty([len(Bestindex),2], dtype=np.float32)\n",
    "  SetofPlots[:,0] = 0.5 + np.minimum(Maxeig, Bestindex[:])\n",
    "  SetofPlots[:,1] = np.minimum(Maxeig, Besttrailingindex[:])\n",
    "  SetofColors = ['black',  'red']\n",
    "  plotquakeregions(25, Dateaxis, SetofPlots,\n",
    "              RunName + ' Best Eigenvalue (Black) Trailing (Red)', 'Eig#', SetofColors, 26,2)\n",
    "\n",
    "  plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "  figure, ax = plt.subplots()\n",
    "  datemin, datemax = makeadateplot(figure, ax, Dateaxis[26:])\n",
    "  ax.plot(Dateaxis[26:], Eig0coeff[26:], 'o', color='black', markersize =2)\n",
    "  ymin, ymax = ax.get_ylim() \n",
    "  ax.plot(Dateaxis[26:], Chi1[26:]*ymax/norm)\n",
    "  \n",
    "  ax.set_title(RunName + ' Fraction Largest Eigenvalue')\n",
    "  ax.set_ylabel('Eig 0')\n",
    "  ax.set_xlabel('Time')\n",
    "  ax.grid(True)\n",
    "  Addfixedearthquakes(ax, datemin, datemax)\n",
    "  ax.tick_params('x', direction = 'in', length=15, width=2, which='major')\n",
    "  ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "  ax.tick_params('x', direction = 'in', length=10, width=1, which='minor')\n",
    "  ax.set_yscale(\"linear\")\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rk4-HhAzdezY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###End of Earthquake. Reset Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g9IO3Y3ddQR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reset Start Date by a year so first entry has a 365 day sample ending at that day and so can be made an input as can all\n",
    "# lower time intervals\n",
    "# Do NOT include 2 year or 4 year in input stream\n",
    "# So we reset start date by one year skipping first 364 daya except to calculate the first one year (and lower limit) observables\n",
    "# Time indices go from 0 to NumberofTimeunits-1\n",
    "# Sequence Indices go from Begin to Begin+Tseq-1 where Begin goes from 0 to NumberofTimeunits-1-Tseq\n",
    "# So Num_Seq = Numberodays-Tseq and Begin has  Num_Seq values\n",
    "\n",
    "if Earthquake:\n",
    "  MagnitudeMethod = 0\n",
    "\n",
    "  SkipTimeUnits = 364\n",
    "  if Dailyunit == 14:\n",
    "    SkipTimeUnits = 25\n",
    "  Num_Time_old = NumberofTimeunits\n",
    "  NumberofTimeunits = NumberofTimeunits - SkipTimeUnits\n",
    "  Num_Time = NumberofTimeunits\n",
    "  InitialDate = InitialDate  + timedelta(days=SkipTimeUnits*Dailyunit)\n",
    "  FinalDate = InitialDate + timedelta(days=(NumberofTimeunits-1)*Dailyunit)\n",
    "  print('Skip ' +str(SkipTimeUnits) + ' New dates: ' + InitialDate.strftime(\"%d/%m/%Y\") + ' To '\n",
    "   + FinalDate.strftime(\"%d/%m/%Y\")+ ' days ' + str(NumberofTimeunits*Dailyunit))\n",
    "\n",
    "\n",
    "  DynamicPropertyTimeSeries =  np.empty([Num_Time,Nloc,NpropperTimeDynamic],dtype = np.float32)\n",
    "  CountNaN = np.zeros(NpropperTimeDynamic, dtype=np.int)\n",
    "  # Skewtime makes certain propert ENDS at given cell and is the cell itself if size = DailyUnit\n",
    "  # NpropperTimeDynamicInput is number of input quantities (4)\n",
    "  # NpropperTimeDynamic adds calculated quantities: accumulated magnitudes (5 magnitude, 6 E^0.25)and Rundle EMA\n",
    "  # Rundle EMA has same time cut as input time series i.e. no skew but all times are used to find EMA\n",
    "  #   addRundleEMA = 4\n",
    "  # RundleLambda = [0.75,1.5,0.75,1.5]\n",
    "  # RundleSteps = [72,72,36,36]\n",
    "  # NpropperTimeDynamicCalculated = 11 + addRundleEMA\n",
    "  # InputIndextogenerateEMA = 3\n",
    "  # FirstEMAIndex = 15\n",
    "  SkewTime = [0] * NpropperTimeDynamicInput\n",
    "  if Dailyunit == 1:\n",
    "    SkewTime = SkewTime + [22,45,91,182,364,0,22,45,91,182,364]\n",
    "  if Dailyunit == 14:\n",
    "    SkewTime = SkewTime + [1, 3, 6, 12, 25,0,1, 3, 6, 12, 25]\n",
    "  if addRundleEMA > 0:\n",
    "    SkewTime = SkewTime + [0]*addRundleEMA\n",
    "\n",
    "  for iprop in range(0,NpropperTimeDynamic):\n",
    "    addtime = SkipTimeUnits - SkewTime[iprop]\n",
    "    \n",
    "    if iprop >= NpropperTimeDynamic-addRundleEMA: # Rundle EMA 15-->18\n",
    "      EMANumber = iprop - NpropperTimeDynamic + addRundleEMA\n",
    "      for iloc in range(0,Nloc):\n",
    "        localEMA = MakeEMAMinCT(BasicInputTimeSeries[:,iloc,InputIndextogenerateEMA], RundleSteps[EMANumber], RundleLambda[EMANumber])\n",
    "        for itime in range(0,NumberofTimeunits):\n",
    "            localval = localEMA[itime+addtime]\n",
    "            if np.math.isnan(localval):\n",
    "              localval = NaN\n",
    "              CountNaN[iprop] +=1\n",
    "            DynamicPropertyTimeSeries[itime,iloc,iprop] = localval\n",
    "      print('Dynamic ' + str(iprop) + ' Rundle EMA ' + str(EMANumber) + ' Prop ' + DynamicNames[iprop] + ' Time Shift ' + str(addtime) + ' NaN ' + str(CountNaN[iprop]) + ' Could be in Pred same name')\n",
    "\n",
    "    else:\n",
    "      if iprop < NpropperTimeDynamicInput: # Input Data 0-->3\n",
    "        for itime in range(0,NumberofTimeunits):\n",
    "          for iloc in range(0,Nloc):\n",
    "            localval = BasicInputTimeSeries[itime+addtime,iloc,iprop]\n",
    "            if np.math.isnan(localval):\n",
    "              localval = NaN\n",
    "              CountNaN[iprop] +=1\n",
    "            DynamicPropertyTimeSeries[itime,iloc,iprop] = localval\n",
    "        print('Dynamic ' + str(iprop) + ' Input Time Series ' + str(iprop) + ' Prop ' + DynamicNames[iprop] + ' Time Shift ' + str(addtime) + ' NaN ' + str(CountNaN[iprop]) + ' Could be in Pred same name')\n",
    "\n",
    "      else:\n",
    "        if iprop < (NpropperTimeDynamic-6-addRundleEMA):  # Transformed Magnitude Dynamic 4-8 from Calc 0 to 4\n",
    "          icalc = iprop-NpropperTimeDynamicInput\n",
    "        else: # Aggregated E^0.25 magnitude Dynamic 9-14 from Calc 9-14\n",
    "          icalc = iprop-NpropperTimeDynamicInput+4\n",
    "        for itime in range(0,NumberofTimeunits):\n",
    "            for iloc in range(0,Nloc):\n",
    "              localval = CalculatedTimeSeries[itime+addtime,iloc,icalc]\n",
    "              if np.math.isnan(localval):\n",
    "                localval = NaN\n",
    "                CountNaN[iprop] +=1\n",
    "              DynamicPropertyTimeSeries[itime,iloc,iprop] = localval\n",
    "        print('Dynamic ' + str(iprop) + ' Calc ' + str(icalc) + ' Prop ' + DynamicNames[iprop] + ' Time Shift ' + str(addtime) + ' NaN ' + str(CountNaN[iprop]) + ' Pred ' + NamespredCalculated[icalc])\n",
    "\n",
    "# Predictions\n",
    "  NewNumTimeSeriesCalculated = NumTimeSeriesCalculated + addRundleEMA\n",
    "  NewCalculatedTimeSeries =  np.empty([Num_Time,Nloc,NewNumTimeSeriesCalculated],dtype = np.float32)\n",
    "  for iprop in range(0, NumTimeSeriesCalculated):\n",
    "    NewCalculatedTimeSeries[:,:,iprop] = CalculatedTimeSeries[SkipTimeUnits:Num_Time+SkipTimeUnits,:,iprop]\n",
    "  if addRundleEMA > 0:\n",
    "    for iEMA in range(0,addRundleEMA):\n",
    "      NewCalculatedTimeSeries[:,:,iEMA+NumTimeSeriesCalculated] = DynamicPropertyTimeSeries[:,:,iEMA+FirstEMAIndex]\n",
    "      NamespredCalculated.append(DynamicNames[iEMA+FirstEMAIndex])\n",
    "      InputPropertyNames.append(DynamicNames[iEMA+FirstEMAIndex])\n",
    "\n",
    "  NumTimeSeriesCalculated = NewNumTimeSeriesCalculated \n",
    "  CalculatedTimeSeries = None\n",
    "  CalculatedTimeSeries = NewCalculatedTimeSeries\n",
    "  BasicInputTimeSeries = None\n",
    "  if GarbageCollect:\n",
    "    gc.collect()\n",
    "\n",
    "  print(startbold+startred+'Predicted NaN values ' + resetfonts)\n",
    "  for icalc in range(0, NumTimeSeriesCalculated):\n",
    "    CountofNaN = 0\n",
    "    for itime in range(0,NumberofTimeunits):\n",
    "      for iloc in range(0,Nloc):\n",
    "        localval = CalculatedTimeSeries[itime,iloc,icalc]\n",
    "        if np.math.isnan(localval):\n",
    "          localval = NaN\n",
    "          CountofNaN += 1\n",
    "    print('Predictions(calc) ' + str(icalc) + ' NaN ' + str(CountofNaN) + ' Pred ' + NamespredCalculated[icalc])\n",
    "\n",
    "\n",
    "  \n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + 'Finish Basic Earthquake Setup ' + current_time + ' ' +RunName + ' ' + RunComment + resetfonts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My9HJsTkbCRo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Set Earthquake Execution Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWmTr4vUbRcA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if Earthquake:\n",
    "  SymbolicWindows = True\n",
    "  Tseq = 26\n",
    "  if UseTFTModel == True:\n",
    "    if Dailyunit == 14:\n",
    "      GenerateFutures = True\n",
    "      UseFutures = True\n",
    "  else:\n",
    "      GenerateFutures = False\n",
    "      UseFutures = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLL834JvEjLd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Plot Earthquake Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfCR1vKwEpVw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "\n",
    "def plotimages(Array,Titles,nrows,ncols):\n",
    "  usedcolormap = \"YlGnBu\"\n",
    "  plt.rcParams[\"figure.figsize\"] = [16,6*nrows]\n",
    "  figure, axs = plt.subplots(nrows=nrows, ncols=ncols, squeeze=False)\n",
    "  iplot=0\n",
    "  images = []\n",
    "  norm = colors.Normalize(vmin=fullmin, vmax=fullmax)\n",
    "  for jplot in range(0,nrows):\n",
    "    for kplot in range (0,ncols):\n",
    "      eachplt = axs[jplot,kplot]\n",
    "      if MapLocation:\n",
    "        Plotit = np.zeros(OriginalNloc, dtype = np.float32)\n",
    "        for jloc in range (0,Nloc):\n",
    "          Plotit[LookupLocations[jloc]] = Array[iplot][jloc]\n",
    "          TwoDArray = np.reshape(Plotit,(40,60))\n",
    "      else:\n",
    "        TwoDArray = np.reshape(Array[iplot],(40,60))\n",
    "      extent = (-120,-114, 36,32)\n",
    "      images.append(eachplt.imshow(TwoDArray, cmap=usedcolormap, norm=norm,extent=extent))\n",
    "      eachplt.label_outer()\n",
    "      eachplt.set_title(Titles[iplot])\n",
    "      iplot +=1\n",
    "  figure.colorbar(images[0], ax=axs, orientation='vertical', fraction=.05)\n",
    "  plt.show()\n",
    "\n",
    "if Earthquake:\n",
    "# DynamicPropertyTimeSeries and CalculatedTimeSeries are dimensione by time 0 ...Num_Time-1\n",
    "# DynamicPropertyTimeSeries holds values upto and including that time\n",
    "# CalculatedTimeSeries holds values STARTING at that time\n",
    "# Plot magnitudes first and them chosen Energy power\n",
    "  for transformedpointer in range(0,2):\n",
    "    localplot1=0\n",
    "    localplot2=NumTimeSeriesCalculatedBasic\n",
    "    if transformedpointer == 0:\n",
    "      fullmin = np.nanmin(CalculatedTimeSeries[:,:,0:NumTimeSeriesCalculatedBasic])\n",
    "      fullmax = np.nanmax(CalculatedTimeSeries[:,:,0:NumTimeSeriesCalculatedBasic])\n",
    "      fullmin = min(fullmin,np.nanmin(DynamicPropertyTimeSeries[:,:,0]))\n",
    "      fullmax = max(fullmax,np.nanmax(DynamicPropertyTimeSeries[:,:,0]))\n",
    "      print('Full Magnitude Ranges ' + str(fullmin) + ' ' + str(fullmax))\n",
    "    else:\n",
    "      localplot1=NumTimeSeriesCalculatedBasic\n",
    "      localplot2=NumTimeSeriesCalculated\n",
    "      fullmin = np.nanmin(CalculatedTimeSeries[:,:,localplot1:localplot2])\n",
    "      fullmax = np.nanmax(CalculatedTimeSeries[:,:,localplot1:localplot2])\n",
    "      print('Full Energy Transformed Ranges ' + str(fullmin) + ' ' + str(fullmax))\n",
    "    Num_Seq = NumberofTimeunits-Tseq\n",
    "    dayindexmax = Num_Seq-Plottingdelay\n",
    "    Numdates = 4\n",
    "    denom = 1.0/np.float64(Numdates-1)\n",
    "    for plotdays in range(0,Numdates):\n",
    "      dayindexvalue = math.floor(0.1 + (plotdays*dayindexmax)*denom)\n",
    "      if dayindexvalue < 0:\n",
    "        dayindexvalue = 0\n",
    "      if dayindexvalue > dayindexmax:\n",
    "        dayindexvalue = dayindexmax \n",
    "      dayindexvalue += Tseq\n",
    "      InputImages =[]\n",
    "      InputTitles =[]\n",
    "      InputImages.append(DynamicPropertyTimeSeries[dayindexvalue,:,0])\n",
    "      ActualDate = InitialDate + timedelta(days=dayindexvalue)\n",
    "      if transformedpointer == 0:\n",
    "        localmax1 = DynamicPropertyTimeSeries[dayindexvalue,:,0].max()\n",
    "        localmin1 = DynamicPropertyTimeSeries[dayindexvalue,:,0].min() \n",
    "        InputTitles.append('Day ' +str(dayindexvalue) + ' ' + ActualDate.strftime(\"%d/%m/%Y\") + ' ' + InputPropertyNames[NpropperTimeStatic] + ' max/min '\n",
    "        + str(round(localmax1,3)) + ' ' + str(round(localmin1,3))) \n",
    "\n",
    "      for localplot in range(localplot1,localplot2):\n",
    "        localmax1 = CalculatedTimeSeries[dayindexvalue,:,localplot].max()\n",
    "        localmin1 = CalculatedTimeSeries[dayindexvalue,:,localplot].min() \n",
    "        InputImages.append(CalculatedTimeSeries[dayindexvalue,:,localplot])\n",
    "        InputTitles.append('Day ' +str(dayindexvalue) + ' ' + ActualDate.strftime(\"%d/%m/%Y\") + ' ' + NamespredCalculated[localplot] + ' max/min '\n",
    "          + str(round(localmax1,3)) + ' ' + str(round(localmin1,3)))\n",
    "      plotimages(InputImages,InputTitles,5,2) # Ten plots fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZkRunIoEAs0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read Hydrology Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JouaTJggKoZs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read Hydrology\n",
    "if Hydrology:\n",
    "  PreparedDataFile = APPLDIR + '/data.tar.bz2'\n",
    "  !ls /content/gdrive/'My Drive'/'Colab Datasets'/Hydrology\n",
    "  !tar xjf /content/gdrive/'My Drive'/'Colab Datasets'/Hydrology/data.tar.bz2 -C /content/gdrive/'My Drive'/'Colab Datasets'/Hydrology\n",
    "  import json\n",
    "\n",
    "  RawInputStaticProps = np.load(APPLDIR + '/BasicInputStaticProps.npy', allow_pickle = True)\n",
    "  RawInputTimeSeries = np.load(APPLDIR + '/BasicInputTimeSeries.npy', allow_pickle = True)\n",
    "  NuminputSeries = RawInputTimeSeries.shape[1]\n",
    "  NuminputProps = RawInputStaticProps.shape[1]\n",
    "  print('Input Hydrology Shapes ' + str(RawInputTimeSeries.shape) + ' ' +str(RawInputStaticProps.shape))\n",
    "  \n",
    "  with open(APPLDIR + '/metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "  Nloc = metadata['Nloc']\n",
    "  TimeSeriesmetadata = metadata['BasicInputTimeSeries']\n",
    "  InitialDate = datetime.strptime(TimeSeriesmetadata['initial_date'],'%Y-%m-%dT%H:%M:%S.%f000')\n",
    "  FinalDate = datetime.strptime(TimeSeriesmetadata['end_date'],'%Y-%m-%dT%H:%M:%S.%f000')\n",
    "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "  print(InitialDate.strftime(\"%d/%m/%Y\") + ' To ' + FinalDate.strftime(\"%d/%m/%Y\")+ ' days ' + str(NumberofTimeunits) + ' Locations ' + str(Nloc))\n",
    "  TimeSeriesLabels = TimeSeriesmetadata['fields']\n",
    "  print(TimeSeriesLabels)\n",
    "  StaticPropsmetadata = metadata['BasicInputStaticProps']\n",
    "  RawLabels = StaticPropsmetadata['fields']\n",
    "  print(RawLabels)\n",
    "  BasicInputTimeSeries = np.delete(RawInputTimeSeries,[0,1],1)\n",
    "  BasicInputTimeSeries = np.reshape(BasicInputTimeSeries,[NumberofTimeunits,Nloc,NuminputSeries-2])\n",
    "  BasicInputStaticProps = np.delete(RawInputStaticProps,[0,12,21,22],1)\n",
    "  StaticLabels = np.delete(RawLabels,[0,12,21,22],0)\n",
    "\n",
    "  Num_Time = NumberofTimeunits\n",
    "  NFIPS = Nloc\n",
    "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs/gauge_id\n",
    "  Locationcolumns = [] # String version of FIPS/gauge_id\n",
    "  FIPSintegerlookup = {}\n",
    "  FIPSstringlookup = {}\n",
    "  Locationname = ['Empty'] * NFIPS\n",
    "  Locationstate = [' '] * NFIPS\n",
    "  Locationpopulation = np.ones(NFIPS, dtype=int)\n",
    "  gauge_idvalues = metadata['locs']\n",
    "  placenames = metadata['loc_names']\n",
    "  for iloc in range(0,Nloc):\n",
    "    fips = str(gauge_idvalues[iloc])\n",
    "    Locationfips[iloc] = int(fips)\n",
    "    Locationcolumns.append(fips)\n",
    "    FIPSintegerlookup[int(fips)] = iloc\n",
    "    FIPSstringlookup[fips] = iloc\n",
    "    Locationname[iloc] = placenames[iloc]\n",
    "  \n",
    "  CDSpecial = False\n",
    "  NpropperTimeDynamic = 6\n",
    "  NpropperTimeStatic = 27\n",
    "  NumpredbasicperTime = NpropperTimeDynamic\n",
    "  NumpredFuturedperTime = NumpredbasicperTime\n",
    "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic   \n",
    "  InputPropertyNames = [' '] * NpropperTime\n",
    "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n",
    "  for iprop in range(0, NpropperTimeStatic):\n",
    "    InputPropertyNames[iprop] = StaticLabels[iprop]\n",
    "  for iprop in range(0, NpropperTimeDynamic):\n",
    "    InputPropertyNames[iprop+NpropperTimeStatic] = TimeSeriesLabels[iprop+2]\n",
    "  Num_Extensive = 0\n",
    "\n",
    "  ScaleProperties = True\n",
    "  GenerateFutures = False\n",
    "  GenerateSequences = True\n",
    "  PredictionsfromInputs = True\n",
    "  ConvertDynamicPredictedQuantity = False\n",
    "\n",
    "  UseFutures = False\n",
    "  PopulationNorm = False \n",
    "  DynamicPropertyTimeSeries = np.empty_like(BasicInputTimeSeries, dtype = np.float32)\n",
    "  CountNaN = np.zeros(NpropperTimeDynamic, dtype=np.int)\n",
    "  for itime in range(0,NumberofTimeunits):\n",
    "    for iloc in range(0,Nloc):\n",
    "      for iprop in range(0,NpropperTimeDynamic):\n",
    "        localval = BasicInputTimeSeries[itime,iloc,iprop]\n",
    "        if np.math.isnan(localval):\n",
    "          localval = NaN\n",
    "          CountNaN[iprop] +=1\n",
    "        else:\n",
    "          if (localval < 0.0) and (iprop==5):\n",
    "            localval = NaN\n",
    "            CountNaN[iprop] +=1\n",
    "        DynamicPropertyTimeSeries[itime,iloc,iprop] = localval\n",
    "  print(startbold+startred+'Input NaN values ' + resetfonts)\n",
    "  for iprop in range(0,NpropperTimeDynamic):\n",
    "    print(InputPropertyNames[iprop+NpropperTimeStatic] + ' ' + str(CountNaN[iprop]))\n",
    "\n",
    "  BasicInputTimeSeries = None\n",
    "  if GarbageCollect:\n",
    "    gc.collect()\n",
    "\n",
    "  \n",
    "# Overall Parameters set for Hydrology \n",
    "  SymbolicWindows = True\n",
    "  Tseq = 21\n",
    "  Plotsplitsize = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d_JaiVAV5yk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read April Nov 2021, May 2022, 7day Covid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiClFXFWWETE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if ReadApril2021Covid:\n",
    "  Dropearlydata = 40 # 3 more than needed by covariates so as to get \"round number of days\"\n",
    "  if ReadNov2021Covid:\n",
    "    Dropearlydata = 0\n",
    "  if ReadMay2022Covid:\n",
    "    Dropearlydata = 40 # XXX\n",
    "  if Read7dayCovid:\n",
    "    Dropearlydata = 0\n",
    "  NIHCovariates = True\n",
    "  UseOLDCovariates = False\n",
    "  LengthFutures = 0\n",
    "\n",
    "  if ReadNov2021Covid:\n",
    "# Set Dropearlydata to any number of days >= 0\n",
    "    InitialDate = datetime(2020,2,29) + timedelta(days=Dropearlydata)\n",
    "    FinalDate = datetime(2021,11,29)\n",
    "\n",
    "# Or set InitialData >= actual Date and FinalDate\n",
    "    Dropearlydata = (InitialDate - datetime(2020,2,29)).days\n",
    "    if Dropearlydata < 0:\n",
    "      printexit('Illegal start date ' + str(Dropearlydata))\n",
    "    NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "    print(\"Total number of Days November 2021 Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
    "\n",
    "    DATASETDIR = APPLDIR + '/CovidNovember2021'\n",
    "    CasesFile = DATASETDIR + '/' + 'Cases.csv'\n",
    "    DeathsFile = DATASETDIR + '/' + 'Deaths.csv'\n",
    "\n",
    "  elif ReadMay2022Covid:\n",
    "    if Read7dayCovid:\n",
    "      InitialDate = datetime(2020,2,29) + timedelta(days=Dropearlydata)\n",
    "      FinalDate = datetime(2022,2,28)\n",
    "      FinalDate = datetime(2021,11,29)\n",
    "      RootCasesDeaths = False\n",
    "      NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "      print(\"Total number of Days 7 day Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
    "\n",
    "      DATASETDIR = APPLDIR + '/Covid7Day/2021-11-29'\n",
    "      CasesFile = DATASETDIR + '/' + 'Cases.csv'\n",
    "      DeathsFile = DATASETDIR + '/' + 'Deaths.csv'\n",
    "\n",
    "    else: # May 2022 Data\n",
    "  # Set Dropearlydata to any number of days >= 0\n",
    "      InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
    "      FinalDate = datetime(2022,5,15) # In current code this is minmum final date between cases and deaths\n",
    "  # Or set InitialData >= actual Initial Date and FinalDate\n",
    "  # Initial Date must be >= datetime(2020,1,22) and final date must be <= datetime(2022,5,15)\n",
    "      Dropearlydata = (InitialDate - datetime(2020,1,22)).days\n",
    "      if Dropearlydata < 0:\n",
    "        printexit('Illegal start date ' + str(Dropearlydata))\n",
    "      NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "      print(\"Total number of Days May 2022 Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
    "\n",
    "      DATASETDIR = APPLDIR + '/CovidMay17-2022'\n",
    "      CasesFile = DATASETDIR + '/' + 'Cases.csv'\n",
    "      DeathsFile = DATASETDIR + '/' + 'Deaths.csv'  \n",
    "\n",
    "  else: # April2021 Covid Data\n",
    "    InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
    "    FinalDate = datetime(2021,4,14)\n",
    "    NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "    print(\"Total number of Days April 2021 Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
    "\n",
    "    DATASETDIR = APPLDIR + '/CovidApril14-2021'\n",
    "\n",
    "    CasesFile = DATASETDIR + '/' + 'US_daily_cumulative_cases_April14.csv'\n",
    "    DeathsFile = DATASETDIR + '/' + 'US_daily_cumulative_deaths_April14.csv'\n",
    "\n",
    "  LocationdataFile = DATASETDIR + '/Population.csv'\n",
    "  VotingdataFile = DATASETDIR + '/2020votes.csv'\n",
    "  AlaskaVotingdataFile = DATASETDIR + '/Alaskavoting2016.csv'\n",
    "\n",
    "  Nloc = 3142\n",
    "  NFIPS = 3142\n",
    "\n",
    "# Set up location information\n",
    "  Num_Time = NumberofTimeunits\n",
    "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs\n",
    "  Locationcolumns = [] # String version of FIPS\n",
    "  FIPSintegerlookup = {}\n",
    "  FIPSstringlookup = {}\n",
    "  BasicInputTimeSeries = np.empty([Num_Time,Nloc,2],dtype = np.float32)\n",
    "\n",
    "# Read in  cases Data into BasicInputTimeSeries\n",
    "  with open(CasesFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS' and Ftype != 'casrn':\n",
    "        printexit('EXIT: Wrong file type Cases ' + Ftype)\n",
    "      hformat = '%Y-%m-%d'\n",
    "      TargetDate = datetime.strptime(header[1], hformat)\n",
    "      if (InitialDate - TargetDate).days != Dropearlydata:\n",
    "        printexit('Incorrect cases initial date ' + str(Dropearlydata) + ' '  + str(TargetDate) + ' ' + str(InitialDate))\n",
    "\n",
    "      iloc = 0    \n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)< NumberofTimeunits + 1 + Dropearlydata):\n",
    "          printexit('EXIT: Incorrect row length Cases ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "        # skip first entry\n",
    "        localfips = nextrow[0]\n",
    "        Locationcolumns.append(localfips)\n",
    "        Locationfips[iloc] = int(localfips)\n",
    "        FIPSintegerlookup[int(localfips)] = iloc\n",
    "        FIPSstringlookup[localfips] = iloc\n",
    "        for itime in range(0, NumberofTimeunits):\n",
    "          BasicInputTimeSeries[itime,iloc,0] = nextrow[itime + 1 + Dropearlydata]\n",
    "          if Dropearlydata > 0:\n",
    "            floatlast = np.float(nextrow[Dropearlydata])\n",
    "            BasicInputTimeSeries[itime,iloc,0] = BasicInputTimeSeries[itime,iloc,0] - floatlast\n",
    "        iloc += 1\n",
    "# End Reading in cases data\n",
    "\n",
    "  if iloc != Nloc:\n",
    "          printexit('EXIT Inconsistent location lengths Cases ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Cases data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "\n",
    "# Read in deaths Data into BasicInputTimeSeries\n",
    "  with open(DeathsFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS' and Ftype != 'casrn':\n",
    "        printexit('EXIT: Wrong file type Deaths ' + Ftype)\n",
    "      hformat = '%Y-%m-%d'\n",
    "      TargetDate = datetime.strptime(header[1], hformat)\n",
    "      if (InitialDate - TargetDate).days != Dropearlydata:\n",
    "        printexit('Incorrect deaths initial date ' + str(Dropearlydata) + ' '  + str(TargetDate) + ' ' + str(InitialDate))\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)<NumberofTimeunits + 1 + Dropearlydata):\n",
    "          printexit('EXIT: Incorrect row length Deaths ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "        localfips = nextrow[0]\n",
    "        if (Locationfips[iloc] != int(localfips)):\n",
    "          printexit('EXIT: Unexpected FIPS Deaths ' + localfips + ' ' +str(Locationfips[iloc]))\n",
    "        for itime in range(0, NumberofTimeunits):\n",
    "          BasicInputTimeSeries[itime,iloc,1] = np.float(nextrow[itime + 1 + Dropearlydata])\n",
    "          if Dropearlydata > 0:\n",
    "            floatlast = np.float(nextrow[Dropearlydata])\n",
    "            BasicInputTimeSeries[itime,iloc,1] = BasicInputTimeSeries[itime,iloc,1] - floatlast\n",
    "        iloc += 1\n",
    "# End Reading in deaths data\n",
    "\n",
    "  if iloc != Nloc:\n",
    "    printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Deaths data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "\n",
    "  Locationname = ['Empty'] * NFIPS\n",
    "  Locationstate = ['Empty'] * NFIPS\n",
    "  Locationpopulation = np.empty(NFIPS, dtype=int)\n",
    "  with open(LocationdataFile, 'r', encoding='latin1') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type Prop Data ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        localfips = int(nextrow[0])\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          Locationname[jloc] = nextrow[4]\n",
    "          Locationstate[jloc] = nextrow[3]\n",
    "          Locationpopulation[jloc] = int(nextrow[2])\n",
    "          iloc += 1 # just counting lines  \n",
    "        else:\n",
    "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))  \n",
    "# END setting NFIPS location properties\n",
    "\n",
    "  DemVoting = np.full(NFIPS, -1.0, dtype=np.float32)\n",
    "  RepVoting = np.full(NFIPS, -1.0, dtype=np.float32)\n",
    "  with open(VotingdataFile, 'r', encoding='latin1') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'state_name':\n",
    "        printexit('EXIT: Wrong file type Voting Data ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        localfips = int(nextrow[1])\n",
    "        if localfips > 2900 and localfips < 2941: # Alaska not useful\n",
    "          continue\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          if DemVoting[jloc] >= 0.0:\n",
    "             printexit('EXIT Double Setting of FIPS ' +str(iloc) + ' ' + str(localfips))\n",
    "          DemVoting[jloc] = nextrow[8]\n",
    "          RepVoting[jloc] = nextrow[7]\n",
    "          iloc += 1 # just counting lines  \n",
    "        else:\n",
    "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))  \n",
    "\n",
    "  with open(AlaskaVotingdataFile, 'r',encoding='utf-8-sig') as read_obj: # remove ufeff\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'SpecialAlaska':\n",
    "        printexit('EXIT: Wrong file type Alaska Voting Data ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        localfips = int(nextrow[1])\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          if DemVoting[jloc] >= 0.0:\n",
    "             printexit('EXIT Double Setting of FIPS ' +str(iloc) + ' ' + str(localfips))\n",
    "          DemVoting[jloc] = float(nextrow[2]) * 42.77/36.5\n",
    "          RepVoting[jloc] = float(nextrow[3]) * 52.83/51.3\n",
    "          iloc += 1 # just counting lines  \n",
    "        else:\n",
    "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))\n",
    "\n",
    "  for iloc in range(0,NFIPS):\n",
    "    if DemVoting[iloc] >= 0.0:\n",
    "      continue\n",
    "    print(str(iloc) + ' Missing Votes ' + str(Locationfips[iloc]) + ' ' + Locationname[iloc] + ' ' + Locationstate[iloc] + ' pop ' + str( Locationpopulation[iloc]))\n",
    "    DemVoting[iloc] = 0.5\n",
    "    RepVoting[iloc] = 0.5\n",
    "\n",
    "# Set Static Properties of the Nloc studied locations\n",
    "# Order is Static, Dynamic, Cases, Deaths\n",
    "# Voting added as 13th covariate\n",
    "# Add fully vaccinated in November 2021\n",
    "  NpropperTimeDynamic = 13\n",
    "  if ReadNov2021Covid:\n",
    "    NpropperTimeDynamic = 14\n",
    "  if ReadMay2022Covid:\n",
    "    NpropperTimeDynamic = 15\n",
    "    if Read7dayCovid:\n",
    "      NpropperTimeDynamic = 7\n",
    "  NpropperTimeStatic = 0\n",
    "\n",
    "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic + 2   \n",
    "  InputPropertyNames = [] * NpropperTime\n",
    "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n",
    "  print('Initial Date ' + str(InitialDate) + ' Final Date '  + str(FinalDate) + ' NpropperTimeStatic ' +str(NpropperTimeStatic) + ' NpropperTimeDynamic ' +str(NpropperTimeDynamic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vYSSeRGWUSJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read January 2021 Covid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d2J58X9EjSv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if ReadJan2021Covid:\n",
    "  Dropearlydata = 37\n",
    "  NIHCovariates = True\n",
    "  UseOLDCovariates = False\n",
    "\n",
    "  InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
    "  FinalDate = datetime(2021,1,26)\n",
    "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "  print(\"Total number of Days January 2021 Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
    "\n",
    "  DATASETDIR = APPLDIR + '/January2021'\n",
    "\n",
    "  CasesFile = DATASETDIR + '/' + 'US_daily_cumulative_cases.csv'\n",
    "  DeathsFile = DATASETDIR + '/' + 'US_daily_cumulative_deaths.csv'\n",
    "  LocationdataFile = DATASETDIR + '/Population.csv'\n",
    "\n",
    "  Nloc = 3142\n",
    "  NFIPS = 3142\n",
    "\n",
    "# Set up location information\n",
    "  Num_Time = NumberofTimeunits\n",
    "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs\n",
    "  Locationcolumns = [] # String version of FIPS\n",
    "  FIPSintegerlookup = {}\n",
    "  FIPSstringlookup = {}\n",
    "  BasicInputTimeSeries = np.empty([Num_Time,Nloc,2],dtype = np.float32)\n",
    "\n",
    "# Read in  cases Data into BasicInputTimeSeries\n",
    "  with open(CasesFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type Cases ' + Ftype)\n",
    "\n",
    "      iloc = 0    \n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)< NumberofTimeunits + 1 + Dropearlydata):\n",
    "          printexit('EXIT: Incorrect row length Cases ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "        # skip first entry\n",
    "        localfips = nextrow[0]\n",
    "        Locationcolumns.append(localfips)\n",
    "        Locationfips[iloc] = int(localfips)\n",
    "        FIPSintegerlookup[int(localfips)] = iloc\n",
    "        FIPSstringlookup[localfips] = iloc\n",
    "        for itime in range(0, NumberofTimeunits):\n",
    "          BasicInputTimeSeries[itime,iloc,0] = nextrow[itime + 1 + Dropearlydata]\n",
    "          if Dropearlydata > 0:\n",
    "            floatlast = np.float(nextrow[Dropearlydata])\n",
    "            BasicInputTimeSeries[itime,iloc,0] = BasicInputTimeSeries[itime,iloc,0] - floatlast\n",
    "        iloc += 1\n",
    "# End Reading in cases data\n",
    "\n",
    "  if iloc != Nloc:\n",
    "          printexit('EXIT Inconsistent location lengths Cases ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Cases data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "\n",
    "# Read in deaths Data into BasicInputTimeSeries\n",
    "  with open(DeathsFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type Deaths ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)<NumberofTimeunits + 1 + Dropearlydata):\n",
    "          printexit('EXIT: Incorrect row length Deaths ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "        localfips = nextrow[0]\n",
    "        if (Locationfips[iloc] != int(localfips)):\n",
    "          printexit('EXIT: Unexpected FIPS Deaths ' + localfips + ' ' +str(Locationfips[iloc]))\n",
    "        for itime in range(0, NumberofTimeunits):\n",
    "          BasicInputTimeSeries[itime,iloc,1] = nextrow[itime + 1 + Dropearlydata]\n",
    "          if Dropearlydata > 0:\n",
    "            floatlast = np.float(nextrow[Dropearlydata])\n",
    "            BasicInputTimeSeries[itime,iloc,1] = BasicInputTimeSeries[itime,iloc,1] - floatlast\n",
    "        iloc += 1\n",
    "# End Reading in deaths data\n",
    "\n",
    "  if iloc != Nloc:\n",
    "    printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Deaths data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "\n",
    "  Locationname = ['Empty'] * NFIPS\n",
    "  Locationstate = ['Empty'] * NFIPS\n",
    "  Locationpopulation = np.empty(NFIPS, dtype=int)\n",
    "  with open(LocationdataFile, 'r', encoding='latin1') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type Prop Data ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        localfips = int(nextrow[0])\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          Locationname[jloc] = nextrow[4]\n",
    "          Locationstate[jloc] = nextrow[3]\n",
    "          Locationpopulation[jloc] = int(nextrow[2])\n",
    "          iloc += 1 # just counting lines  \n",
    "        else:\n",
    "          printexit('EXIT Inconsistent FIPS ' +str(iloc) + ' ' + str(localfips))  \n",
    "# END setting NFIPS location properties\n",
    "\n",
    "# Set Static Properties of the Nloc studied locations\n",
    "# Order is Static, Dynamic, Cases, Deaths\n",
    "  NpropperTimeDynamic = 12\n",
    "  NpropperTimeStatic = 0\n",
    "\n",
    "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic + 2   \n",
    "  InputPropertyNames = [' '] * NpropperTime\n",
    "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n",
    "\n",
    "\n",
    "\n",
    "# Finish this after NIH Covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrQQI8N7GWMN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read Data defining COVID problem August 2020 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwHtZFK4GXeT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if ReadAugust2020Covid:\n",
    "  InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
    "  FinalDate = datetime(2020,8,13)\n",
    "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "  print(\"Total number of Days August Dataset \" + str(NumberofTimeunits) + ' Dropping at start ' + str(Dropearlydata))\n",
    "\n",
    "  DATASETDIR = APPLDIR +'/MidAugust2020Data'\n",
    "\n",
    "  CasesFile = DATASETDIR + '/' + 'covid-cases.csv'\n",
    "  DeathsFile = DATASETDIR + '/' + 'covid-deaths.csv'\n",
    "  CovariatesFile = DATASETDIR + '/' + 'PVI-31July2020.csv'\n",
    "  if RereadMay2020 or UseOLDCovariates:\n",
    "    CovariatesFile = DATASETDIR + '/' + 'Static_316USCities_Pop.csv'\n",
    "  LocationdataFile = DATASETDIR + '/' + 'Static_316USCities_Pop.csv'\n",
    "\n",
    "  Nloc = 314\n",
    "  NFIPS = 316\n",
    "\n",
    "if RereadMay2020:\n",
    "  InitialDate = datetime(2020,1,22) + timedelta(days=Dropearlydata)\n",
    "  FinalDate = datetime(2020,5,25)\n",
    "  NumberofTimeunits = (FinalDate-InitialDate).days + 1\n",
    "  print(\"Total number of Days May Dataset \" + str(NumberofTimeunits)  + ' Dropping at start ' + str(Dropearlydata))\n",
    "\n",
    "  DATASETDIR = APPLDIR +'/EndMay2020fromfiles'\n",
    "\n",
    "  CasesFile = DATASETDIR + '/' + 'Covid19-cases-110USCities.csv'\n",
    "  DeathsFile = DATASETDIR + '/' + 'Covid19-deaths-110USCities.csv'\n",
    "  CovariatesFile = DATASETDIR + '/' + 'PVI-31July2020.csv'\n",
    "  if UseOLDCovariates:\n",
    "    CovariatesFile = DATASETDIR + '/' + 'Static_316USCities_Pop.csv'\n",
    "  LocationdataFile = DATASETDIR + '/' + 'Static_316USCities_Pop.csv'\n",
    "\n",
    "  Nloc = 110\n",
    "  NFIPS = 112\n",
    "\n",
    "if ReadAugust2020Covid or RereadMay2020:\n",
    "\n",
    "# Set up location information\n",
    "  Num_Time = NumberofTimeunits\n",
    "  Locationfips = np.empty(NFIPS, dtype=int) # integer version of FIPs\n",
    "  Locationcolumns = [] # String version of FIPS\n",
    "  FIPSintegerlookup = {}\n",
    "  FIPSstringlookup = {}\n",
    "  BasicInputTimeSeries = np.empty([Num_Time,Nloc,2],dtype = np.float32)\n",
    "\n",
    "# Read in  cases Data into BasicInputTimeSeries\n",
    "  with open(CasesFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      \n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)!=NumberofTimeunits + 1 + Dropearlydata):\n",
    "          printexit('EXIT: Incorrect row length Cases ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "        localfips = nextrow[0]\n",
    "        Locationcolumns.append(localfips)\n",
    "        Locationfips[iloc] = int(localfips)\n",
    "        FIPSintegerlookup[int(localfips)] = iloc\n",
    "        FIPSstringlookup[localfips] = iloc\n",
    "        for itime in range(0, NumberofTimeunits):\n",
    "          BasicInputTimeSeries[itime,iloc,0] = nextrow[itime + 1 + Dropearlydata]\n",
    "        iloc += 1\n",
    "# End Reading in cases data\n",
    "\n",
    "  if iloc != Nloc:\n",
    "          printexit('EXIT Inconsistent location lengths Cases ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Cases data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "\n",
    "# Read in deaths Data into BasicInputTimeSeries\n",
    "  with open(DeathsFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        if (len(nextrow)!=NumberofTimeunits + 1 + Dropearlydata):\n",
    "          printexit('EXIT: Incorrect row length Deaths ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "        localfips = nextrow[0]\n",
    "        if (Locationfips[iloc] != int(localfips)):\n",
    "          printexit('EXIT: Unexpected FIPS Deaths ' + localfips + ' ' +str(Locationfips[iloc]))\n",
    "        for itime in range(0, NumberofTimeunits):\n",
    "          BasicInputTimeSeries[itime,iloc,1] = nextrow[itime + 1 + Dropearlydata]\n",
    "        iloc += 1\n",
    "# End Reading in deaths data\n",
    "\n",
    "  if iloc != Nloc:\n",
    "    printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
    "  print('Read Deaths data locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time))\n",
    "\n",
    "# START setting location properties -- there are NFIPS of these\n",
    "# NFIPS can be larger than Nloc. Any fips in studied group must be in fips property group\n",
    "# Add missing FIPS in 315 and not 314 set are 49057  and 49053\n",
    "# while 48203 is in 314 but not 315; 316 adds 48023\n",
    "  Locationfips[Nloc] = 49057\n",
    "  Locationfips[Nloc+1] = 49053\n",
    "  Locationcolumns.append(str(Locationfips[Nloc]))\n",
    "  FIPSintegerlookup[Locationfips[Nloc]] = Nloc\n",
    "  FIPSstringlookup[str(Locationfips[Nloc])] = Nloc\n",
    "  Locationcolumns.append(str(Locationfips[Nloc+1]))\n",
    "  FIPSintegerlookup[Locationfips[Nloc+1]] = Nloc+1\n",
    "  FIPSstringlookup[str(Locationfips[Nloc+1])] = Nloc+1\n",
    "\n",
    "  Locationname = ['Empty'] * NFIPS\n",
    "  Locationstate = ['Empty'] * NFIPS\n",
    "  Locationpopulation = np.empty(NFIPS, dtype=int)\n",
    "  with open(LocationdataFile, 'r') as read_obj:\n",
    "      csv_reader = reader(read_obj)\n",
    "      header = next(csv_reader)\n",
    "      Ftype = header[0]\n",
    "      if Ftype != 'FIPS':\n",
    "        printexit('EXIT: Wrong file type ' + Ftype)\n",
    "\n",
    "      iloc = 0\n",
    "      for nextrow in csv_reader:\n",
    "        localfips = int(nextrow[0])\n",
    "        if localfips in FIPSintegerlookup.keys():\n",
    "          jloc = FIPSintegerlookup[localfips]\n",
    "          Locationname[jloc] = nextrow[2]\n",
    "          Locationstate[jloc] = nextrow[1]\n",
    "          Locationpopulation[jloc] = int(nextrow[5])\n",
    "          iloc += 1 # just counting lines\n",
    "       \n",
    "  if iloc != Nloc+2:\n",
    "    printexit('EXIT Inconsistent old static data lengths ' +str(iloc) + ' ' + str(Nloc+2))\n",
    "  if 48203 in FIPSintegerlookup.keys():\n",
    "    iloc = FIPSintegerlookup[48203]\n",
    "    Locationname[iloc] = 'Harrison'\n",
    "    Locationstate[iloc] = 'Texas'\n",
    "    Locationpopulation[iloc] = 66553\n",
    "# END setting NFIPS location properties\n",
    "\n",
    "# Set Static Properties of the Nloc studied locations\n",
    "# Order is Static, Dynamic, Cases, Deaths\n",
    "  if NIHCovariates:\n",
    "      NpropperTimeDynamic = 11\n",
    "      NpropperTimeStatic = 0\n",
    "  else:\n",
    "      NpropperTimeDynamic = 0\n",
    "      NpropperTimeStatic = 12\n",
    "      if UseOLDCovariates:\n",
    "        NpropperTimeStatic = 26\n",
    "  NpropperTime = NpropperTimeStatic + NpropperTimeDynamic + 2   \n",
    "  InputPropertyNames = [] * NpropperTime\n",
    "  Property_is_Intensive = np.full(NpropperTime, True, dtype = np.bool)\n",
    "\n",
    "  if not NIHCovariates:\n",
    "      BasicInputStaticProps = np.empty([Nloc,NpropperTimeStatic],dtype = np.float32)\n",
    "      \n",
    "      with open(CovariatesFile, 'r') as read_obj:\n",
    "          csv_reader = reader(read_obj)\n",
    "          header = next(csv_reader)\n",
    "          Ftype = header[0]\n",
    "          if Ftype != 'FIPS':\n",
    "              printexit('EXIT: Wrong file type ' + Ftype)\n",
    "          throwaway = 2\n",
    "          if UseOLDCovariates:\n",
    "            throwaway = 6\n",
    "          if ( len(header) != (throwaway+NpropperTimeStatic)):\n",
    "              printexit('EXIT: Incorrect property header length ' + str(len(header)) + ' ' +str(2+NpropperTimeStatic))\n",
    "          InputPropertyNames[:] = header[throwaway:]\n",
    "\n",
    "          iloc = 0\n",
    "          for nextrow in csv_reader:\n",
    "            if (len(nextrow)!= (throwaway+NpropperTimeStatic)):\n",
    "              printexit('EXIT: Incorrect row length ' + str(iloc) + ' ' + str(2+NpropperTimeStatic) + ' ' +str(len(nextrow)))\n",
    "            localfips = int(nextrow[0])\n",
    "            if not localfips in FIPSintegerlookup.keys():\n",
    "              continue\n",
    "    #           printexit('EXIT: Missing FIPS ' + str(localfips))\n",
    "            jloc = FIPSintegerlookup[localfips]\n",
    "            if jloc >= Nloc:\n",
    "              print('FIPS ' + str(localfips) + ' skipped in property read')\n",
    "              continue # skip this FIPS\n",
    "            BasicInputStaticProps[jloc,:] = np.asarray(nextrow[throwaway:], dtype=np.float32)\n",
    "            iloc += 1\n",
    "    # End Reading in Static Properties data\n",
    "\n",
    "      if iloc != Nloc:\n",
    "        printexit('EXIT Inconsistent location lengths ' +str(iloc) + ' ' + str(Nloc))\n",
    "      print('Read Static Properties for locations ' + str(Nloc) + ' Properties ' + str(NpropperTimeStatic))\n",
    "\n",
    "  # August Covariates all intensive and no missing data\n",
    "  # May Coviates have intensive properties missing for Harrison TX\n",
    "      if UseOLDCovariates:\n",
    "        Property_is_Intensive[20] = False\n",
    "        Property_is_Intensive[21] = False\n",
    "        Property_is_Intensive[22] = False\n",
    "\n",
    "# Finish this after NIH Covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMsHzx8Rk0QC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Clean up Extensive and Undefined Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EdsHkx7jLJX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read and setup NIH Covariates August 2020 and January, April 2021 Data\n",
    "\n",
    "new collection of time dependent covariates (even if constant).\n",
    "\n",
    "cases and deaths and location property from previous data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PeTJNz4jUPq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "if NIHCovariates:\n",
    "  if ReadJan2021Covid:\n",
    "    Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Comorbidities.csv\",\"Demographics.csv\", \"Disease Spread.csv\", \n",
    "                     \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Intervention Testing.csv\", \"Mobility.csv\", \n",
    "                     \"Residential Density.csv\", \"Social Distancing.csv\",  \"Transmissible Cases.csv\"]\n",
    "    Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\",  \"Demographics\", \"Disease Spread\", \n",
    "                 \"Health Disparities\", \"Hospital Beds\", \"Intervention Testing\", \"Mobility\", \"Residential Density\", \n",
    "                 \"Social Distancing\", \"Transmissible Cases\"]\n",
    "  \n",
    "  elif ReadApril2021Covid:\n",
    "    if ReadMay2022Covid:\n",
    "      if Read7dayCovid:\n",
    "        Propfilenames = [\"Age Distribution.csv\", \"Disease Spread.csv\", \n",
    "                      \"Health Disparities.csv\", \n",
    "                      \"Social Distancing.csv\",  \"Transmissible Cases.csv\", \"Vaccination.csv\", \"NOFILE\"]\n",
    "        Propnames = [\"Age Distribution\",  \"Disease Spread\", \n",
    "                  \"Health Disparities\",\n",
    "                  \"Social Distancing\", \"Transmissible Cases\", \"Vaccination\", \"voting\"]\n",
    "      else:\n",
    "        Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Co-morbidities.csv\",\"Pop Demographics.csv\", \"Disease Spread.csv\", \n",
    "                      \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Pop Mobility.csv\", \n",
    "                      \"Residential Density.csv\", \"Social Distancing.csv\", \"Testing.csv\", \"Transmissible Cases.csv\", \"Vaccination.csv\", \"VaccinationOneDose.csv\",\"NOFILE\"]\n",
    "        Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\",  \"Pop Demographics\", \"Disease Spread\", \n",
    "                  \"Health Disparities\", \"Hospital Beds\",  \"Pop Mobility\", \"Residential Density\", \n",
    "                  \"Social Distancing\", \"Testing\",\"Transmissible Cases\", \"Vaccination\", \"VaccinationOneDose\",\"voting\"]\n",
    "    else:\n",
    "      Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Comorbidities.csv\",\"Demographics.csv\", \"Disease Spread.csv\", \n",
    "                      \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Mobility.csv\", \n",
    "                      \"Residential Density.csv\", \"Social Distancing.csv\", \"Testing.csv\", \"Transmissible Cases.csv\",\"NOFILE\"]\n",
    "      Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\",  \"Demographics\", \"Disease Spread\", \n",
    "                  \"Health Disparities\", \"Hospital Beds\",  \"Mobility\", \"Residential Density\", \n",
    "                  \"Social Distancing\", \"Testing\",\"Transmissible Cases\",\"voting\"]\n",
    "      if ReadNov2021Covid:\n",
    "        Propfilenames.append(\"FullyVaccinated.csv\")\n",
    "        Propnames.append(\"Fully Vaccinated\")\n",
    "  else:\n",
    "    Propfilenames = [\"Age Distribution.csv\", \"Air Pollution.csv\", \"Co-morbidities.csv\", \"Health Disparities.csv\", \"Hospital Beds.csv\", \"Pop Demographics.csv\", \"Pop Mobility.csv\", \"Residential Density.csv\", \"Social Distancing.csv\", \"Testing.csv\", \"Transmissible Cases.csv\"]\n",
    "    Propnames = [\"Age Distribution\", \"Air Pollution\", \"Co-morbidities\", \"Health Disparities\", \"Hospital Beds\", \"Pop Demographics\", \"Pop Mobility\", \"Residential Density\", \"Social Distancing\", \"Testing\", \"Transmissible Cases\"]\n",
    "  \n",
    "  NIHDATADIR = DATASETDIR + '/' \n",
    "  numberfiles = len(Propnames)\n",
    "  NpropperTimeStatic = 0\n",
    "  if NpropperTimeDynamic != numberfiles:\n",
    "    printexit('EXIT: Dynamic Properties set wrong ' + str(numberfiles) + ' ' + str(NpropperTimeDynamic))\n",
    "  DynamicPropertyTimeSeries = np.zeros([Num_Time,Nloc,numberfiles],dtype = np.float32)\n",
    "  enddifference = NaN\n",
    "\n",
    "  for ifiles in range(0,numberfiles):\n",
    "    InputPropertyNames.append(Propnames[ifiles])\n",
    "    if Propfilenames[ifiles] == 'NOFILE': # Special case of Voting Data\n",
    "      for iloc in range(0,Nloc):\n",
    "        Demsize = DemVoting[iloc]\n",
    "        RepSize = RepVoting[iloc]\n",
    "        Votingcovariate = Demsize/(RepSize+Demsize)\n",
    "        DynamicPropertyTimeSeries[:,iloc,ifiles] = Votingcovariate\n",
    "      continue # over ifile loop\n",
    "\n",
    "    DynamicPropFile = NIHDATADIR + Propfilenames[ifiles]\n",
    "    if not (ReadJan2021Covid or ReadApril2021Covid):\n",
    "      DynamicPropFile = DATASETDIR + '/ThirdCovariates/' + Propfilenames[ifiles]\n",
    "\n",
    "    # Read in  Covariate Data into DynamicPropertyTimeSeries\n",
    "    with open(DynamicPropFile, 'r') as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        header = next(csv_reader)\n",
    "        skip = 1\n",
    "        if ReadNov2021Covid:\n",
    "          skip = 2\n",
    "          Ftype = header[1]\n",
    "          if Ftype != 'Name':\n",
    "            printexit('EXIT: Wrong file type ' + Ftype)\n",
    "          Ftype = header[0]\n",
    "          if Ftype != 'FIPS':\n",
    "            printexit('EXIT: Wrong file type ' + Ftype)\n",
    "        elif ReadMay2022Covid:\n",
    "          if Read7dayCovid:\n",
    "            skip = 2\n",
    "            Ftype = header[0]\n",
    "            if Ftype != 'Name':\n",
    "              printexit('EXIT: Wrong file type ' + Ftype)\n",
    "            Ftype = header[1]\n",
    "            if Ftype != 'FIPS':\n",
    "              printexit('EXIT: Wrong file type ' + Ftype) \n",
    "          else:\n",
    "            if (Propfilenames[ifiles] == \"Vaccination.csv\") or (Propfilenames[ifiles] == \"VaccinationOneDose.csv\") :\n",
    "              skip = 1\n",
    "              Ftype = header[0]\n",
    "              if Ftype != 'FIPS':\n",
    "                printexit('EXIT: Wrong file type ' + Ftype)\n",
    "            else:\n",
    "              skip = 2\n",
    "              Ftype = header[1]\n",
    "              if Ftype != 'Name':\n",
    "                printexit('EXIT: Wrong file type ' + Ftype)\n",
    "              Ftype = header[0]\n",
    "              if Ftype != 'FIPS':\n",
    "                printexit('EXIT: Wrong file type ' + Ftype)         \n",
    "        else:\n",
    "          if (ReadJan2021Covid or ReadApril2021Covid):\n",
    "            skip = 2\n",
    "            Ftype = header[0]\n",
    "            if Ftype != 'Name':\n",
    "              printexit('EXIT: Wrong file type ' + Ftype)\n",
    "          Ftype = header[skip-1]\n",
    "          if Ftype != 'FIPS':\n",
    "            printexit('EXIT: Wrong file type ' + Ftype)\n",
    "\n",
    "        # Check Date\n",
    "        hformat = '%m-%d-%Y'\n",
    "        if ReadJan2021Covid or ReadApril2021Covid:\n",
    "          hformat = '%Y-%m-%d'\n",
    "        if Propfilenames[ifiles] == \"FullyVaccinated.csv\" and ReadNov2021Covid:\n",
    "          hformat = '%m/%d/%Y'\n",
    "        print(str(ifiles)  + \" \" + str(skip) + \" \" + header[0] + \" \" +header[1] + ' ' + Propnames[ifiles])\n",
    "        stringdate = header[skip]\n",
    "        stringdatelist = re.findall('(.*) .*', stringdate)       \n",
    "        if stringdatelist:\n",
    "          stringdate = stringdatelist[0]\n",
    "        firstdate = datetime.strptime(stringdate, hformat)\n",
    "        tdelta = (firstdate-InitialDate).days \n",
    "        if tdelta > 0:\n",
    "          print(Propnames[ifiles] + ' Missing Covariate Data at start ' + str(tdelta))\n",
    "        stringdate = header[len(header)-1]\n",
    "        stringdatelist = re.findall('(.*) .*', stringdate)       \n",
    "        if stringdatelist:\n",
    "          stringdate = stringdatelist[0]\n",
    "        lastdate = datetime.strptime(stringdate, hformat)\n",
    "        enddifference1 = (FinalDate-lastdate).days\n",
    "        if math.isnan(enddifference):\n",
    "          enddifference = enddifference1\n",
    "          print(Propnames[ifiles] + ' Missing days at the end ' + str(enddifference))\n",
    "        else:\n",
    "          if enddifference != enddifference1:\n",
    "            print('Change in time length at end ' + Propnames[ifiles] + ' expected ' + str(enddifference) + ' actual ' +str(enddifference1))\n",
    "        iloc = 0\n",
    "        \n",
    "        for nextrow in csv_reader:\n",
    "          if (len(nextrow)!=NumberofTimeunits + skip -enddifference1-tdelta):\n",
    "            printexit('EXIT: Incorrect row length ' + Propnames[ifiles] + ' Location ' + str(iloc) + ' ' +str(len(nextrow)))\n",
    "          if ReadNov2021Covid or ReadMay2022Covid:\n",
    "            localfips = nextrow[0]\n",
    "            if Read7dayCovid:\n",
    "              localfips = nextrow[1]\n",
    "          else:\n",
    "            localfips = nextrow[skip-1]\n",
    "          intversion = int(localfips)\n",
    "          if intversion >  56045:\n",
    "            continue\n",
    "          jloc = FIPSstringlookup[localfips] \n",
    "          FinalTimeIndex = min(NumberofTimeunits - enddifference1,NumberofTimeunits)\n",
    "          FirstTimeIndex = max(tdelta,0)\n",
    "          for itime in range(FirstTimeIndex, FinalTimeIndex):\n",
    "            DynamicPropertyTimeSeries[itime,jloc,ifiles] = nextrow[itime + skip - tdelta]\n",
    "        # Use previous week value for missing data at the end\n",
    "          for itime in range(FinalTimeIndex, NumberofTimeunits):\n",
    "            DynamicPropertyTimeSeries[itime,jloc,ifiles] = DynamicPropertyTimeSeries[itime-7,jloc,ifiles]\n",
    "          iloc += 1\n",
    "# End Reading in dynamic property data\n",
    "\n",
    "    if iloc != Nloc:\n",
    "            printexit('EXIT Inconsistent location lengths ' + Propnames[ifiles] + str(iloc) + ' ' + str(Nloc))\n",
    "    if tdelta <= 0:\n",
    "      print('Read ' + Propnames[ifiles] + ' data for locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time) + ' Days dropped at start ' + str(-tdelta))\n",
    "    else: \n",
    "      print('Read ' + Propnames[ifiles] + ' data for locations ' + str(Nloc) + ' Time Steps ' + str(Num_Time) + ' zero value Days added at start ' + str(tdelta))\n",
    "\n",
    "  if ReadApril2021Covid:\n",
    "    CovidPopulationCut = 0 # Use this if NumberCut = 0\n",
    "    NumberCut = 2642\n",
    "    if Read7dayCovid:\n",
    "      NumberCut = 0\n",
    "    uselocation = np.full(Nloc, True, dtype = np.bool)\n",
    "    if (CovidPopulationCut > 0) or (NumberCut > 0):\n",
    "      if NumberCut >0:\n",
    "        smalllocations = np.argsort(Locationpopulation)\n",
    "        for jloc in range(0,NumberCut):\n",
    "          uselocation[smalllocations[jloc]] = False\n",
    "        CovidPopulationCut = Locationpopulation[smalllocations[NumberCut]]\n",
    "      else:\n",
    "        NumberCut =0\n",
    "        for iloc in range(0,Nloc):\n",
    "          if Locationpopulation[iloc] < CovidPopulationCut:\n",
    "            uselocation[iloc] = False\n",
    "            NumberCut += 1\n",
    "      print(' Population Cut ' + str(CovidPopulationCut) + ' removes ' + str(NumberCut) + ' of ' + str(Nloc))\n",
    "    if(NumberCut > 0):\n",
    "      NewNloc = Nloc - NumberCut\n",
    "      NewNFIPS = NewNloc\n",
    "      NewLocationfips = np.empty(NewNFIPS, dtype=int) # integer version of FIPs\n",
    "      NewLocationcolumns = [] # String version of FIPS\n",
    "      NewFIPSintegerlookup = {}\n",
    "      NewFIPSstringlookup = {}\n",
    "      NewBasicInputTimeSeries = np.empty([Num_Time,NewNloc,2],dtype = np.float32)\n",
    "      NewLocationname = ['Empty'] * NewNFIPS\n",
    "      NewLocationstate = ['Empty'] * NewNFIPS\n",
    "      NewLocationpopulation = np.empty(NewNFIPS, dtype=int)\n",
    "      NewDynamicPropertyTimeSeries = np.empty([Num_Time,NewNloc,numberfiles],dtype = np.float32) \n",
    "\n",
    "      Newiloc = 0\n",
    "      for iloc in range(0,Nloc):\n",
    "        if not uselocation[iloc]:\n",
    "          continue\n",
    "        NewBasicInputTimeSeries[:,Newiloc,:] = BasicInputTimeSeries[:,iloc,:]\n",
    "        NewDynamicPropertyTimeSeries[:,Newiloc,:] = DynamicPropertyTimeSeries[:,iloc,:]\n",
    "        localfips = Locationcolumns[iloc]\n",
    "        NewLocationcolumns.append(localfips)\n",
    "        NewLocationfips[Newiloc] = int(localfips)\n",
    "        NewFIPSintegerlookup[int(localfips)] = Newiloc\n",
    "        NewFIPSstringlookup[localfips] = Newiloc \n",
    "        NewLocationpopulation[Newiloc] = Locationpopulation[iloc]\n",
    "        NewLocationstate[Newiloc] = Locationstate[iloc]\n",
    "        NewLocationname[Newiloc] = Locationname[iloc]\n",
    "        Newiloc +=1\n",
    "\n",
    "      BasicInputTimeSeries = NewBasicInputTimeSeries\n",
    "      DynamicPropertyTimeSeries = NewDynamicPropertyTimeSeries\n",
    "      Locationname = NewLocationname\n",
    "      Locationstate = NewLocationstate\n",
    "      Locationpopulation = NewLocationpopulation\n",
    "      FIPSstringlookup = NewFIPSstringlookup\n",
    "      FIPSintegerlookup = NewFIPSintegerlookup\n",
    "      Locationcolumns = NewLocationcolumns\n",
    "      Locationfips = NewLocationfips\n",
    "      NFIPS = NewNFIPS\n",
    "      Nloc = NewNloc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awLjz1adEXr3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Process Input Data  in various ways\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrugyhFU66md",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Convert Cumulative to Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipjkf86A6imL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert  cumulative to Daily. \n",
    "# Replace negative daily values by zero\n",
    "# remove daily to sqrt(daily)  and Then normalize maximum to 1\n",
    "if ConvertDynamicPredictedQuantity:\n",
    "  NewBasicInputTimeSeries = np.empty_like(BasicInputTimeSeries, dtype=np.float32)\n",
    "  Zeroversion = np.zeros_like(BasicInputTimeSeries, dtype=np.float32)\n",
    "  Rolleddata = np.roll(BasicInputTimeSeries, 1, axis=0)\n",
    "  Rolleddata[0,:,:] = Zeroversion[0,:,:]\n",
    "  NewBasicInputTimeSeries = np.maximum(np.subtract(BasicInputTimeSeries,Rolleddata),Zeroversion)\n",
    "  originalnumber = np.sum(BasicInputTimeSeries[NumberofTimeunits-1,:,:],axis=0)\n",
    "  newnumber = np.sum(NewBasicInputTimeSeries,axis=(0,1))\n",
    "  print('Original summed counts ' + str(originalnumber) + ' become ' + str(newnumber)+ ' Cases, Deaths')\n",
    "\n",
    "  BasicInputTimeSeries = NewBasicInputTimeSeries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZLkGseQpGlr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Static and Dynamic specials for COVID\n",
    "\n",
    "except case where Romeo data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fhi0Ug84ehL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove special status of Cases and Deaths\n",
    "if CDSpecial:\n",
    "    \n",
    "  NewNpropperTimeDynamic = NpropperTimeDynamic + 2\n",
    "  NewNpropperTime = NpropperTimeStatic + NewNpropperTimeDynamic   \n",
    "\n",
    "  NewProperty_is_Intensive = np.full(NewNpropperTime, True, dtype = np.bool)\n",
    "  NewInputPropertyNames = []\n",
    "  NewDynamicPropertyTimeSeries = np.empty([Num_Time,Nloc,NewNpropperTimeDynamic],dtype = np.float32)\n",
    "\n",
    "  for casesdeaths in range(0,2):\n",
    "    NewDynamicPropertyTimeSeries[:,:,casesdeaths] = BasicInputTimeSeries[:,:,casesdeaths]\n",
    "  BasicInputTimeSeries = None\n",
    "\n",
    "  for iprop in range(0,NpropperTimeStatic):\n",
    "    NewInputPropertyNames.append(InputPropertyNames[iprop])\n",
    "    NewProperty_is_Intensive[iprop] = Property_is_Intensive[iprop]\n",
    "  NewProperty_is_Intensive[NpropperTimeStatic] = False\n",
    "  NewProperty_is_Intensive[NpropperTimeStatic+1] = False\n",
    "  NewInputPropertyNames.append('Cases')\n",
    "  NewInputPropertyNames.append('Deaths')\n",
    "  for ipropdynamic in range(0,NpropperTimeDynamic):\n",
    "    Newiprop = NpropperTimeStatic+2+ipropdynamic\n",
    "    iprop = NpropperTimeStatic+ipropdynamic\n",
    "    NewDynamicPropertyTimeSeries[:,:,Newiprop] = DynamicPropertyTimeSeries[:,:,iprop]\n",
    "    NewInputPropertyNames.append(InputPropertyNames[iprop])\n",
    "    NewProperty_is_Intensive[Newiprop] = Property_is_Intensive[iprop]\n",
    "  \n",
    "  NpropperTimeDynamic = NewNpropperTimeDynamic\n",
    "  NpropperTime = NewNpropperTime\n",
    "  DynamicPropertyTimeSeries = NewDynamicPropertyTimeSeries\n",
    "  InputPropertyNames = NewInputPropertyNames\n",
    "  Property_is_Intensive = NewProperty_is_Intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTNVFciYtS5Y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Static Property Manipulations for Covid Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5BPSUk7i377",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Execute under all COVID circumstances properties generated here\n",
    "if CDSpecial:\n",
    "  if NpropperTimeStatic > 0:\n",
    "    Num_Extensive = 0\n",
    "    for iprop in range(0,NpropperTimeStatic):\n",
    "      if not Property_is_Intensive[iprop]:\n",
    "        Num_Extensive +=1\n",
    "    print(startbold + startred + ' Number of Extensive parameters ' + str(Num_Extensive) + resetfonts)\n",
    "    for iprop in range(0,NpropperTimeStatic):\n",
    "      if not Property_is_Intensive[iprop]:\n",
    "        print(InputPropertyNames[iprop])\n",
    "\n",
    "    # Convert Extensive covariates to SQRT(Population normed)\n",
    "    # Replace negatives by mean of positives and zeroes\n",
    "    positivemean = np.zeros(NpropperTimeStatic, dtype = np.float32)\n",
    "    countvalidentries = np.zeros(NpropperTimeStatic, dtype = np.float32)\n",
    "    for iloc in range(0,Nloc):\n",
    "      for iprop in range(0,NpropperTimeStatic):\n",
    "        if not Property_is_Intensive[iprop]:\n",
    "          BasicInputStaticProps[iloc,iprop] = np.sqrt(BasicInputStaticProps[iloc,iprop]/Locationpopulation[iloc])\n",
    "        else:\n",
    "          if BasicInputStaticProps[iloc,iprop] >= 0:\n",
    "            positivemean[iprop] += BasicInputStaticProps[iloc,iprop]\n",
    "            countvalidentries[iprop] += 1.0\n",
    "\n",
    "    for iprop in range(0,NpropperTimeStatic):\n",
    "        if Property_is_Intensive[iprop]:\n",
    "          positivemean[iprop] /= countvalidentries[iprop]\n",
    "\n",
    "    for iloc in range(0,Nloc):\n",
    "      for iprop in range(0,NpropperTimeStatic):\n",
    "        if Property_is_Intensive[iprop]:\n",
    "          if BasicInputStaticProps[iloc,iprop] < 0:\n",
    "            BasicInputStaticProps[iloc,iprop] = positivemean[iprop]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSDyT65ly4Q-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Normalize All Static and Dynamic Properties\n",
    "\n",
    "for Static Properties BasicInputStaticProps[Nloc,NpropperTimeStatic] converts to NormedInputStaticProps[Nloc,NpropperTimeStatic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGbBzf47zv1m",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SetTakeroot(x,n):\n",
    "    if np.isnan(x):\n",
    "      return NaN   \n",
    "    if n == 3:\n",
    "      return np.cbrt(x)\n",
    "    elif n == 2:\n",
    "      if x <= 0.0:\n",
    "        return 0.0\n",
    "      return np.sqrt(x) \n",
    "    return x \n",
    "\n",
    "def DynamicPropertyScaling(InputTimeSeries):\n",
    "    Results = np.full(7, 0.0,dtype=np.float32)\n",
    "    Results[1] = np.nanmax(InputTimeSeries, axis = (0,1))\n",
    "    Results[0] = np.nanmin(InputTimeSeries, axis = (0,1))\n",
    "    Results[3] = np.nanmean(InputTimeSeries, axis = (0,1))\n",
    "    Results[4] = np.nanstd(InputTimeSeries, axis = (0,1))\n",
    "    Results[2] = np.reciprocal(np.subtract(Results[1],Results[0]))\n",
    "    Results[5] = np.multiply(Results[2],np.subtract(Results[3],Results[0]))\n",
    "    Results[6] = np.multiply(Results[2],Results[4])\n",
    "    return Results\n",
    "\n",
    "NpropperTimeMAX = NpropperTime + NumTimeSeriesCalculated  \n",
    "print('Static Proprties ',NpropperTimeStatic,' Total Number of Basic Properties Static+Dynamic ', NpropperTime,' Number of additional Calculated Properties ',NumTimeSeriesCalculated,' Absolute Total Properties ', NpropperTimeMAX)\n",
    "if ScaleProperties:\n",
    "  QuantityTakeroot = np.full(NpropperTimeMAX,1,dtype=np.int)\n",
    "  if Hydrology:\n",
    "    QuantityTakeroot[27] = 3\n",
    "    QuantityTakeroot[32] = 3\n",
    "  if CDSpecial:\n",
    "    if RootCasesDeaths:\n",
    "      print(' Cases and Deaths Square-rooted')\n",
    "      QuantityTakeroot[NpropperTimeStatic] =2\n",
    "      QuantityTakeroot[NpropperTimeStatic+1] =2\n",
    "    else:\n",
    "      print(' Cases and Deaths NOT Square-rooted')\n",
    "\n",
    "# Scale data by roots if requested\n",
    "  for iprop in range(0, NpropperTimeMAX):\n",
    "    if QuantityTakeroot[iprop] >= 2:\n",
    "      if iprop < NpropperTimeStatic:\n",
    "        for iloc in range(0,Nloc):\n",
    "          BasicInputStaticProps[iloc,iprop] = SetTakeroot(BasicInputStaticProps[iloc,iprop],QuantityTakeroot[iprop])\n",
    "      elif iprop < NpropperTime:\n",
    "        for itime in range(0,NumberofTimeunits):\n",
    "          for iloc in range(0,Nloc):\n",
    "            DynamicPropertyTimeSeries[itime,iloc,iprop-NpropperTimeStatic] = SetTakeroot(\n",
    "                DynamicPropertyTimeSeries[itime,iloc,iprop-NpropperTimeStatic],QuantityTakeroot[iprop])\n",
    "      else:\n",
    "        for itime in range(0,NumberofTimeunits):\n",
    "          for iloc in range(0,Nloc):\n",
    "            CalculatedTimeSeries[itime,iloc,iprop-NpropperTime] =SetTakeroot(\n",
    "                CalculatedTimeSeries[itime,iloc,iprop-NpropperTime],QuantityTakeroot[iprop])\n",
    "\n",
    "  QuantityStatisticsNames = ['Min','Max','Norm','Mean','Std','Normed Mean','Normed Std']\n",
    "  QuantityStatistics = np.zeros([NpropperTimeMAX,7], dtype=np.float32)\n",
    "  if NpropperTimeStatic > 0:  \n",
    "    print(BasicInputStaticProps.shape)\n",
    "    max_value = np.amax(BasicInputStaticProps, axis = 0)\n",
    "    min_value = np.amin(BasicInputStaticProps, axis = 0)\n",
    "    mean_value = np.mean(BasicInputStaticProps, axis = 0)\n",
    "    std_value = np.std(BasicInputStaticProps, axis = 0)\n",
    "    normval = np.reciprocal(np.subtract(max_value,min_value))\n",
    "    normed_mean = np.multiply(normval,np.subtract(mean_value,min_value))\n",
    "    normed_std = np.multiply(normval,std_value)\n",
    "    QuantityStatistics[0:NpropperTimeStatic,0] = min_value\n",
    "    QuantityStatistics[0:NpropperTimeStatic,1] = max_value\n",
    "    QuantityStatistics[0:NpropperTimeStatic,2] = normval\n",
    "    QuantityStatistics[0:NpropperTimeStatic,3] = mean_value\n",
    "    QuantityStatistics[0:NpropperTimeStatic,4] = std_value\n",
    "    QuantityStatistics[0:NpropperTimeStatic,5] = normed_mean\n",
    "    QuantityStatistics[0:NpropperTimeStatic,6] = normed_std\n",
    "\n",
    "    NormedInputStaticProps =np.empty_like(BasicInputStaticProps)\n",
    "    for iloc in range(0,Nloc):\n",
    "      NormedInputStaticProps[iloc,:] = np.multiply((BasicInputStaticProps[iloc,:] - min_value[:]),normval[:])\n",
    "\n",
    "  if (NpropperTimeDynamic > 0) or (NumTimeSeriesCalculated>0):\n",
    "    for iprop in range(NpropperTimeStatic,NpropperTimeStatic+NpropperTimeDynamic):\n",
    "      QuantityStatistics[iprop,:] = DynamicPropertyScaling(DynamicPropertyTimeSeries[:,:,iprop-NpropperTimeStatic])\n",
    "    for iprop in range(0,NumTimeSeriesCalculated):\n",
    "      QuantityStatistics[iprop+NpropperTime,:] = DynamicPropertyScaling(CalculatedTimeSeries[:,:,iprop]) \n",
    "\n",
    "    NormedDynamicPropertyTimeSeries = np.empty_like(DynamicPropertyTimeSeries)\n",
    "    for iprop in range(NpropperTimeStatic,NpropperTimeStatic+NpropperTimeDynamic):\n",
    "      NormedDynamicPropertyTimeSeries[:,:,iprop - NpropperTimeStatic] = np.multiply((DynamicPropertyTimeSeries[:,:,iprop - NpropperTimeStatic]\n",
    "                                                - QuantityStatistics[iprop,0]),QuantityStatistics[iprop,2])\n",
    "    \n",
    "    if NumTimeSeriesCalculated > 0:\n",
    "      NormedCalculatedTimeSeries = np.empty_like(CalculatedTimeSeries)\n",
    "      for iprop in range(NpropperTime,NpropperTimeMAX):\n",
    "        NormedCalculatedTimeSeries[:,:,iprop - NpropperTime] = np.multiply((CalculatedTimeSeries[:,:,iprop - NpropperTime]\n",
    "                                                - QuantityStatistics[iprop,0]),QuantityStatistics[iprop,2])\n",
    "      CalculatedTimeSeries = None\n",
    "  \n",
    "    BasicInputStaticProps = None\n",
    "    DynamicPropertyTimeSeries = None\n",
    "    print(startbold + \"Properties scaled\" +resetfonts)\n",
    "\n",
    "  line = 'Name   '\n",
    "  for propval in range (0,7):\n",
    "    line += QuantityStatisticsNames[propval] + '    '\n",
    "  print('\\n' + startbold +startpurple + line + resetfonts)\n",
    "  for iprop in range(0,NpropperTimeMAX):\n",
    "    if iprop == NpropperTimeStatic:\n",
    "      print('\\n')\n",
    "    line = startbold + startpurple + str(iprop) + ' ' + InputPropertyNames[iprop] + resetfonts  + ' Root ' + str(QuantityTakeroot[iprop])\n",
    "    for propval in range (0,7):\n",
    "      line += ' ' + str(round(QuantityStatistics[iprop,propval],3))\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW9bPWExf4YK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Set up Futures \n",
    "\n",
    "-- currently at unit time level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uwExtALgrsW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Future:\n",
    "    def __init__(self, name, daystart = 0, days =[], wgt=1.0, classweight = 1.0):\n",
    "        self.name = name\n",
    "        self.days = np.array(days)\n",
    "        self.daystart = daystart\n",
    "        self.wgts = np.full_like(self.days,wgt,dtype=float)\n",
    "        self.size = len(self.days)\n",
    "        self.classweight = classweight\n",
    "\n",
    "LengthFutures = 0\n",
    "Unit = \"Day\"\n",
    "if Earthquake:\n",
    "  Unit = \"2wk\"\n",
    "if GenerateFutures: \n",
    "\n",
    "  Futures =[]\n",
    "  daylimit = 14\n",
    "  if Earthquake:\n",
    "    daylimit = 25\n",
    "  for ifuture in range(0,daylimit):\n",
    "    xx = Future(Unit + 'x' + str(ifuture+2), days=[ifuture+2])\n",
    "    Futures.append(xx)\n",
    "  LengthFutures = len(Futures)\n",
    "  Futuresmaxday = 0\n",
    "  Futuresmaxweek = 0\n",
    "  for i in range(0,LengthFutures):\n",
    "      j = len(Futures[i].days)\n",
    "      if j == 1:\n",
    "          Futuresmaxday = max(Futuresmaxday, Futures[i].days[0])\n",
    "      else:\n",
    "          Futuresmaxweek = max(Futuresmaxweek, Futures[i].days[j-1])\n",
    "      Futures[i].daystart -= Dropearlydata\n",
    "      if Futures[i].daystart < 0: Futures[i].daystart = 0\n",
    "      if Earthquake:\n",
    "        Futures[i].daystart = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdm4DDFL92NJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Set up mappings of locations\n",
    "\n",
    "In next cell, we map locations for BEFORE location etc added\n",
    "\n",
    "In cell after that we do same for sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRZm-x13980a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OriginalNloc = Nloc\n",
    "if Earthquake:\n",
    "  MapLocation = True\n",
    "\n",
    "  MappedDynamicPropertyTimeSeries =  np.empty([Num_Time,MappedNloc,NpropperTimeDynamic],dtype = np.float32)\n",
    "  MappedNormedInputStaticProps = np.empty([MappedNloc,NpropperTimeStatic],dtype = np.float32)\n",
    "  MappedCalculatedTimeSeries =  np.empty([Num_Time,MappedNloc,NumTimeSeriesCalculated],dtype = np.float32)\n",
    "  \n",
    "  print(LookupLocations)\n",
    "  MappedDynamicPropertyTimeSeries[:,:,:] = NormedDynamicPropertyTimeSeries[:,LookupLocations,:]\n",
    "  NormedDynamicPropertyTimeSeries = None\n",
    "  NormedDynamicPropertyTimeSeries = MappedDynamicPropertyTimeSeries\n",
    "  \n",
    "  MappedCalculatedTimeSeries[:,:,:] = NormedCalculatedTimeSeries[:,LookupLocations,:]\n",
    "  NormedCalculatedTimeSeries = None\n",
    "  NormedCalculatedTimeSeries = MappedCalculatedTimeSeries\n",
    "\n",
    "  MappedNormedInputStaticProps[:,:] = NormedInputStaticProps[LookupLocations,:]\n",
    "  NormedInputStaticProps = None\n",
    "  NormedInputStaticProps = MappedNormedInputStaticProps\n",
    "\n",
    "  Nloc = MappedNloc\n",
    "  if GarbageCollect:\n",
    "    gc.collect()\n",
    "  print('Number of locations reduced to ' + str(Nloc))\n",
    "\n",
    "else:\n",
    "  MappedLocations = np.arange(0,Nloc, dtype=np.int)\n",
    "  LookupLocations = np.arange(0,Nloc, dtype=np.int)\n",
    "  MappedNloc = Nloc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTUIpVT3vris",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Property and Prediction  Data Structures\n",
    "\n",
    "Two important Lists Properties and Predictions that are related\n",
    "\n",
    " * Data stored in series is for properties, the calculated value occuring at or ending that day\n",
    " * For predictions, the data is the calculated value from that date or later. \n",
    "\n",
    " * We store data labelled by time so that\n",
    "  * for inputs we use time 0 upto last value - 1 i.e. position [length of array - 1]\n",
    "  * for outputs (predictions) with sequence Tseq, we use array locations [Tseq] to [length of array -1]\n",
    "  * This implies Num_Seq = Num_Time - Tseq\n",
    "\n",
    "\n",
    "**Properties**\n",
    "\n",
    "Everything appears in Property list -- both input and output (predicted)\n",
    "DynamicPropertyTimeSeries holds input property time series where value is value at that time using data before this time for aggregations\n",
    "  * NpropperTimeStatic is the number of static properties -- typically read in or calculated from input information\n",
    "  * NpropperTimeDynamicInput is total number of input time series\n",
    "  * NpropperTimeDynamicCalculated is total number of calculated dynamic quantities  used in Time series analysis as input properties and/or output predictions\n",
    "  * NpropperTimeDynamic = NpropperTimeDynamicInput + NpropperTimeDynamicCalculated ONLY includes input properties\n",
    "  * NpropperTime = NpropperTimeStatic + NpropperTimeDynamic will not include futures and NOT include calculated predictions\n",
    "  * InputPropertyNames is a list of size NpropperTime holding names\n",
    "  * NpropperTimeMAX = NpropperTime + NumTimeSeriesCalculated has calculated predictions following input properties ignoring futures \n",
    "  * QuantityStatistics has 7 statistics used in normalizing for NpropperTimeMAX properties\n",
    "  * Normalization takes NpropperTimeStatic static features in BasicInputStaticProps and stores in NormedInputStaticProps\n",
    "  * Normalization takes NpropperTimeDynamicInput dynamic features in BasicInputTimeSeries and stores in NormedInputTimeSeries\n",
    "  * Normalization takes NpropperTimeDynamicCalculated dynamic features in DynamicPropertyTimeSeries and stores in NormedDynamicPropertyTimeSeries\n",
    "\n",
    "**Predictions**\n",
    "\n",
    " * NumpredbasicperTime can be 1 upto NpropperTimeDynamic and are part of dynamic input series. It includes input values that are to be predicted (these MUST be at start) plus NumTimeSeriesCalculated calculated series\n",
    " * NumpredFuturedperTime is <= NumpredbasicperTime and is the number of input dynamic series that are futured\n",
    " * NumTimeSeriesCalculated is number of calculated (not as futures) time series stored in CalculatedTimeSeries and names in NamespredCalculated\n",
    " * Typically NumpredbasicperTime = NumTimeSeriesCalculated + NumpredFuturedperTime (**Currently this is assumed**)\n",
    " * Normalization takes NumTimeSeriesCalculated calculated series in CalculatedTimeSeries and stores in NormedCalculatedTimeSeries\n",
    " * Predictions per Time are  NpredperTime = NumpredbasicperTime + NumpredFuturedperTime*LengthFutures\n",
    " * Predictions per sequence Npredperseq = NpredperTime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGvEtAj5xHhR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set Requested Properties Predictions Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lycrtgBHxQCq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FuturePred = -1 Means NO FUTURE >= 0 FUTURED\n",
    "# BASIC EARTHQUAKE SET JUST LOG ENERGY AND MULTIPLICITY\n",
    "if Earthquake:\n",
    "  InputSource = ['Static','Static','Static','Static','Dynamic','Dynamic','Dynamic','Dynamic'\n",
    "    ,'Dynamic','Dynamic','Dynamic','Dynamic','Dynamic']\n",
    "  InputSourceNumber = [0,1,2,3,0,1,2,3,4,5,6,7,8]\n",
    "  if addRundleEMA > 0:\n",
    "    InputSource += ['Dynamic']*addRundleEMA\n",
    "    for i in range(0,addRundleEMA):\n",
    "      InputSourceNumber += [15+i]\n",
    "\n",
    "  PredSource = ['Dynamic','Calc','Calc','Calc','Calc','Calc','Calc','Calc','Calc','Calc']\n",
    "  PredSourceNumber = [0,0,1,2,3,4,5,6,7,8]\n",
    "  if addRundleEMA > 0:\n",
    "    PredSource += ['Calc']*addRundleEMA\n",
    "    for i in range(0,addRundleEMA):\n",
    "      PredSourceNumber += [19+i]\n",
    "\n",
    "  FuturedPred = [-1]*len(PredSource)\n",
    "\n",
    "  # Earthquake Space-Time\n",
    "  SpaceTimeEncodingPropTypes = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','BottomUp','BottomUp','BottomUp','BottomUp']\n",
    "  SpaceTimeEncodingPropValues = [0, 0, 1, 2, 3,4, 8,16,32,64]\n",
    "\n",
    "  SpaceTimeEncodingPredTypes =  ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','BottomUp','BottomUp','BottomUp','BottomUp']\n",
    "  SpaceTimeEncodingPredValues = [0, 0, 1, 2, 3,4, 8,16,32,64]\n",
    "\n",
    "  if UseTFTModel:\n",
    "    InputSource = ['Static','Static','Static','Static','Dynamic','Dynamic','Dynamic','Dynamic'\n",
    "      ,'Dynamic','Dynamic','Dynamic','Dynamic','Dynamic']\n",
    "    InputSourceNumber = [0,1,2,3,0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "    PredSource = ['Dynamic','Dynamic']\n",
    "    PredSourceNumber = [0,7]\n",
    "    FuturedPred = [1,1]\n",
    "    \n",
    "    SpaceTimeEncodingPredTypes =[]\n",
    "    SpaceTimeEncodingPredValues = []\n",
    "\n",
    "\n",
    "    #TFT2 1 year\n",
    "    PredSource = ['Dynamic','Dynamic','Dynamic','Dynamic']\n",
    "    PredSourceNumber = [0,6,7,8]\n",
    "    FuturedPred = [1,1,1,1]\n",
    "\n",
    "#Hydrology\n",
    "# Last Dynamic Variable predicted but not input as has undefined variables\n",
    "if Hydrology:\n",
    "  InputSource = ['Static']*27 + ['Dynamic']*5\n",
    "  InputSourceNumber = list(range(0,27)) + list(range(0,5))\n",
    "\n",
    "  PredSource = ['Dynamic']*6\n",
    "  PredSourceNumber = list(range(0,6))\n",
    "  FuturedPred = [-1]*len(PredSource)\n",
    "\n",
    "  # Hydrology Space-Time\n",
    "  SpaceTimeEncodingPropTypes = ['Annual', 'TopDown', 'Spatial']\n",
    "  SpaceTimeEncodingPropValues = [0, 1, 0]\n",
    "\n",
    "  SpaceTimeEncodingPredTypes = ['Annual', 'TopDown', 'Spatial']\n",
    "  SpaceTimeEncodingPredValues = [0, 1, 0]\n",
    "   \n",
    "# Recent Covid Default\n",
    "if ReadApril2021Covid:\n",
    "  if ReadNov2021Covid:\n",
    "    InputSource = ['Dynamic']*16\n",
    "    InputSourceNumber = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]  \n",
    "  elif ReadMay2022Covid:\n",
    "    if Read7dayCovid:\n",
    "      InputSource = ['Dynamic']*8 # NOT voting read in\n",
    "      InputSourceNumber = [0,1,2,3,4,5,6,7]\n",
    "    else:\n",
    "      InputSource = ['Dynamic']*17\n",
    "      InputSourceNumber = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]      \n",
    "  else:\n",
    "    InputSource = ['Dynamic']*15\n",
    "    InputSourceNumber = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "\n",
    "  if RunName == 'Covid7day22-LSTM8': # DONT PREDICT DEATHS\n",
    "    PredSource = ['Dynamic']\n",
    "    PredSourceNumber = [0]\n",
    "    FuturedPred = [1]\n",
    "  else:\n",
    "    PredSource = ['Dynamic','Dynamic']\n",
    "    PredSourceNumber = [0,1]\n",
    "    FuturedPred = [1,1]\n",
    "\n",
    "  # Encodings\n",
    "  SpaceTimeEncodingPropTypes = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','Weekly']\n",
    "  SpaceTimeEncodingPropValues = [0, 0, 1, 2, 3,4, 0]\n",
    "\n",
    "  SpaceTimeEncodingPredTypes = Types = ['Spatial', 'TopDown', 'TopDown','TopDown','TopDown','TopDown','Weekly']\n",
    "  SpaceTimeEncodingPredValues = [0, 0, 1, 2, 3,4, 0]\n",
    "\n",
    "  if RunName == 'Covid7day22-LSTM4R':\n",
    "    SpaceTimeEncodingPropTypes = ['Spatial', 'Weekly']\n",
    "    SpaceTimeEncodingPropValues = [0, 0]\n",
    "\n",
    "    SpaceTimeEncodingPredTypes = Types = ['Spatial', 'Weekly']\n",
    "    SpaceTimeEncodingPredValues = [0,  0]\n",
    "\n",
    "  if UseTFTModel:\n",
    "    SpaceTimeEncodingPredTypes =[]\n",
    "    SpaceTimeEncodingPredValues = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZbYR4a2lGCe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Choose Input and Predicted Quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXz5CLaOlOnn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if len(InputSource) != len(InputSourceNumber):\n",
    "  printexit(' Inconsistent Source Lengths ' + str(len(InputSource)) + ' ' +str(len(InputSourceNumber)) )\n",
    "if len(PredSource) != len(PredSourceNumber):\n",
    "  printexit(' Inconsistent Prediction Lengths ' + str(len(PredSource)) + ' ' + str(len(PredSourceNumber)) )\n",
    "\n",
    "# Executed by all even if GenerateFutures false except for direct Romeo data\n",
    "if (not ReadJuly2020Covid) and not (ReadJan2021Covid) and not (ReadApril2021Covid):\n",
    "  if not UseFutures:\n",
    "      LengthFutures = 0\n",
    "  print(startbold + \"Number of Futures -- separate for each regular prediction \" +str(LengthFutures) + resetfonts)\n",
    "  Usedaystart = False\n",
    "\n",
    "if len(PredSource) > 0: # set up Predictions\n",
    "  NumpredbasicperTime = len(PredSource)\n",
    "  FuturedPointer = np.full(NumpredbasicperTime,-1,dtype=np.int)\n",
    "  NumpredFuturedperTime = 0\n",
    "  NumpredfromInputsperTime = 0\n",
    "  for ipred in range(0,len(PredSource)):\n",
    "    if PredSource[ipred] == 'Dynamic':\n",
    "      NumpredfromInputsperTime += 1\n",
    "  countinputs = 0\n",
    "  countcalcs = 0\n",
    "  for ipred in range(0,len(PredSource)):\n",
    "    if not(PredSource[ipred] == 'Dynamic' or PredSource[ipred] == 'Calc'):\n",
    "      printexit('Illegal Prediction ' + str(ipred) + ' ' + PredSource[ipred])\n",
    "    if PredSource[ipred] == 'Dynamic':\n",
    "      countinputs += 1 \n",
    "    else:\n",
    "      countcalcs += 1\n",
    "    if FuturedPred[ipred] >= 0:\n",
    "      if LengthFutures > 0:\n",
    "        FuturedPred[ipred] = NumpredFuturedperTime\n",
    "        FuturedPointer[ipred] = NumpredFuturedperTime\n",
    "        NumpredFuturedperTime += 1\n",
    "      else:\n",
    "        FuturedPred[ipred] = -1\n",
    "\n",
    "else: # Set defaults\n",
    "  NumpredfromInputsperTime = NumpredFuturedperTime\n",
    "  FuturedPointer = np.full(NumpredbasicperTime,-1,dtype=np.int)\n",
    "  PredSource =[]\n",
    "  PredSourceNumber = []\n",
    "  FuturedPred =[]\n",
    "  futurepos = 0\n",
    "  for ipred in range(0,NumpredFuturedperTime): \n",
    "    PredSource.append('Dynamic')\n",
    "    PredSourceNumber.append(ipred)\n",
    "    futured = -1\n",
    "    if LengthFutures > 0:\n",
    "      futured = futurepos\n",
    "      FuturedPointer[ipred] = futurepos\n",
    "      futurepos += 1\n",
    "    FuturedPred.append(futured)\n",
    "  for ipred in range(0,NumTimeSeriesCalculated):\n",
    "    PredSource.append('Calc')\n",
    "    PredSourceNumber.append(ipred)\n",
    "    FuturedPred.append(-1) \n",
    "  print('Number of Predictions ' + str(len(PredSource)))   \n",
    "\n",
    "\n",
    "PropertyNameIndex = np.empty(NpropperTime, dtype = np.int32)\n",
    "PropertyAverageValuesPointer = np.empty(NpropperTime, dtype = np.int32)\n",
    "for iprop in range(0,NpropperTime):\n",
    "  PropertyNameIndex[iprop] = iprop # names\n",
    "  PropertyAverageValuesPointer[iprop] = iprop # normalizations\n",
    "\n",
    "# Reset Source -- if OK as read don't set InputSource InputSourceNumber\n",
    "# Reset NormedDynamicPropertyTimeSeries and NormedInputStaticProps\n",
    "# Reset NpropperTime = NpropperTimeStatic + NpropperTimeDynamic\n",
    "if len(InputSource) > 0: # Reset Input Source\n",
    "  NewNpropperTimeStatic = 0\n",
    "  NewNpropperTimeDynamic = 0\n",
    "  for isource in range(0,len(InputSource)):\n",
    "    if InputSource[isource] == 'Static':\n",
    "      NewNpropperTimeStatic += 1\n",
    "    if InputSource[isource] == 'Dynamic':\n",
    "      NewNpropperTimeDynamic += 1\n",
    "  NewNormedDynamicPropertyTimeSeries = np.empty([Num_Time,Nloc,NewNpropperTimeDynamic],dtype = np.float32)  \n",
    "  NewNormedInputStaticProps = np.empty([Nloc,NewNpropperTimeStatic],dtype = np.float32)\n",
    "  NewNpropperTime = NewNpropperTimeStatic + NewNpropperTimeDynamic\n",
    "  NewPropertyNameIndex = np.empty(NewNpropperTime, dtype = np.int32)\n",
    "  NewPropertyAverageValuesPointer = np.empty(NewNpropperTime, dtype = np.int32)\n",
    "  countstatic = 0\n",
    "  countdynamic = 0\n",
    "  for isource in range(0,len(InputSource)):\n",
    "    if InputSource[isource] == 'Static':\n",
    "      OldstaticNumber = InputSourceNumber[isource]\n",
    "      NewNormedInputStaticProps[:,countstatic] = NormedInputStaticProps[:,OldstaticNumber]\n",
    "      NewPropertyNameIndex[countstatic] = PropertyNameIndex[OldstaticNumber]\n",
    "      NewPropertyAverageValuesPointer[countstatic] = PropertyAverageValuesPointer[OldstaticNumber]\n",
    "      countstatic += 1\n",
    "\n",
    "    elif InputSource[isource] == 'Dynamic':\n",
    "      OlddynamicNumber =InputSourceNumber[isource]\n",
    "      NewNormedDynamicPropertyTimeSeries[:,:,countdynamic] = NormedDynamicPropertyTimeSeries[:,:,OlddynamicNumber]\n",
    "      NewPropertyNameIndex[countdynamic+NewNpropperTimeStatic] = PropertyNameIndex[OlddynamicNumber+NpropperTimeStatic]\n",
    "      NewPropertyAverageValuesPointer[countdynamic+NewNpropperTimeStatic] = PropertyAverageValuesPointer[OlddynamicNumber+NpropperTimeStatic]\n",
    "      countdynamic += 1\n",
    "    \n",
    "    else:\n",
    "     printexit('Illegal Property ' + str(isource) + ' ' + InputSource[isource]) \n",
    "\n",
    "else: # pretend data altered\n",
    "  NewPropertyNameIndex = PropertyNameIndex\n",
    "  NewPropertyAverageValuesPointer = PropertyAverageValuesPointer\n",
    "  NewNpropperTime = NpropperTime\n",
    "  NewNpropperTimeStatic = NpropperTimeStatic\n",
    "  NewNpropperTimeDynamic = NpropperTimeDynamic\n",
    "\n",
    "  NewNormedInputStaticProps = NormedInputStaticProps\n",
    "  NewNormedDynamicPropertyTimeSeries = NormedDynamicPropertyTimeSeries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb8-aCUg3Ry5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Calculate Futures\n",
    "\n",
    "Start Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx4PkF7nkLu_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Order of Predictions *****************************\n",
    "# Basic \"futured\" Predictions from property dynamic arrays\n",
    "# Additional predictions without futures and NOT in property arrays including Calculated time series\n",
    "# LengthFutures predictions for first NumpredFuturedperTime predictions\n",
    "# Special predictions (temporal, positional) added later\n",
    "NpredperTime = NumpredbasicperTime + NumpredFuturedperTime*LengthFutures\n",
    "Npredperseq = NpredperTime\n",
    "Predictionbasicname = [' '] * NumpredbasicperTime\n",
    "for ipred in range(0,NumpredbasicperTime):\n",
    "  if PredSource[ipred] == 'Dynamic':\n",
    "    Predictionbasicname[ipred] = InputPropertyNames[PredSourceNumber[ipred]+NpropperTimeStatic]\n",
    "  else:\n",
    "    Predictionbasicname[ipred]= NamespredCalculated[PredSourceNumber[ipred]]\n",
    "\n",
    "TotalFutures = 0\n",
    "if NumpredFuturedperTime <= 0:\n",
    "  GenerateFutures = False\n",
    "if GenerateFutures:\n",
    "  TotalFutures = NumpredFuturedperTime * LengthFutures\n",
    "print(startbold + 'Predictions Total ' + str(Npredperseq) + ' Basic ' + str(NumpredbasicperTime) + ' Of which futured are '\n",
    "  + str(NumpredFuturedperTime) + ' Giving number explicit futures ' + str(TotalFutures) + resetfonts )\n",
    "Predictionname = [' '] * Npredperseq\n",
    "Predictionnametype = [' '] * Npredperseq\n",
    "Predictionoldvalue = np.empty(Npredperseq, dtype=int)\n",
    "Predictionnewvalue = np.empty(Npredperseq, dtype=int)\n",
    "Predictionday = np.empty(Npredperseq, dtype=int)\n",
    "PredictionAverageValuesPointer = np.empty(Npredperseq, dtype=int)\n",
    "Predictionwgt = [1.0] * Npredperseq\n",
    "for ipred in range(0,NumpredbasicperTime):\n",
    "  Predictionnametype[ipred] = PredSource[ipred]\n",
    "  Predictionoldvalue[ipred] = PredSourceNumber[ipred]\n",
    "  Predictionnewvalue[ipred] = ipred\n",
    "  if PredSource[ipred] == 'Dynamic':\n",
    "    PredictionAverageValuesPointer[ipred] = NpropperTimeStatic + Predictionoldvalue[ipred]\n",
    "  else:\n",
    "    PredictionAverageValuesPointer[ipred] = NpropperTime + PredSourceNumber[ipred]\n",
    "  Predictionwgt[ipred] = 1.0\n",
    "  Predictionday[ipred] = 1\n",
    "  extrastring =''\n",
    "  Predictionname[ipred] = 'Next ' + Predictionbasicname[ipred]\n",
    "  if FuturedPred[ipred] >= 0:\n",
    "    extrastring = ' Explicit Futures Added '   \n",
    "  print(str(ipred)+  ' Internal Property # ' + str(PredictionAverageValuesPointer[ipred]) + ' ' + Predictionname[ipred]\n",
    "      + ' Weight ' + str(round(Predictionwgt[ipred],3)) + ' Day ' + str(Predictionday[ipred]) + extrastring )\n",
    "\n",
    "for ifuture in range(0,LengthFutures):\n",
    "  for ipred in range(0,NumpredbasicperTime):\n",
    "    if FuturedPred[ipred] >= 0:\n",
    "      FuturedPosition = NumpredbasicperTime + NumpredFuturedperTime*ifuture + FuturedPred[ipred]\n",
    "      Predictionname[FuturedPosition] = Predictionbasicname[ipred] + ' ' + Futures[ifuture].name\n",
    "      Predictionday[FuturedPosition] = Futures[ifuture].days[0]\n",
    "      Predictionwgt[FuturedPosition] = Futures[ifuture].classweight\n",
    "      Predictionnametype[FuturedPosition] = Predictionnametype[ipred]\n",
    "      Predictionoldvalue[FuturedPosition] = Predictionoldvalue[ipred]\n",
    "      Predictionnewvalue[FuturedPosition] = Predictionnewvalue[ipred]\n",
    "      PredictionAverageValuesPointer[FuturedPosition] = PredictionAverageValuesPointer[ipred]\n",
    "      print(str(iprop)+  ' Internal Property # ' + str(PredictionAverageValuesPointer[FuturedPosition]) + ' ' + \n",
    "        Predictionname[FuturedPosition] + ' Weight ' + str(round(Predictionwgt[FuturedPosition],3))\n",
    "         + ' Day ' + str(Predictionday[FuturedPosition]) + ' This is Explicit Future ')\n",
    "\n",
    "Predictionnamelookup = {}\n",
    "print(startbold + '\\nBasic Predicted Quantities' + resetfonts)\n",
    "for ipred in range(0,Npredperseq):\n",
    "  Predictionnamelookup[Predictionname[ipred]] = ipred\n",
    "\n",
    "  iprop = Predictionnewvalue[ipred]\n",
    "  line = startbold + startred + Predictionbasicname[iprop]\n",
    "  line += ' Weight ' + str(round(Predictionwgt[ipred],4))\n",
    "  if (iprop < NumpredFuturedperTime) or (iprop >= NumpredbasicperTime):\n",
    "    line += ' Day= ' + str(Predictionday[ipred])\n",
    "    line += ' Name ' + Predictionname[ipred]\n",
    "  line += resetfonts\n",
    "  jpred = PredictionAverageValuesPointer[ipred]\n",
    "  line += ' Processing Root ' + str(QuantityTakeroot[jpred])\n",
    "  for proppredval in range (0,7):\n",
    "      line += ' ' + QuantityStatisticsNames[proppredval] + ' ' + str(round(QuantityStatistics[jpred,proppredval],3))\n",
    "  print(wraptotext(line,size=150))\n",
    "  \n",
    "  print(line)\n",
    "\n",
    "  # Note that only Predictionwgt and Predictionname defined for later addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4V0SXGd-nVfX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set up Predictions \n",
    "\n",
    "first for time arrays; we will extend to sequences next. Sequences include the predictions for final time in sequence.\n",
    "\n",
    "This is prediction for sequence ending one day before the labelling time index. So sequence must end one unit before last time value\n",
    "\n",
    "Note this is  \"pure forecast\" which are of quantities used in driving data allowing us to iitialize prediction to input\n",
    "\n",
    "NaN represents non existent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXMefJVkkFL7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PredictionsfromInputs:\n",
    "  InputPredictionsbyTime = np.zeros([Num_Time, Nloc, Npredperseq], dtype = np.float32)\n",
    "  for ipred in range (0,NumpredbasicperTime):\n",
    "    if Predictionnametype[ipred] == 'Dynamic':\n",
    "      InputPredictionsbyTime[:,:,ipred] = NormedDynamicPropertyTimeSeries[:,:,Predictionoldvalue[ipred]]\n",
    "    else:\n",
    "      InputPredictionsbyTime[:,:,ipred] = NormedCalculatedTimeSeries[:,:,Predictionoldvalue[ipred]]\n",
    "\n",
    "  # Add Futures based on Futured properties\n",
    "  if LengthFutures > 0:\n",
    "    NaNall = np.full([Nloc],NaN,dtype = np.float32)\n",
    "    daystartveto = 0\n",
    "    atendveto = 0\n",
    "    allok = NumpredbasicperTime \n",
    "    for ifuture in range(0,LengthFutures):\n",
    "      for itime in range(0,Num_Time):\n",
    "        ActualTime = itime+Futures[ifuture].days[0]-1\n",
    "        if ActualTime >= Num_Time:\n",
    "          for ipred in range (0,NumpredbasicperTime):\n",
    "            Putithere = FuturedPred[ipred]\n",
    "            if Putithere >=0:\n",
    "              InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] = NaNall\n",
    "          atendveto +=1\n",
    "        elif Usedaystart and (itime < Futures[ifuture].daystart):\n",
    "          for ipred in range (0,NumpredbasicperTime):\n",
    "            Putithere = FuturedPred[ipred]\n",
    "            if Putithere >=0:\n",
    "              InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] = NaNall \n",
    "          daystartveto +=1     \n",
    "        else:\n",
    "          for ipred in range (0,NumpredbasicperTime):\n",
    "            Putithere = FuturedPred[ipred]\n",
    "            if Putithere >=0:\n",
    "              if Predictionnametype[ipred] == 'Dynamic':\n",
    "                InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] \\\n",
    "                  = NormedDynamicPropertyTimeSeries[ActualTime,:,Predictionoldvalue[ipred]]\n",
    "              else:\n",
    "                InputPredictionsbyTime[itime,:,NumpredbasicperTime + NumpredFuturedperTime*ifuture + Putithere] \\\n",
    "                  = NormedCalculatedTimeSeries[ActualTime,:,Predictionoldvalue[ipred]]\n",
    "          allok += NumpredFuturedperTime\n",
    "    print(startbold + 'Futures Added: Predictions set from inputs OK ' +str(allok) + \n",
    "          ' Veto at end ' + str(atendveto) +  ' Veto at start ' + str(daystartveto) + ' Times number of locations' + resetfonts)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlGIiaIWIrYm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Clean-up Input quantities#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Gq6G5JjIw_g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def checkNaN(y):\n",
    "  countNaN = 0\n",
    "  countnotNaN = 0\n",
    "  ctprt = 0\n",
    "  if y is None:\n",
    "    return\n",
    "  if len(y.shape) == 2:\n",
    "    for i in range(0,y.shape[0]):\n",
    "        for j in range(0,y.shape[1]):\n",
    "            if(np.math.isnan(y[i,j])):\n",
    "                countNaN += 1\n",
    "            else:\n",
    "                countnotNaN += 1\n",
    "  else:\n",
    "    for i in range(0,y.shape[0]):\n",
    "      for j in range(0,y.shape[1]):\n",
    "        for k in range(0,y.shape[2]):\n",
    "          if(np.math.isnan(y[i,j,k])):\n",
    "              countNaN += 1\n",
    "              ctprt += 1\n",
    "              if ctprt <= 10:\n",
    "                print('NaN Data ' + str(i) + ' ' + str(j) + ' ' + str(k))\n",
    "          else:\n",
    "              countnotNaN += 1\n",
    "\n",
    "  percent = (100.0*countNaN)/(countNaN + countnotNaN)\n",
    "  print(' is NaN ',str(countNaN),' percent ',str(round(percent,2)),' not NaN ', str(countnotNaN))\n",
    "\n",
    "# Clean-up Input Source\n",
    "if len(InputSource) > 0: \n",
    "  PropertyNameIndex = NewPropertyNameIndex\n",
    "  NewPropertyNameIndex = None\n",
    "  PropertyAverageValuesPointer = NewPropertyAverageValuesPointer\n",
    "  NewPropertyAverageValuesPointer = None\n",
    "\n",
    "  NormedInputStaticProps = NewNormedInputStaticProps\n",
    "  NewNormedInputStaticProps = None\n",
    "  NormedDynamicPropertyTimeSeries = NewNormedDynamicPropertyTimeSeries\n",
    "  NewNormedDynamicPropertyTimeSeries = None\n",
    "\n",
    "  NpropperTime = NewNpropperTime\n",
    "  NpropperTimeStatic = NewNpropperTimeStatic\n",
    "  NpropperTimeDynamic = NewNpropperTimeDynamic\n",
    "\n",
    "print('Static Properties')\n",
    "if NpropperTimeStatic > 0 :\n",
    "  checkNaN(NormedInputStaticProps)\n",
    "else:\n",
    "  print(' None Defined')\n",
    "print('Dynamic Properties')\n",
    "\n",
    "checkNaN(NormedDynamicPropertyTimeSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eRJTbE7ypBX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Covid Data: Agree on Tseq Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJLbkWv6xSoV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if ReadAugust2020Covid or RereadMay2020:\n",
    "  Tseq = 9\n",
    "if ReadJan2021Covid or ReadAugust2020Covid or ReadApril2021Covid:\n",
    "  Tseq = 13\n",
    "print(Tseq)\n",
    "print(SymbolicWindows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZyZD9mEio0z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Setup Sequences and Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTQVBsqmix8O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Num_SeqExtraUsed = Tseq-1\n",
    "Num_Seq = Num_Time - Tseq\n",
    "Num_SeqPred = Num_Seq\n",
    "TseqPred = Tseq\n",
    "TFTExtraTimes = 0\n",
    "Num_TimeTFT = Num_Time\n",
    "if UseTFTModel:\n",
    "  TFTExtraTimes = 1 + LengthFutures\n",
    "  SymbolicWindows = True\n",
    "  Num_SeqExtraUsed = Tseq # as last position needed in input\n",
    "  Num_TimeTFT = Num_Time +TFTExtraTimes\n",
    "  Num_SeqPred = Num_Seq\n",
    "  TseqPred = Tseq\n",
    "\n",
    "# If SymbolicWindows, sequences are not made but we use same array with that dimension (RawInputSeqDimension) set to 1\n",
    "# reshape can get rid of this irrelevant dimension\n",
    "# Predictions and Input Properties are associated with sequence number which is first time value used in sequence\n",
    "# if SymbolicWindows false then sequences are labelled by sequence # and contain time values from sequence # to sequence# + Tseq-1\n",
    "# if SymbolicWindows True then sequences are labelled by time # and contain one value. They are displaced by Tseq\n",
    "# If TFT Inputs and Predictions do NOT differ by Tseq\n",
    "# Num_SeqExtra extra positions in RawInputSequencesTOT for Symbolic windows True as need to store full window\n",
    "# TFTExtraTimes are extra times\n",
    "RawInputSeqDimension = Tseq\n",
    "Num_SeqExtra = 0\n",
    "if SymbolicWindows:\n",
    "  RawInputSeqDimension = 1\n",
    "  Num_SeqExtra =  Num_SeqExtraUsed\n",
    "print('Tseq ' +str(Tseq) + ' TseqPred ' + str(TseqPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYgeVR4S11pc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Generate Sequences from Time labelled data \n",
    "given Tseq set above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUnmDWwS3Iai",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if GenerateSequences:\n",
    "  UseProperties = np.full(NpropperTime, True, dtype=np.bool)\n",
    "#  if Hydrology:\n",
    "#    UseProperties[NpropperTime-1] = False\n",
    "  Npropperseq = 0\n",
    "  IndexintoPropertyArrays = np.empty(NpropperTime, dtype = np.int)\n",
    "  for iprop in range(0,NpropperTime):\n",
    "    if UseProperties[iprop]:\n",
    "      IndexintoPropertyArrays[Npropperseq] = iprop\n",
    "      Npropperseq +=1\n",
    "  RawInputSequences = np.zeros([Num_Seq + Num_SeqExtra, Nloc, RawInputSeqDimension, Npropperseq], dtype =np.float32)\n",
    "  RawInputPredictions = np.zeros([Num_SeqPred, Nloc, Npredperseq], dtype =np.float32)\n",
    "\n",
    "  locationarray = np.empty(Nloc, dtype=np.float32)\n",
    "  for iseq in range(0,Num_Seq  + Num_SeqExtra):\n",
    "    for windowposition in range(0,RawInputSeqDimension):\n",
    "      itime = iseq + windowposition\n",
    "      for usedproperty  in range (0,Npropperseq):\n",
    "        iprop = IndexintoPropertyArrays[usedproperty]\n",
    "        if iprop>=NpropperTimeStatic:\n",
    "          jprop =iprop-NpropperTimeStatic\n",
    "          locationarray = NormedDynamicPropertyTimeSeries[itime,:,jprop]\n",
    "        else:\n",
    "          locationarray = NormedInputStaticProps[:,iprop]\n",
    "        RawInputSequences[iseq,:,windowposition,usedproperty] = locationarray\n",
    "    if iseq < Num_SeqPred:\n",
    "      RawInputPredictions[iseq,:,:] = InputPredictionsbyTime[iseq+TseqPred,:,:]\n",
    "  print(startbold + 'Sequences set from Time values Num Seq ' + str(Num_SeqPred) + ' Time ' +str(Num_Time) + ' Tseq ' + str(Tseq) + resetfonts)  \n",
    "\n",
    "NormedInputTimeSeries = None\n",
    "NormedDynamicPropertyTimeSeries = None\n",
    "if GarbageCollect:\n",
    "  gc.collect()\n",
    "\n",
    "GlobalTimeMask = np.empty([1,1,1,Tseq,Tseq],dtype =np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lprQwdZFby5Y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define Possible Temporal and Spatial Positional Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tu9Oy46Nb4LO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def LinearLocationEncoding(TotalLoc):\n",
    "  linear = np.empty(TotalLoc, dtype=float)\n",
    "  for i in range(0,TotalLoc):\n",
    "    linear[i] = float(i)/float(TotalLoc)\n",
    "  return linear\n",
    "\n",
    "def LinearTimeEncoding(Dateslisted):\n",
    "  Firstdate = Dateslisted[0]\n",
    "  numtofind = len(Dateslisted)\n",
    "  dayrange = (Dateslisted[numtofind-1]-Firstdate).days + 1\n",
    "  linear = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    linear[i] = float((Dateslisted[i]-Firstdate).days)/float(dayrange)\n",
    "  return linear\n",
    "\n",
    "def P2TimeEncoding(numtofind):\n",
    "  P2 = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    x =  -1 + 2.0*i/(numtofind-1)\n",
    "    P2[i] = 0.5*(3*x*x-1)\n",
    "  return P2\n",
    "\n",
    "def P3TimeEncoding(numtofind):\n",
    "  P3 = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    x =  -1 + 2.0*i/(numtofind-1)\n",
    "    P3[i] = 0.5*(5*x*x-3)*x\n",
    "  return P3\n",
    "\n",
    "def P4TimeEncoding(numtofind):\n",
    "  P4 = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    x =  -1 + 2.0*i/(numtofind-1)\n",
    "    P4[i] = 0.125*(35*x*x*x*x - 30*x*x + 3)\n",
    "  return P4\n",
    "\n",
    "def WeeklyTimeEncoding(Dateslisted):\n",
    "  numtofind = len(Dateslisted)\n",
    "  costheta = np.empty(numtofind, dtype=float)\n",
    "  sintheta = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    j = Dateslisted[i].date().weekday()\n",
    "    theta = float(j)*2.0*math.pi/7.0\n",
    "    costheta[i] = math.cos(theta)\n",
    "    sintheta[i] = math.sin(theta)\n",
    "  return costheta, sintheta\n",
    "\n",
    "def AnnualTimeEncoding(Dateslisted): \n",
    "  numtofind = len(Dateslisted)\n",
    "  costheta = np.empty(numtofind, dtype=float)\n",
    "  sintheta = np.empty(numtofind, dtype=float)\n",
    "  for i in range(0,numtofind):\n",
    "    runningdate = Dateslisted[i]\n",
    "    year = runningdate.year\n",
    "    datebeginyear = datetime(year, 1, 1)\n",
    "    displacement = (runningdate-datebeginyear).days\n",
    "    daysinyear = (datetime(year,12,31)-datebeginyear).days+1\n",
    "    if displacement >= daysinyear:\n",
    "      printexit(\"EXIT Bad Date \", runningdate)\n",
    "    theta = float(displacement)*2.0*math.pi/float(daysinyear)\n",
    "    costheta[i] = math.cos(theta)\n",
    "    sintheta[i] = math.sin(theta)\n",
    "  return costheta, sintheta\n",
    "\n",
    "def ReturnEncoding(numtofind,Typeindex, Typevalue):\n",
    "  Dummy = costheta = np.empty(0, dtype=float)\n",
    "  if Typeindex == 1:\n",
    "    return LinearoverLocationEncoding, Dummy, ('LinearSpace',0.,1.0,0.5,0.2887), ('Dummy',0.,0.,0.,0.)\n",
    "  if Typeindex == 2:\n",
    "    if Dailyunit == 1:\n",
    "      return CosWeeklytimeEncoding, SinWeeklytimeEncoding, ('CosWeekly',-1.0, 1.0, 0.,0.7071), ('SinWeekly',-1.0, 1.0, 0.,0.7071)\n",
    "    else:\n",
    "      return Dummy, Dummy, ('Dummy',0.,0.,0.,0.), ('Dummy',0.,0.,0.,0.)\n",
    "  if Typeindex == 3:\n",
    "    return CosAnnualtimeEncoding, SinAnnualtimeEncoding, ('CosAnnual',-1.0, 1.0, 0.,0.7071), ('SinAnnual',-1.0, 1.0, 0.,0.7071)\n",
    "  if Typeindex == 4:\n",
    "    if Typevalue == 0:\n",
    "      ConstArray = np.full(numtofind,0.5, dtype = float)\n",
    "      return ConstArray, Dummy, ('Constant',0.5,0.5,0.5,0.0), ('Dummy',0.,0.,0.,0.)\n",
    "    if Typevalue == 1:\n",
    "      return LinearovertimeEncoding, Dummy, ('LinearTime',0., 1.0, 0.5,0.2887), ('Dummy',0.,0.,0.,0.)\n",
    "    if Typevalue == 2:\n",
    "      return P2TimeEncoding(numtofind), Dummy, ('P2-Time',-1.0, 1.0, 0.,0.4472), ('Dummy',0.,0.,0.,0.)\n",
    "    if Typevalue == 3:\n",
    "      return P3TimeEncoding(numtofind), Dummy, ('P3-Time',-1.0, 1.0, 0.,0.3780), ('Dummy',0.,0.,0.,0.)\n",
    "    if Typevalue == 4:\n",
    "      return P4TimeEncoding(numtofind), Dummy, ('P4-Time',-1.0, 1.0, 0.,0.3333), ('Dummy',0.,0.,0.,0.)\n",
    "  if Typeindex == 5:\n",
    "      costheta = np.empty(numtofind, dtype=float)\n",
    "      sintheta = np.empty(numtofind, dtype=float)\n",
    "      j = 0\n",
    "      for i in range(0,numtofind):\n",
    "        theta = float(j)*2.0*math.pi/Typevalue\n",
    "        costheta[i] = math.cos(theta)\n",
    "        sintheta[i] = math.sin(theta)\n",
    "        j += 1\n",
    "        if j >= Typevalue:\n",
    "          j = 0\n",
    "      return costheta, sintheta,('Cos '+str(Typevalue)+ ' Len',-1.0, 1.0,0.,0.7071), ('Sin '+str(Typevalue)+ ' Len',-1.0, 1.0,0.,0.7071)\n",
    "\n",
    "# Dates set up in Python datetime format as Python LISTS\n",
    "# All encodings are Numpy arrays\n",
    "print(\"Total number of Time Units \" + str(NumberofTimeunits) + ' ' + TimeIntervalUnitName)\n",
    "if NumberofTimeunits != (Num_Seq + Tseq):\n",
    "  printexit(\"EXIT Wrong Number of Time Units \" + str(Num_Seq + Tseq))\n",
    "\n",
    "Dateslist = []\n",
    "for i in range(0,NumberofTimeunits + TFTExtraTimes):\n",
    "  Dateslist.append(InitialDate+timedelta(days=i*Dailyunit))\n",
    "\n",
    "LinearoverLocationEncoding = LinearLocationEncoding(Nloc)\n",
    "LinearovertimeEncoding = LinearTimeEncoding(Dateslist)\n",
    "\n",
    "if Dailyunit == 1:\n",
    "  CosWeeklytimeEncoding, SinWeeklytimeEncoding = WeeklyTimeEncoding(Dateslist)\n",
    "CosAnnualtimeEncoding, SinAnnualtimeEncoding = AnnualTimeEncoding(Dateslist)\n",
    "\n",
    "\n",
    "# Encodings\n",
    "\n",
    "# linearlocationposition\n",
    "# Supported Time Dependent Probes that can be in properties and/or predictions\n",
    "# Spatial\n",
    "# Annual\n",
    "# Weekly\n",
    "# \n",
    "# Top Down\n",
    "# TD0 Constant at 0.5\n",
    "# TD1 Linear from 0 to 1\n",
    "# TD2 P2(x) where x goes from -1 to 1 as time goes from start to end\n",
    "# \n",
    "# Bottom Up\n",
    "# n-way Cos and sin theta where n = 4 7 8 16 24 32\n",
    "\n",
    "EncodingTypes = {'Spatial':1, 'Weekly':2,'Annual':3,'TopDown':4,'BottomUp':5}\n",
    "\n",
    "PropIndex =[]\n",
    "PropNameMeanStd = []\n",
    "PropMeanStd = []\n",
    "PropArray = []\n",
    "PropPosition = []\n",
    "\n",
    "PredIndex =[]\n",
    "PredNameMeanStd = []\n",
    "PredArray = []\n",
    "PredPosition = []\n",
    "\n",
    "Numberpropaddons = 0\n",
    "propposition = Npropperseq\n",
    "Numberpredaddons = 0\n",
    "predposition = Npredperseq\n",
    "\n",
    "numprop = len(SpaceTimeEncodingPropTypes)\n",
    "if numprop != len(SpaceTimeEncodingPropValues):\n",
    "  printexit('Error in property addons ' + str(numprop) + ' ' + str(len(SpaceTimeEncodingPropValues)))\n",
    "for newpropinlist in range(0,numprop):\n",
    "  Typeindex = EncodingTypes[SpaceTimeEncodingPropTypes[newpropinlist]]\n",
    "  a,b,c,d = ReturnEncoding(Num_Time + TFTExtraTimes,Typeindex, SpaceTimeEncodingPropValues[newpropinlist])\n",
    "  if c[0] != 'Dummy':\n",
    "    PropIndex.append(Typeindex)\n",
    "    PropNameMeanStd.append(c)\n",
    "    InputPropertyNames.append(c[0])\n",
    "    PropArray.append(a)\n",
    "    PropPosition.append(propposition)\n",
    "    propposition += 1\n",
    "    Numberpropaddons += 1\n",
    "    line = ' '\n",
    "    for ipr in range(0,20):\n",
    "      line += str(round(a[ipr],4)) + ' '\n",
    "#    print('c'+line)\n",
    "  if d[0] != 'Dummy':\n",
    "    PropIndex.append(Typeindex)\n",
    "    PropNameMeanStd.append(d)\n",
    "    InputPropertyNames.append(d[0])\n",
    "    PropArray.append(b)\n",
    "    PropPosition.append(propposition)\n",
    "    propposition += 1\n",
    "    Numberpropaddons += 1\n",
    "    line = ' '\n",
    "    for ipr in range(0,20):\n",
    "      line += str(round(b[ipr],4)) + ' '\n",
    "#    print('d'+line)\n",
    "\n",
    "numpred = len(SpaceTimeEncodingPredTypes)\n",
    "if numpred != len(SpaceTimeEncodingPredValues):\n",
    "  printexit('Error in prediction addons ' + str(numpred) + ' ' + str(len(SpaceTimeEncodingPredValues)))\n",
    "for newpredinlist in range(0,numpred):\n",
    "  Typeindex = EncodingTypes[SpaceTimeEncodingPredTypes[newpredinlist]]\n",
    "  a,b,c,d = ReturnEncoding(Num_Time + TFTExtraTimes,Typeindex, SpaceTimeEncodingPredValues[newpredinlist])\n",
    "  if c[0] != 'Dummy':\n",
    "    PredIndex.append(Typeindex)\n",
    "    PredNameMeanStd.append(c)\n",
    "    PredArray.append(a)\n",
    "    Predictionname.append(c[0])\n",
    "    Predictionnamelookup[c] = predposition\n",
    "    PredPosition.append(predposition)\n",
    "    predposition += 1\n",
    "    Numberpredaddons += 1\n",
    "    Predictionwgt.append(0.25)\n",
    "  if d[0] != 'Dummy':\n",
    "    PredIndex.append(Typeindex)\n",
    "    PredNameMeanStd.append(d)\n",
    "    PredArray.append(b)\n",
    "    Predictionname.append(d[0])\n",
    "    Predictionnamelookup[d[0]] = predposition\n",
    "    PredPosition.append(predposition)\n",
    "    predposition += 1\n",
    "    Numberpredaddons += 1\n",
    "    Predictionwgt.append(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANMrg0vjoPxS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add in Temporal and Spatial Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0FxRdZa81_Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Set up NNSE and Plots including Futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I977Ffv_obEC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SetNewAverages(InputList): # name min max mean std\n",
    "  results = np.empty(7, dtype = np.float32)\n",
    "  results[0] = InputList[1]\n",
    "  results[1] = InputList[2]\n",
    "  results[2] = 1.0\n",
    "  results[3] = InputList[3]\n",
    "  results[4] = InputList[4]\n",
    "  results[5] = InputList[3]\n",
    "  results[6] = InputList[4]\n",
    "  return results\n",
    "\n",
    "\n",
    "NpropperseqTOT = Npropperseq + Numberpropaddons\n",
    "\n",
    "# These include both Property and Prediction Variables\n",
    "NpropperTimeMAX =len(QuantityTakeroot)\n",
    "NewNpropperTimeMAX = NpropperTimeMAX + Numberpropaddons + Numberpredaddons  \n",
    "NewQuantityStatistics = np.zeros([NewNpropperTimeMAX,7], dtype=np.float32)\n",
    "NewQuantityTakeroot = np.full(NewNpropperTimeMAX,1,dtype=np.int) # All new ones aare 1 and are set here\n",
    "NewQuantityStatistics[0:NpropperTimeMAX,:] = QuantityStatistics[0:NpropperTimeMAX,:]\n",
    "NewQuantityTakeroot[0:NpropperTimeMAX] = QuantityTakeroot[0:NpropperTimeMAX]\n",
    "\n",
    "# Lookup for property names\n",
    "NewPropertyNameIndex = np.empty(NpropperseqTOT, dtype = np.int32)\n",
    "NumberofNames = len(InputPropertyNames)-Numberpropaddons\n",
    "NewPropertyNameIndex[0:Npropperseq] = PropertyNameIndex[0:Npropperseq]\n",
    "\n",
    "NewPropertyAverageValuesPointer = np.empty(NpropperseqTOT, dtype = np.int32)\n",
    "NewPropertyAverageValuesPointer[0:Npropperseq] = PropertyAverageValuesPointer[0:Npropperseq]\n",
    "\n",
    "for propaddons in range(0,Numberpropaddons):\n",
    "  NewPropertyNameIndex[Npropperseq+propaddons] = NumberofNames + propaddons\n",
    "  NewPropertyAverageValuesPointer[Npropperseq+propaddons] = NpropperTimeMAX + propaddons\n",
    "  NewQuantityStatistics[NpropperTimeMAX + propaddons,:] = SetNewAverages(PropNameMeanStd[propaddons])\n",
    "\n",
    "# Set extra Predictions metadata for Sequences\n",
    "NpredperseqTOT = Npredperseq + Numberpredaddons\n",
    "\n",
    "NewPredictionAverageValuesPointer = np.empty(NpredperseqTOT, dtype = np.int32)\n",
    "NewPredictionAverageValuesPointer[0:Npredperseq] = PredictionAverageValuesPointer[0:Npredperseq]\n",
    "\n",
    "for predaddons in range(0,Numberpredaddons):\n",
    "  NewPredictionAverageValuesPointer[Npredperseq +predaddons] = NpropperTimeMAX + +Numberpropaddons + predaddons\n",
    "  NewQuantityStatistics[NpropperTimeMAX + Numberpropaddons + predaddons,:] = SetNewAverages(PredNameMeanStd[predaddons])\n",
    "\n",
    "RawInputSequencesTOT = np.empty([Num_Seq  + Num_SeqExtra + TFTExtraTimes, Nloc, RawInputSeqDimension, NpropperseqTOT], dtype =np.float32)\n",
    "flsize = np.float(Num_Seq  + Num_SeqExtra)*np.float(Nloc)*np.float(RawInputSeqDimension)* np.float(NpropperseqTOT)* 4.0\n",
    "print('Total storage ' +str(round(flsize,0)) + ' Bytes')\n",
    "\n",
    "for i in range(0,Num_Seq  + Num_SeqExtra):\n",
    "  for iprop in range(0,Npropperseq):\n",
    "    RawInputSequencesTOT[i,:,:,iprop] = RawInputSequences[i,:,:,iprop]\n",
    "for i in range(Num_Seq  + Num_SeqExtra,Num_Seq  + Num_SeqExtra + TFTExtraTimes):\n",
    "  for iprop in range(0,Npropperseq):\n",
    "    RawInputSequencesTOT[i,:,:,iprop] = NaN\n",
    "\n",
    "for i in range(0,Num_Seq  + Num_SeqExtra + TFTExtraTimes):\n",
    "    for k in range(0,RawInputSeqDimension):\n",
    "      for iprop in range(0, Numberpropaddons):\n",
    "        if PropIndex[iprop] == 1:\n",
    "          continue\n",
    "        RawInputSequencesTOT[i,:,k,PropPosition[iprop]] = PropArray[iprop][i+k]\n",
    "\n",
    "for iprop in range(0, Numberpropaddons):\n",
    "  if PropIndex[iprop] == 1:\n",
    "    for j in range(0,Nloc):       \n",
    "        RawInputSequencesTOT[:,j,:,PropPosition[iprop]] = PropArray[iprop][j]\n",
    "\n",
    "# Set extra Predictions for Sequences\n",
    "RawInputPredictionsTOT = np.empty([Num_SeqPred + TFTExtraTimes, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "\n",
    "for i in range(0,Num_SeqPred):\n",
    "  for ipred in range(0,Npredperseq):\n",
    "    RawInputPredictionsTOT[i,:,ipred] = RawInputPredictions[i,:,ipred]\n",
    "for i in range(Num_SeqPred, Num_SeqPred + TFTExtraTimes):\n",
    "  for ipred in range(0,Npredperseq):\n",
    "    RawInputPredictionsTOT[i,:,ipred] = NaN\n",
    "\n",
    "for i in range(0,Num_SeqPred + TFTExtraTimes):\n",
    "  for ipred in range(0, Numberpredaddons):\n",
    "    if PredIndex[ipred] == 1:\n",
    "      continue\n",
    "    actualarray = PredArray[ipred]\n",
    "    RawInputPredictionsTOT[i,:,PredPosition[ipred]] = actualarray[i+TseqPred]\n",
    "\n",
    "for ipred in range(0, Numberpredaddons):\n",
    "  if PredIndex[ipred] == 1:\n",
    "    for j in range(0,Nloc):\n",
    "      RawInputPredictionsTOT[:,j,PredPosition[ipred]] = PredArray[ipred][j]\n",
    "\n",
    "PropertyNameIndex  = None\n",
    "PropertyNameIndex = NewPropertyNameIndex\n",
    "QuantityStatistics = None\n",
    "QuantityStatistics = NewQuantityStatistics\n",
    "QuantityTakeroot = None\n",
    "QuantityTakeroot = NewQuantityTakeroot\n",
    "PropertyAverageValuesPointer = None\n",
    "PropertyAverageValuesPointer = NewPropertyAverageValuesPointer\n",
    "PredictionAverageValuesPointer = None\n",
    "PredictionAverageValuesPointer = NewPredictionAverageValuesPointer\n",
    "\n",
    "print('Time and Space encoding added to input and predictions')\n",
    "\n",
    "if SymbolicWindows:\n",
    "  SymbolicInputSequencesTOT = np.empty([Num_Seq, Nloc], dtype =np.int32) # This is sequences\n",
    "  for iseq in range(0,Num_Seq):\n",
    "    for iloc in range(0,Nloc):\n",
    "      SymbolicInputSequencesTOT[iseq,iloc] = np.left_shift(iseq,16) + iloc\n",
    "  ReshapedSequencesTOT = np.transpose(RawInputSequencesTOT,(1,0,3,2))\n",
    "  ReshapedSequencesTOT = np.reshape(ReshapedSequencesTOT,(Nloc,Num_Seq  + Num_SeqExtra + TFTExtraTimes,NpropperseqTOT))\n",
    "\n",
    "# To calculate masks (identical to Symbolic windows)\n",
    "SpacetimeforMask = np.empty([Num_Seq, Nloc], dtype =np.int32)\n",
    "for iseq in range(0,Num_Seq):\n",
    "  for iloc in range(0,Nloc):\n",
    "    SpacetimeforMask[iseq,iloc] = np.left_shift(iseq,16) + iloc\n",
    "    \n",
    "print(PropertyNameIndex)\n",
    "print(InputPropertyNames)\n",
    "for iprop in range(0,NpropperseqTOT):\n",
    "  line = 'Property ' + str(iprop) + ' ' + InputPropertyNames[PropertyNameIndex[iprop]]\n",
    "  jprop = PropertyAverageValuesPointer[iprop]\n",
    "  line += ' Processing Root ' + str(QuantityTakeroot[jprop])\n",
    "  for proppredval in range (0,7):\n",
    "      line += ' ' + QuantityStatisticsNames[proppredval] + ' ' + str(round(QuantityStatistics[jprop,proppredval],3))\n",
    "  print(wraptotext(line,size=150))\n",
    "\n",
    "PredictionNameIndex = np.empty(NpredperseqTOT, dtype =np.int32))\n",
    "for ipred in range(0,NpredperseqTOT):\n",
    "  PredictionNameIndex[ipred] = ipred\n",
    "  line = 'Prediction ' + str(ipred) + ' ' + Predictionname[PredictionNameIndex[ipred]] + ' ' + str(round(Predictionwgt[ipred],3))\n",
    "  jpred = PredictionAverageValuesPointer[ipred]\n",
    "  line += ' Processing Root ' + str(QuantityTakeroot[jpred])\n",
    "  for proppredval in range (0,7):\n",
    "      line += ' ' + QuantityStatisticsNames[proppredval] + ' ' + str(round(QuantityStatistics[jpred,proppredval],3))\n",
    "  print(wraptotext(line,size=150))\n",
    "\n",
    "\n",
    "\n",
    "RawInputPredictions = None\n",
    "RawInputSequences = None\n",
    "if SymbolicWindows:\n",
    "  RawInputSequencesTOT = None\n",
    "if GarbageCollect:\n",
    "  gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdFW7f6l3Uo-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Set up NNSE Normalized Nash Sutcliffe Efficiency\n",
    "CalculateNNSE = np.full(NpredperseqTOT, False, dtype = np.bool)\n",
    "PlotPredictions  = np.full(NpredperseqTOT, False, dtype = np.bool)\n",
    "for ipred in range(0,NpredperseqTOT):\n",
    "  CalculateNNSE[ipred] = True\n",
    "  if  (Predictionname[PredictionNameIndex[ipred]] == 'Constant') or (Predictionname[PredictionNameIndex[ipred]] == 'LinearSpace'):\n",
    "    CalculateNNSE[ipred] = False # as standard deviation over time zero\n",
    "  PlotPredictions[ipred] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hytLQj7QW3gx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Location Based Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2g_-MHEhyGr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LocationBasedValidation = False\n",
    "LocationValidationFraction = 0.0\n",
    "RestartLocationBasedValidation = False\n",
    "RestartValidationSetRunName = RunName\n",
    "if Earthquake: \n",
    "  LocationBasedValidation = True\n",
    "  LocationValidationFraction = 0.2\n",
    "  RestartLocationBasedValidation = True\n",
    "  RestartValidationSetRunName = 'EARTHQN-Transformer3'\n",
    "FullSetValidation = False\n",
    "\n",
    "global SeparateValandTrainingPlots\n",
    "SeparateValandTrainingPlots = True\n",
    "if not LocationBasedValidation:\n",
    "  SeparateValandTrainingPlots = False\n",
    "  LocationValidationFraction = 0.0\n",
    "ListofTrainingLocs = np.arange(Nloc, dtype = np.int32)\n",
    "ListofValidationLocs = np.full(Nloc, -1, dtype = np.int32)\n",
    "MappingtoTraining = np.arange(Nloc, dtype = np.int32)\n",
    "MappingtoValidation = np.full(Nloc, -1, dtype = np.int32)\n",
    "TrainingNloc = Nloc\n",
    "ValidationNloc = 0\n",
    "if LocationBasedValidation:\n",
    "  if RestartLocationBasedValidation:\n",
    "      InputFileName = APPLDIR + '/Validation' + RestartValidationSetRunName\n",
    "      with open(InputFileName, 'r', newline='') as inputfile:\n",
    "        Myreader = reader(inputfile, delimiter=',')\n",
    "        header = next(Myreader)\n",
    "        LocationValidationFraction = np.float32(header[0])\n",
    "        TrainingNloc = np.int32(header[1])\n",
    "        ValidationNloc = np.int32(header[2])     \n",
    "\n",
    "        ListofTrainingLocs = np.empty(TrainingNloc, dtype = np.int32)\n",
    "        ListofValidationLocs = np.empty(ValidationNloc,  dtype = np.int32)\n",
    "        nextrow = next(Myreader)\n",
    "        for iloc in range(0, TrainingNloc):\n",
    "          ListofTrainingLocs[iloc] = np.int32(nextrow[iloc])\n",
    "        nextrow = next(Myreader)\n",
    "        for iloc in range(0, ValidationNloc):\n",
    "          ListofValidationLocs[iloc] = np.int32(nextrow[iloc])\n",
    "\n",
    "      LocationTrainingfraction = 1.0 - LocationValidationFraction\n",
    "      if TrainingNloc + ValidationNloc != Nloc:\n",
    "        printexit('EXIT: Inconsistent location counts for Location Validation ' +str(Nloc)\n",
    "          + ' ' + str(TrainingNloc) + ' ' + str(ValidationNloc))\n",
    "      print(' Validation restarted Fraction ' +str(round(LocationValidationFraction,4)) + ' ' + RestartValidationSetRunName)\n",
    "\n",
    "  else:\n",
    "    LocationTrainingfraction = 1.0 - LocationValidationFraction\n",
    "    TrainingNloc = math.ceil(LocationTrainingfraction*Nloc)\n",
    "    ValidationNloc = Nloc - TrainingNloc\n",
    "    np.random.shuffle(ListofTrainingLocs)\n",
    "    ListofValidationLocs = ListofTrainingLocs[TrainingNloc:Nloc]\n",
    "    ListofTrainingLocs = ListofTrainingLocs[0:TrainingNloc]\n",
    "\n",
    "  for iloc in range(0,TrainingNloc):\n",
    "    jloc = ListofTrainingLocs[iloc]\n",
    "    MappingtoTraining[jloc] = iloc\n",
    "    MappingtoValidation[jloc] = -1\n",
    "  for iloc in range(0,ValidationNloc):\n",
    "    jloc = ListofValidationLocs[iloc]\n",
    "    MappingtoValidation[jloc] = iloc\n",
    "    MappingtoTraining[jloc] = -1\n",
    "  if ValidationNloc <= 0:\n",
    "    SeparateValandTrainingPlots = False\n",
    "\n",
    "  if not RestartLocationBasedValidation:\n",
    "    OutputFileName = APPLDIR + '/Validation' + RunName\n",
    "    with open(OutputFileName, 'w', newline='') as outputfile:\n",
    "      Mywriter = writer(outputfile, delimiter=',')\n",
    "      Mywriter.writerow([LocationValidationFraction, TrainingNloc, ValidationNloc] )\n",
    "      Mywriter.writerow(ListofTrainingLocs)\n",
    "      Mywriter.writerow(ListofValidationLocs)\n",
    "\n",
    "  print('Training Locations ' + str(TrainingNloc) + ' Validation Locations ' + str(ValidationNloc))\n",
    "  if ValidationNloc <=0:\n",
    "    LocationBasedValidation = False\n",
    "\n",
    "\n",
    "if Earthquake:\n",
    "  StartDate = np.datetime64(InitialDate).astype('datetime64[D]') + np.timedelta64(Tseq*Dailyunit + int(Dailyunit/2),'D')\n",
    "  dayrange = np.timedelta64(Dailyunit,'D')\n",
    "  Numericaldate = np.empty(numberspecialeqs, dtype=np.float32)\n",
    "  PrimaryTrainingList = []\n",
    "  SecondaryTrainingList = []\n",
    "  PrimaryValidationList = []\n",
    "  SecondaryValidationList = []\n",
    "  for iquake in range(0,numberspecialeqs):\n",
    "    Numericaldate[iquake] = max(0,math.floor((Specialdate[iquake] - StartDate)/dayrange))\n",
    "    Trainingsecondary = False\n",
    "    Validationsecondary = False\n",
    "    for jloc in range(0,Nloc):\n",
    "      iloc = LookupLocations[jloc] # original location\n",
    "      result = quakesearch(iquake, iloc)\n",
    "      if result == 0:\n",
    "        continue\n",
    "      kloc = MappingtoTraining[jloc]\n",
    "      if result == 1: # Primary\n",
    "        if kloc >= 0:\n",
    "          PrimaryTrainingList.append(iquake)\n",
    "          Trainingsecondary = True\n",
    "        else:\n",
    "          PrimaryValidationList.append(iquake)\n",
    "          Validationsecondary = True\n",
    "      else: # Secondary\n",
    "        if kloc >= 0:\n",
    "          if Trainingsecondary:\n",
    "            continue\n",
    "          Trainingsecondary = True\n",
    "          SecondaryTrainingList.append(iquake)\n",
    "        else:\n",
    "          if Validationsecondary:\n",
    "            continue\n",
    "          Validationsecondary = True\n",
    "          SecondaryValidationList.append(iquake) \n",
    "    iloc = Specialxpos[iquake] + 60*Specialypos[iquake]\n",
    "    jloc = MappedLocations[iloc]\n",
    "    kloc = -2\n",
    "    if jloc >= 0:\n",
    "      kloc = LookupLocations[jloc]\n",
    "    line = str(iquake) + \" \" + str(Trainingsecondary) + \" \" + str(Validationsecondary) + \" \"\n",
    "    line += str(iloc) + \" \" + str(jloc) + \" \" + str(kloc) + \" \" + str(round(Specialmags[iquake],1)) + ' ' + Specialeqname[iquake]\n",
    "    print(line)      \n",
    "\n",
    "\n",
    "  PrimaryTrainingvetoquake = np.full(numberspecialeqs,True, dtype = np.bool) \n",
    "  SecondaryTrainingvetoquake = np.full(numberspecialeqs,True, dtype = np.bool)\n",
    "  PrimaryValidationvetoquake = np.full(numberspecialeqs,True, dtype = np.bool)\n",
    "  SecondaryValidationvetoquake = np.full(numberspecialeqs,True, dtype = np.bool) \n",
    "  for jquake in PrimaryTrainingList:\n",
    "    PrimaryTrainingvetoquake[jquake] = False\n",
    "  for jquake in PrimaryValidationList:\n",
    "    PrimaryValidationvetoquake[jquake] = False\n",
    "  for jquake in SecondaryTrainingList:\n",
    "    if not PrimaryTrainingvetoquake[jquake]:\n",
    "      continue\n",
    "    SecondaryTrainingvetoquake[jquake] = False\n",
    "  for jquake in SecondaryValidationList:\n",
    "    if not PrimaryValidationvetoquake[jquake]:\n",
    "      continue\n",
    "    SecondaryValidationvetoquake[jquake] = False\n",
    "\n",
    "  for iquake in range(0,numberspecialeqs):\n",
    "    iloc = Specialxpos[iquake] + 60*Specialypos[iquake]\n",
    "    line = str(iquake) + \" Loc \" + str(iloc) + \" \" + str(MappedLocations[iloc]) + \" Date \" + str(Specialdate[iquake]) + \" \" + str(Numericaldate[iquake]) \n",
    "    line +=  \" \" + str(PrimaryTrainingvetoquake[iquake]) + \" \" + str(SecondaryTrainingvetoquake[iquake])\n",
    "    line +=  \" Val \" + str(PrimaryValidationvetoquake[iquake]) + \" \" + str(SecondaryValidationvetoquake[iquake])\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33FLmGmcilz5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LSTM Control Parameters EDIT TFTT..epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ds28euHRi5vt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CustomLoss = 1         \n",
    "UseClassweights = True\n",
    "\n",
    "PredictionTraining = False\n",
    "\n",
    "if (not Hydrology) and (not Earthquake) and (NpredperseqTOT <=2):\n",
    "  useFutures = False\n",
    "  CustomLoss = 0\n",
    "  UseClassweights = False\n",
    "\n",
    "number_of_LSTMworkers = 1\n",
    "TFTTransformerepochs = 10\n",
    "LSTMbatch_size = TrainingNloc\n",
    "LSTMbatch_size = min(LSTMbatch_size, TrainingNloc)\n",
    "\n",
    "LSTMactivationvalue = \"selu\"\n",
    "LSTMrecurrent_activation = \"sigmoid\"\n",
    "LSTMoptimizer = 'adam'\n",
    "LSTMdropout1=0.2\n",
    "LSTMrecurrent_dropout1 = 0.2\n",
    "LSTMdropout2=0.2\n",
    "LSTMrecurrent_dropout2 = 0.2\n",
    "number_LSTMnodes= 16\n",
    "LSTMFinalMLP = 64\n",
    "LSTMInitialMLP = 32\n",
    "LSTMThirdLayer = False\n",
    "\n",
    "LSTMSkipInitial = False\n",
    "\n",
    "LSTMverbose = 0\n",
    "LSTMvalidationfrac = 0.0\n",
    "UsedLSTMvalidationfrac = LSTMvalidationfrac\n",
    "if LocationBasedValidation:\n",
    "  UsedLSTMvalidationfrac = LocationBasedValidation\n",
    "  LSTMvalidationfrac = UsedLSTMvalidationfrac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CdCNdQ_yGWV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## General Control Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwkXnZZGgJ_1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "OuterBatchDimension = Num_Seq * TrainingNloc\n",
    "IndividualPlots = False\n",
    "Plotrealnumbers = False\n",
    "PlotsOnlyinTestFIPS = True\n",
    "ListofTestFIPS = ['36061','53033','17031','6037']\n",
    "if Hydrology:\n",
    "  ListofTestFIPS = ['6224000','6622700']\n",
    "  ListofTestFIPS = ['','']\n",
    "if Earthquake:\n",
    "  ListofTestFIPS = ['','']\n",
    "  Plotrealnumbers = True\n",
    "\n",
    "StartDate = np.datetime64(InitialDate).astype('datetime64[D]') + np.timedelta64(Tseq*Dailyunit + int(Dailyunit/2),'D')\n",
    "if Earthquake: \n",
    "  dayrange = np.timedelta64(Dailyunit,'D')\n",
    "  CutoffDate = np.datetime64('1989-01-01')\n",
    "  NumericalCutoff = math.floor((CutoffDate - StartDate)/dayrange)\n",
    "else:\n",
    "  NumericalCutoff = int(Num_Seq/2)\n",
    "  CutoffDate = StartDate + np.timedelta64(NumericalCutoff*Dailyunit,'D')\n",
    "print('Start ' + str(StartDate) + ' Cutoff ' + str(CutoffDate) + \" sequence index \" + str(NumericalCutoff))\n",
    "\n",
    "TimeCutLabel = [' All Time ',' Start ',' End ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4V88mmqms1pq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Size of sequence window Tseq \", str(Tseq))\n",
    "print(\"Number of Sequences in time Num_Seq \", str(Num_Seq))\n",
    "print(\"Number of locations Nloc \", str(Nloc))\n",
    "print(\"Number of Training Sequences in Location and Time \", str(OuterBatchDimension))\n",
    "print(\"Number of internal properties per sequence including static or dynamic Npropperseq \", str(Npropperseq))\n",
    "print(\"Number of internal properties per sequence adding in explicit space-time encoding \", str(NpropperseqTOT))\n",
    "print(\"Total number of predictions per sequence NpredperseqTOT \", str(NpredperseqTOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikdmffIpA6AC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Useful Time series utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2QTzC0vnSGP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DLprediction\n",
    "\n",
    "Prediction and Visualization LSTM+Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aarkiMHirB1S",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def DLprediction(Xin, yin, DLmodel, modelflag, LabelFit =''):\n",
    "  # modelflag = 0 LSTM = 1 Transformer\n",
    "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
    "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
    "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
    "    current_time = timenow()\n",
    "    print(startbold + startred + current_time + ' ' + RunName + \" DLPrediction \" +RunComment + resetfonts)\n",
    "\n",
    "    FitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "    # Compare to RawInputPredictionsTOT\n",
    "\n",
    "    RMSEbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSETRAINbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSEVALbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSVbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    AbsEbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    AbsVbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    ObsVbytimeandclass = np.zeros([Num_Seq, NpredperseqTOT,3], dtype=np.float64)\n",
    "    Predbytimeandclass = np.zeros([Num_Seq, NpredperseqTOT,3], dtype=np.float64)\n",
    "    countbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    countVALbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    countTRAINbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    totalcount = 0\n",
    "    overcount = 0\n",
    "    weightedcount = 0.0\n",
    "    weightedovercount = 0.0\n",
    "    weightedrmse1 = 0.0\n",
    "    weightedrmse1TRAIN = 0.0\n",
    "    weightedrmse1VAL = 0.0\n",
    "\n",
    "    closs = 0.0\n",
    "    dloss = 0.0\n",
    "    eloss = 0.0\n",
    "    floss = 0.0\n",
    "    sw = np.empty([Nloc,NpredperseqTOT],dtype = np.float32)\n",
    "    for iloc in range(0,Nloc):\n",
    "      for k in range(0,NpredperseqTOT):\n",
    "        sw[iloc,k] = Predictionwgt[k]\n",
    "\n",
    "    global tensorsw\n",
    "    tensorsw = tf.convert_to_tensor(sw, np.float32)\n",
    "    Ctime1 = 0.0\n",
    "    Ctime2 = 0.0\n",
    "    Ctime3 = 0.0\n",
    "    samplebar = notebook.trange(Num_Seq,  desc='Predict loop', unit  = 'sequences')\n",
    "    countingcalls = 0\n",
    "\n",
    "    for iseq in range(0, Num_Seq):\n",
    "      StopWatch.start('label1')\n",
    "      if SymbolicWindows:\n",
    "        if modelflag == 2:\n",
    "          InputVector = np.empty((Nloc,2), dtype = int)\n",
    "          for iloc in range (0,Nloc):\n",
    "            InputVector[iloc,0] = iloc\n",
    "            InputVector[iloc,1] = iseq\n",
    "        else:\n",
    "          InputVector = Xin[:,iseq:iseq+Tseq,:]\n",
    "      else:\n",
    "        InputVector = Xin[iseq]\n",
    "      Time = None\n",
    "      if modelflag == 0:\n",
    "        InputVector = np.reshape(InputVector,(-1,Tseq,NpropperseqTOT))\n",
    "      elif modelflag == 1:\n",
    "        InputVector = np.reshape(InputVector,(1,Tseq*Nloc,NpropperseqTOT))\n",
    "        BasicTimes = np.full(Nloc,iseq, dtype=np.int32)\n",
    "        Time = SetSpacetime(np.reshape(BasicTimes,(1,-1)))\n",
    "      StopWatch.stop('label1')\n",
    "      Ctime1 += StopWatch.get('label1', digits=4)\n",
    "      \n",
    "      StopWatch.start('label2')\n",
    "      PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time)\n",
    "      StopWatch.stop('label2')\n",
    "      Ctime2 += StopWatch.get('label2', digits=4)\n",
    "      StopWatch.start('label3')\n",
    "      PredictedVector = np.reshape(PredictedVector,(Nloc,NpredperseqTOT))\n",
    "      TrueVector = yin[iseq]\n",
    "      functionval = numpycustom_lossGCF1(TrueVector,PredictedVector,sw)\n",
    "      closs += functionval\n",
    "      PredictedVector_t = tf.convert_to_tensor(PredictedVector)\n",
    "      yin_t = tf.convert_to_tensor(TrueVector)\n",
    "      dloss += weightedcustom_lossGCF1(yin_t,PredictedVector_t,tensorsw)\n",
    "      eloss += custom_lossGCF1spec(yin_t,PredictedVector_t) \n",
    "      OutputLoss = 0.0\n",
    "      FitPredictions[iseq] = PredictedVector\n",
    "      for iloc in range(0,Nloc):\n",
    "        yy = yin[iseq,iloc]\n",
    "        yyhat = PredictedVector[iloc]\n",
    "\n",
    "        sum1 = 0.0\n",
    "        for i in range(0,NpredperseqTOT):\n",
    "          overcount += 1\n",
    "          weightedovercount += Predictionwgt[i]\n",
    "\n",
    "          if(math.isnan(yy[i])):\n",
    "            continue\n",
    "          weightedcount += Predictionwgt[i]\n",
    "          totalcount += 1\n",
    "          mse1 = ((yy[i]-yyhat[i])**2)\n",
    "          mse = mse1*sw[iloc,i]\n",
    "          if i < Npredperseq:\n",
    "            floss += mse\n",
    "          sum1 += mse\n",
    "          AbsEbyclass[i] += abs(yy[i] - yyhat[i])\n",
    "          RMSVbyclass[i] += yy[i]**2\n",
    "          AbsVbyclass[i] += abs(yy[i])\n",
    "          RMSEbyclass[i,0] += mse\n",
    "          countbyclass[i,0] += 1.0\n",
    "          if iseq < NumericalCutoff:\n",
    "            countbyclass[i,1] += 1.0\n",
    "            RMSEbyclass[i,1] += mse\n",
    "          else:\n",
    "            countbyclass[i,2] += 1.0\n",
    "            RMSEbyclass[i,2] += mse\n",
    "          if LocationBasedValidation:\n",
    "            if MappingtoTraining[iloc] >= 0:\n",
    "              ObsVbytimeandclass [iseq,i,1] += abs(yy[i])\n",
    "              Predbytimeandclass [iseq,i,1] += abs(yyhat[i])\n",
    "              RMSETRAINbyclass[i,0] += mse\n",
    "              countTRAINbyclass[i,0] += 1.0\n",
    "              if iseq < NumericalCutoff:\n",
    "                RMSETRAINbyclass[i,1] += mse\n",
    "                countTRAINbyclass[i,1] += 1.0\n",
    "              else:\n",
    "                RMSETRAINbyclass[i,2] += mse\n",
    "                countTRAINbyclass[i,2] += 1.0\n",
    "            if MappingtoValidation[iloc] >= 0:\n",
    "              ObsVbytimeandclass [iseq,i,2] += abs(yy[i])\n",
    "              Predbytimeandclass [iseq,i,2] += abs(yyhat[i])\n",
    "              RMSEVALbyclass[i,0] += mse\n",
    "              countVALbyclass[i,0] += 1.0\n",
    "              if iseq < NumericalCutoff:\n",
    "                RMSEVALbyclass[i,1] += mse\n",
    "                countVALbyclass[i,1] += 1.0\n",
    "              else:\n",
    "                RMSEVALbyclass[i,2] += mse\n",
    "                countVALbyclass[i,2] += 1.0\n",
    "          else:\n",
    "              ObsVbytimeandclass [iseq,i,1] += abs(yy[i])\n",
    "              Predbytimeandclass [iseq,i,1] += abs(yyhat[i])\n",
    "          ObsVbytimeandclass [iseq,i,0] += abs(yy[i])\n",
    "          Predbytimeandclass [iseq,i,0] += abs(yyhat[i])\n",
    "        weightedrmse1 += sum1\n",
    "        if LocationBasedValidation:\n",
    "          if MappingtoTraining[iloc] >= 0:\n",
    "            weightedrmse1TRAIN += sum1\n",
    "          if MappingtoValidation[iloc] >= 0:\n",
    "            weightedrmse1VAL += sum1\n",
    "        OutputLoss += sum1\n",
    "      StopWatch.stop('label3')\n",
    "      Ctime3 += StopWatch.get('label3', digits=4)\n",
    "      OutputLoss /= Nloc\n",
    "      countingcalls += 1\n",
    "      samplebar.update(1)\n",
    "      samplebar.set_postfix( Call = countingcalls, TotalLoss = OutputLoss)\n",
    "\n",
    "    print('Times ' + str(round(Ctime1,5))  + ' ' + str(round(Ctime3,5)) + ' TF ' + str(round(Ctime2,5)))\n",
    "    weightedrmse1 /= (Num_Seq * Nloc)\n",
    "    floss /= (Num_Seq * Nloc)\n",
    "    if LocationBasedValidation:\n",
    "      weightedrmse1TRAIN /= (Num_Seq * TrainingNloc)\n",
    "      if ValidationNloc>0:\n",
    "        weightedrmse1VAL /= (Num_Seq * ValidationNloc)\n",
    "    dloss = dloss.numpy()\n",
    "    eloss = eloss.numpy()\n",
    "    closs /= Num_Seq\n",
    "    dloss /= Num_Seq\n",
    "    eloss /= Num_Seq\n",
    "\n",
    "    current_time = timenow()\n",
    "    line1 = ''\n",
    "    global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
    "    GlobalLoss = weightedrmse1\n",
    "    if LocationBasedValidation:\n",
    "      line1 = ' Training ' + str(round(weightedrmse1TRAIN,6)) + ' Validation ' + str(round(weightedrmse1VAL,6))\n",
    "      GlobalTrainingLoss = weightedrmse1TRAIN\n",
    "      GlobalValidationLoss = weightedrmse1VAL\n",
    "    print( startbold + startred + current_time + ' DLPrediction Averages' + ' ' + RunName + ' ' + RunComment +  resetfonts)\n",
    "    line = LabelFit + ' ' + RunName + ' Weighted sum over predicted values ' + str(round(weightedrmse1,6))\n",
    "    line += ' No Encoding Preds ' + str(round(floss,6)) + line1\n",
    "    line += ' from loss function ' + str(round(closs,6)) + ' TF version ' + str(round(dloss,6)) + ' TFspec version ' + str(round(eloss,6))  \n",
    "    print(wraptotext(line))\n",
    "    print('Count ignoring NaN ' +str(round(weightedcount,4))+ ' Counting NaN ' + str(round(weightedovercount,4)), 70 )\n",
    "    print(' Unwgt Count no NaN ',totalcount, ' Unwgt Count with NaN ',overcount, ' Number Sequences ', Nloc*Num_Seq)\n",
    "\n",
    "    ObsvPred = np.sum( np.abs(ObsVbytimeandclass-Predbytimeandclass) , axis=0)\n",
    "    TotalObs = np.sum( ObsVbytimeandclass , axis=0)\n",
    "    SummedEbyclass = np.divide(ObsvPred,TotalObs)\n",
    "    RMSEbyclass1 = np.divide(RMSEbyclass,countbyclass) # NO SQRT\n",
    "    RMSEbyclass2 = np.sqrt(np.divide(RMSEbyclass[:,0],RMSVbyclass))\n",
    "    RelEbyclass = np.divide(AbsEbyclass, AbsVbyclass)\n",
    "    extracomments = []\n",
    "\n",
    "    line1 = '\\nErrors by Prediction Components -- class weights not included except in final Loss components\\n Name Count without NaN, '\n",
    "    line2 = 'sqrt(sum errors**2/sum target**2), sum(abs(error)/sum(abs(value), abs(sum(abs(value)-abs(pred)))/sum(abs(pred)'\n",
    "    print(wraptotext(startbold + startred + line1 + line2 + resetfonts))\n",
    "    countbasic = 0\n",
    "    \n",
    "    for i in range(0,NpredperseqTOT):\n",
    "      line = startbold + startred + ' AVG MSE '\n",
    "      for timecut in range(0,3):\n",
    "        line += TimeCutLabel[timecut] + 'Full ' + str(round(RMSEbyclass1[i,timecut],6)) + resetfonts\n",
    "      if LocationBasedValidation:\n",
    "        RTRAIN = np.divide(RMSETRAINbyclass[i],countTRAINbyclass[i])\n",
    "        RVAL = np.full(3,0.0, dtype =np.float32)\n",
    "        if countVALbyclass[i,0] > 0:\n",
    "          RVAL = np.divide(RMSEVALbyclass[i],countVALbyclass[i])\n",
    "        for timecut in range(0,3):\n",
    "          line += startbold + startpurple + TimeCutLabel[timecut] + 'TRAIN ' + resetfonts + str(round(RTRAIN[timecut],6))\n",
    "          line += startbold + ' VAL ' + resetfonts + str(round(RVAL[timecut],6))\n",
    "      else:\n",
    "        RTRAIN = RMSEbyclass1[i]\n",
    "        RVAL = np.full(3,0.0, dtype =np.float32)\n",
    "  \n",
    "      print(wraptotext(str(i) + ' ' + startbold + Predictionname[PredictionNameIndex[i]] + resetfonts + ' All Counts ' + str(round(countbyclass[i,0],0)) + ' IndE^2/IndObs^2 '\n",
    "       + str(round(100.0*RMSEbyclass2[i],2)) + '% IndE/IndObs ' + str(round(100.0*RelEbyclass[i],2)) + '% summedErr/SummedObs ' + str(round(100.0*SummedEbyclass[i,0],2)) + '%' +line ) )\n",
    "      Trainline = 'AVG MSE F=' + str(round(RTRAIN[0],8)) + ' S=' + str(round(RTRAIN[1],8))\n",
    "      Trainline += ' E=' + str(round(RTRAIN[2],8)) + ' TOTAL summedErr/SummedObs ' + str(round(100.0*SummedEbyclass[i,1],2)) + '%'\n",
    "      Valline = 'AVG MSE F=' + str(round(RVAL[0],6)) + ' S=' + str(round(RVAL[1],6)) + ' E=' + str(round(RVAL[2],6)) + ' TOTAL summedErr/SummedObs ' + str(round(100.0*SummedEbyclass[i,2],2)) + '%'\n",
    "      extracomments.append([Trainline, Valline] )\n",
    "      countbasic += 1\n",
    "      if countbasic == NumpredbasicperTime:\n",
    "        countbasic = 0\n",
    "        print(' ')\n",
    "\n",
    "# Don't use DLPrediction for Transformer Plots. Wait for DL2B,D,E\n",
    "    if modelflag == 1:\n",
    "      return FitPredictions\n",
    "    \n",
    "    FindNNSE(yin, FitPredictions)\n",
    "    print('\\n Next plots come from DLPrediction')\n",
    "    PredictedQuantity = -NumpredbasicperTime\n",
    "    for ifuture in range (0,1+LengthFutures):\n",
    "      increment = NumpredbasicperTime\n",
    "      if ifuture > 1:\n",
    "        increment = NumpredFuturedperTime\n",
    "      PredictedQuantity += increment\n",
    "      if not PlotPredictions[PredictedQuantity]:\n",
    "        continue\n",
    "      Dumpplot = False\n",
    "      if PredictedQuantity ==0:\n",
    "        Dumpplot = True\n",
    "      Location_summed_plot(ifuture, yin, FitPredictions, extracomments = extracomments, Dumpplot = Dumpplot)  \n",
    "\n",
    "    if IndividualPlots:\n",
    "      ProduceIndividualPlots(yin, FitPredictions)\n",
    "\n",
    "    if Earthquake and EarthquakeImagePlots:\n",
    "      ProduceSpatialQuakePlot(yin, FitPredictions)\n",
    "\n",
    "# Call DLprediction2F here if modelflag=0\n",
    "    DLprediction2F(Xin, yin, DLmodel, modelflag)\n",
    "\n",
    "    return FitPredictions   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW3dd6kVriWQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Spatial Earthquake Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgmRznp-ryGp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ProduceSpatialQuakePlot(Observations, FitPredictions):\n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + current_time + ' Produce Spatial Earthquake Plots ' + RunName + ' ' + RunComment + resetfonts)\n",
    "  dayindexmax = Num_Seq-Plottingdelay\n",
    "  Numdates = 4\n",
    "  denom = 1.0/np.float64(Numdates-1)\n",
    "  for plotdays in range(0,Numdates):\n",
    "    dayindexvalue = math.floor(0.1 + (plotdays*dayindexmax)*denom)\n",
    "    if dayindexvalue < 0:\n",
    "      dayindexvalue = 0\n",
    "    if dayindexvalue > dayindexmax:\n",
    "      dayindexvalue = dayindexmax \n",
    "    FixedTimeSpatialQuakePlot(dayindexvalue,Observations, FitPredictions)\n",
    "\n",
    "def EQrenorm(casesdeath,value):\n",
    "  if Plotrealnumbers:\n",
    "    predaveragevaluespointer = PredictionAverageValuesPointer[casesdeath]\n",
    "    newvalue = value/QuantityStatistics[predaveragevaluespointer,2] + QuantityStatistics[predaveragevaluespointer,0]\n",
    "    rootflag = QuantityTakeroot[predaveragevaluespointer]\n",
    "    if rootflag == 2:\n",
    "      newvalue = newvalue**2\n",
    "    if rootflag == 3:\n",
    "      newvalue = newvalue**3\n",
    "  else:\n",
    "    newvalue=value\n",
    "  return newvalue\n",
    "\n",
    "def FixedTimeSpatialQuakePlot(PlotTime,Observations, FitPredictions):\n",
    "  Actualday  = InitialDate + timedelta(days=(PlotTime+Tseq))\n",
    "  print(startbold + startred + ' Spatial Earthquake Plots ' + Actualday.strftime(\"%d/%m/%Y\") + ' ' + RunName + ' ' + RunComment + resetfonts)\n",
    "  NlocationsPlotted = Nloc\n",
    "  real = np.zeros([NumpredbasicperTime,NlocationsPlotted])\n",
    "  predict = np.zeros([NumpredbasicperTime,NlocationsPlotted])\n",
    "  print('Ranges for Prediction numbers/names/property pointer')\n",
    "  for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "    for iloc in range(0,NlocationsPlotted):\n",
    "        real[PredictedQuantity,iloc] = EQrenorm(PredictedQuantity,Observations[PlotTime, iloc, PredictedQuantity])\n",
    "        predict[PredictedQuantity,iloc] = EQrenorm(PredictedQuantity,FitPredictions[PlotTime, iloc, PredictedQuantity])\n",
    "    localmax1 = real[PredictedQuantity].max()\n",
    "    localmin1 = real[PredictedQuantity].min() \n",
    "    localmax2 = predict[PredictedQuantity].max()\n",
    "    localmin2 = predict[PredictedQuantity].min() \n",
    "    predaveragevaluespointer = PredictionAverageValuesPointer[PredictedQuantity]\n",
    "    expectedmax = QuantityStatistics[predaveragevaluespointer,1]\n",
    "    expectedmin = QuantityStatistics[predaveragevaluespointer,0]\n",
    "    print(' Real max/min ' + str(round(localmax1,3)) + ' ' + str(round(localmin1,3))\n",
    "      + ' Predicted max/min ' + str(round(localmax2,3)) + ' ' + str(round(localmin2,3))\n",
    "      + ' Overall max/min ' + str(round(expectedmax,3)) + ' ' + str(round(expectedmin,3))\n",
    "      + str(PredictedQuantity) + ' ' + Predictionbasicname[PredictedQuantity] + str(predaveragevaluespointer))\n",
    "\n",
    "  InputImages =[]\n",
    "  InputTitles =[]\n",
    "  for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "    InputImages.append(real[PredictedQuantity])\n",
    "    InputTitles.append(Actualday.strftime(\"%d/%m/%Y\") + ' Observed ' + Predictionbasicname[PredictedQuantity])\n",
    "    InputImages.append(predict[PredictedQuantity])\n",
    "    InputTitles.append(Actualday.strftime(\"%d/%m/%Y\") + ' Predicted ' + Predictionbasicname[PredictedQuantity])\n",
    "  plotimages(InputImages,InputTitles,NumpredbasicperTime,2)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIHPso_LrPJy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Organize Location v Time Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WsspqAef_yR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ProduceIndividualPlots(Observations, FitPredictions):\n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + current_time + ' Produce Individual Plots ' + RunName + ' ' + RunComment + resetfonts)\n",
    "# Find Best and Worst Locations\n",
    "  fips_b, fips_w = bestandworst(Observations, FitPredictions)\n",
    "  if Hydrology or Earthquake:\n",
    "    plot_by_fips(fips_b, Observations, FitPredictions)\n",
    "    plot_by_fips(fips_w, Observations, FitPredictions)\n",
    "  else:\n",
    "    plot_by_fips(6037, Observations, FitPredictions)\n",
    "    plot_by_fips(36061, Observations, FitPredictions)\n",
    "    plot_by_fips(17031, Observations, FitPredictions)\n",
    "    plot_by_fips(53033, Observations, FitPredictions)\n",
    "    if (fips_b!=6037) and (fips_b!=36061) and (fips_b!=17031) and (fips_b!=53033):\n",
    "        plot_by_fips(fips_b, Observations, FitPredictions)\n",
    "    if (fips_w!=6037) and (fips_w!=36061) and (fips_w!=17031) and (fips_w!=53033):\n",
    "        plot_by_fips(fips_w, Observations, FitPredictions)\n",
    "\n",
    "  # Plot top 10 largest cities\n",
    "    sortedcities = np.flip(np.argsort(Locationpopulation))\n",
    "    for pickout in range (0,10):\n",
    "      Locationindex = sortedcities[pickout]\n",
    "      fips = Locationfips[Locationindex]\n",
    "      if not(Hydrology or Earthquake):\n",
    "        if (fips == 6037 or fips == 36061 or fips == 17031 or fips == 53033):\n",
    "          continue\n",
    "      if (fips == fips_b or fips == fips_w):\n",
    "        continue\n",
    "      plot_by_fips(fips, Observations, FitPredictions)\n",
    "      \n",
    "  if LengthFutures > 1:\n",
    "      plot_by_futureindex(2, Observations, FitPredictions)\n",
    "  if LengthFutures > 6:\n",
    "      plot_by_futureindex(7, Observations, FitPredictions)\n",
    "  if LengthFutures > 11:\n",
    "      plot_by_futureindex(12, Observations, FitPredictions)\n",
    "  return\n",
    "\n",
    "def bestandworst(Observations, FitPredictions):\n",
    "    current_time = timenow()\n",
    "    print(startbold +  startred + current_time + ' ' + RunName + \" Best and Worst \" +RunComment + resetfonts)\n",
    "\n",
    "    keepabserrorvalues = np.zeros([Nloc,NumpredbasicperTime], dtype=np.float64)\n",
    "    keepRMSEvalues = np.zeros([Nloc,NumpredbasicperTime], dtype=np.float64)\n",
    "    testabserrorvalues = np.zeros(Nloc, dtype=np.float64)\n",
    "    testRMSEvalues = np.zeros(Nloc, dtype=np.float64)\n",
    "\n",
    "    real = np.zeros([NumpredbasicperTime,Num_Seq], dtype=np.float64)\n",
    "    predictsmall = np.zeros([NumpredbasicperTime,Num_Seq], dtype=np.float64) \n",
    "    c_error_props = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
    "    c_error_props = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
    " \n",
    "  \n",
    "    for icity in range(0,Nloc):\n",
    "      validcounts = np.zeros([NumpredbasicperTime], dtype=np.float64) \n",
    "      RMSE = np.zeros([NumpredbasicperTime], dtype=np.float64)\n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        for itime in range (0,Num_Seq):\n",
    "          if not math.isnan(Observations[itime, icity, PredictedQuantity]):\n",
    "            real[PredictedQuantity,itime] = Observations[itime, icity, PredictedQuantity]\n",
    "            predictsmall[PredictedQuantity,itime] = FitPredictions[itime, icity, PredictedQuantity]\n",
    "            validcounts[PredictedQuantity] += 1.0\n",
    "            RMSE[PredictedQuantity] += (Observations[itime, icity, PredictedQuantity]-FitPredictions[itime, icity, PredictedQuantity])**2\n",
    "        c_error_props[PredictedQuantity] = cumulative_error(predictsmall[PredictedQuantity], real[PredictedQuantity]) # abs(error) as percentage\n",
    "        keepabserrorvalues[icity,PredictedQuantity] = c_error_props[PredictedQuantity]\n",
    "        keepRMSEvalues[icity,PredictedQuantity] = RMSE[PredictedQuantity] *100. / validcounts[PredictedQuantity]\n",
    "\n",
    "      testabserror = 0.0\n",
    "      testRMSE = 0.0\n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "         testabserror += c_error_props[PredictedQuantity]\n",
    "         testRMSE += keepRMSEvalues[icity,PredictedQuantity]\n",
    "      testabserrorvalues[icity] = testabserror\n",
    "      testRMSEvalues[icity] = testRMSE\n",
    "    \n",
    "    sortingindex = np.argsort(testabserrorvalues)\n",
    "    bestindex = sortingindex[0]\n",
    "    worstindex = sortingindex[Nloc-1]\n",
    "    fips_b = Locationfips[bestindex]\n",
    "    fips_w = Locationfips[worstindex]\n",
    "\n",
    "    current_time = timenow()\n",
    "    print( startbold + \"\\n\" + current_time + \" Best \" + str(fips_b) + \" \" + Locationname[bestindex] + \" \" + Locationstate[bestindex] + ' ABS(error) ' + \n",
    "          str(round(testabserrorvalues[bestindex],2)) + ' RMSE ' + str(round(testRMSEvalues[bestindex],2)) + resetfonts)\n",
    "     \n",
    "    for topcities in range(0,10):\n",
    "      localindex = sortingindex[topcities]\n",
    "      printstring = str(topcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] + \" ABS(error) Total \" + str(round(testabserrorvalues[localindex],4)) + \" Components \" \n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        printstring += ' ' + str(round(keepabserrorvalues[localindex,PredictedQuantity],2))\n",
    "      print(printstring)\n",
    "    print(\"\\nlist RMSE\")\n",
    "    for topcities in range(0,9):\n",
    "      localindex = sortingindex[topcities]\n",
    "      printstring = str(topcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" RMSE Total \" + str(round(testRMSEvalues[localindex],4)) + \" Components \" \n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        printstring += ' ' + str(round(keepRMSEvalues[localindex,PredictedQuantity],2))\n",
    "      print(printstring)\n",
    "\n",
    "    print( startbold + \"\\n\" + current_time + \" Worst \" + str(fips_w) + \" \" + Locationname[worstindex] + \" \" + Locationstate[worstindex] + ' ABS(error) ' + \n",
    "          str(round(testabserrorvalues[worstindex],2)) + ' RMSE ' + str(round(testRMSEvalues[worstindex],2)) + resetfonts)\n",
    " \n",
    "    for badcities in range(Nloc-1,Nloc-11,-1):\n",
    "      localindex = sortingindex[badcities]\n",
    "      printstring = str(badcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" ABS(error) Total \" + str(round(testabserrorvalues[localindex],4)) + \" Components \" \n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        printstring += ' ' + str(round(keepabserrorvalues[localindex,PredictedQuantity],2))\n",
    "      print(printstring)\n",
    "    print(\"\\nlist RMSE\")\n",
    "    for badcities in range(0,9):\n",
    "      localindex = sortingindex[badcities]\n",
    "      printstring = str(badcities) + \") \" + str(Locationfips[localindex]) + \" \" + Locationname[localindex] +  \" RMSE Total \" + str(round(testRMSEvalues[localindex],4)) + \" Components \" \n",
    "      for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "        printstring += ' ' + str(round(keepRMSEvalues[localindex,PredictedQuantity],2))\n",
    "      print(printstring)\n",
    " \n",
    "    return fips_b,fips_w  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S2QaUybnLTb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Summed & By Location Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPYbNYwTq4s_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def setValTrainlabel(iValTrain):\n",
    "\n",
    "  if SeparateValandTrainingPlots:\n",
    "    if iValTrain == 0:\n",
    "      Overalllabel = 'Training ' \n",
    "      if GlobalTrainingLoss > 0.0001:\n",
    "        Overalllabel += str(round(GlobalTrainingLoss,5)) + ' '\n",
    "      if JournalSimplePrint:\n",
    "        Overalllabel = 'Training ' \n",
    "    if iValTrain == 1:\n",
    "      Overalllabel = 'Validation '\n",
    "      if GlobalValidationLoss > 0.0001:\n",
    "        Overalllabel += str(round(GlobalValidationLoss,5)) + ' '\n",
    "      if JournalSimplePrint:\n",
    "        Overalllabel = 'Validation '\n",
    "  else:\n",
    "    Overalllabel = 'Full ' + str(round(GlobalLoss,5)) + ' '\n",
    "    if JournalSimplePrint:\n",
    "      Overalllabel = 'Full '\n",
    "  if not JournalSimplePrint:\n",
    "    Overalllabel += RunName + ' '\n",
    "  return Overalllabel\n",
    "\n",
    "def Location_summed_plot(selectedfuture, Observations, FitPredictions,  fill=True, otherlabs= [], otherfits=[], extracomments = None, Dumpplot = False):\n",
    "    #  plot sum over locations\n",
    "\n",
    "    current_time = timenow()\n",
    "    print(wraptotext(startbold + startred + current_time + ' Location_summed_plot ' + RunName + ' ' + RunComment + resetfonts))\n",
    "    otherlen = len(otherlabs)\n",
    "    basiclength = Num_Seq\n",
    "    predictlength = LengthFutures\n",
    "    if (not UseFutures) or (selectedfuture > 0):\n",
    "        predictlength = 0\n",
    "    totallength = basiclength + predictlength\n",
    "    if extracomments is None:\n",
    "      extracomments = []\n",
    "      for PredictedQuantity in range(0,NpredperseqTOT):\n",
    "        extracomments.append([' ',''])\n",
    "\n",
    "    NumberValTrainLoops = 1\n",
    "    if SeparateValandTrainingPlots:\n",
    "      NumberValTrainLoops = 2\n",
    "\n",
    "    selectedfield = NumpredbasicperTime + NumpredFuturedperTime*(selectedfuture-1)\n",
    "    selectednumplots = NumpredFuturedperTime\n",
    "    if selectedfuture == 0:\n",
    "      selectedfield = 0    \n",
    "      selectednumplots = NumpredbasicperTime\n",
    "    ActualQuantity = np.arange(selectednumplots,dtype=np.int32)\n",
    "    if selectedfuture > 0:\n",
    "      for ipred in range(0,NumpredbasicperTime):\n",
    "        ifuture = FuturedPointer[ipred]\n",
    "        if ifuture >= 0:\n",
    "          ActualQuantity[ifuture] = ipred\n",
    "\n",
    "    real = np.zeros([selectednumplots,NumberValTrainLoops,basiclength])\n",
    "    predictsmall = np.zeros([selectednumplots,NumberValTrainLoops,basiclength])\n",
    "    predict = np.zeros([selectednumplots,NumberValTrainLoops,totallength])\n",
    "    if otherlen!=0:\n",
    "      otherpredict = np.zeros([otherlen,selectednumplots,NumberValTrainLoops, totallength])  \n",
    "\n",
    "      \n",
    "    for PlottedIndex in range(0,selectednumplots):\n",
    "      PredictedPos = PlottedIndex+selectedfield\n",
    "      ActualObservable = ActualQuantity[PlottedIndex]\n",
    "      for iValTrain in range(0,NumberValTrainLoops):\n",
    "\n",
    "        for iloc in range(0,Nloc):\n",
    "          if SeparateValandTrainingPlots:\n",
    "            if iValTrain == 0:\n",
    "              if MappingtoTraining[iloc] < 0:\n",
    "                continue\n",
    "            if iValTrain == 1:\n",
    "              if MappingtoTraining[iloc] >= 0:\n",
    "                continue\n",
    "          for itime in range (0,Num_Seq):\n",
    "            if np.math.isnan(Observations[itime, iloc, PredictedPos]):\n",
    "              real[PlottedIndex,iValTrain,itime] += normalizeforplot(PredictedPos, iloc, FitPredictions[itime, iloc, PredictedPos])\n",
    "            else:\n",
    "              real[PlottedIndex,iValTrain,itime] += normalizeforplot(PredictedPos, iloc, Observations[itime, iloc, PredictedPos])\n",
    "            predict[PlottedIndex,iValTrain,itime] += normalizeforplot(PredictedPos, iloc, FitPredictions[itime, iloc, PredictedPos])\n",
    "            for others in range (0,otherlen):\n",
    "              otherpredict[others,PlottedIndex,iValTrain,itime] += normalizeforplot(PredictedPos, iloc, FitPredictions[itime, iloc, PredictedPos] + otherfits[others,itime, iloc, PredictedPos])\n",
    "          if selectedfuture == 0:\n",
    "            if FuturedPointer[PlottedIndex] >= 0:\n",
    "              for ifuture in range(selectedfuture,LengthFutures):\n",
    "                jfuture = NumpredbasicperTime + NumpredFuturedperTime*ifuture\n",
    "                predict[PlottedIndex,iValTrain,Num_Seq+ifuture] += normalizeforplot(PredictedPos, iloc, FitPredictions[itime, iloc, FuturedPointer[PlottedIndex] + jfuture]) \n",
    "                for others in range (0,otherlen):\n",
    "                  otherpredict[others,PlottedIndex,iValTrain,Num_Seq+ifuture] += normalizeforplot(PredictedPos, iloc, FitPredictions[itime, iloc, PlottedIndex + jfuture] + otherfits[others, itime, iloc, PlottedIndex + jfuture])\n",
    "        for itime in range(0,basiclength):\n",
    "            predictsmall[PlottedIndex,iValTrain,itime] = predict[PlottedIndex,iValTrain,itime]\n",
    "          \n",
    "    error = np.absolute(real - predictsmall)\n",
    "    xsmall = np.arange(0,Num_Seq)\n",
    "\n",
    "    neededrows = math.floor((selectednumplots*NumberValTrainLoops +1.1)/2)\n",
    "    iValTrain = -1\n",
    "    PlottedIndex = -1\n",
    "    for rowloop in range(0,neededrows):\n",
    "      if JournalSimplePrint:\n",
    "        plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "      else:\n",
    "        plt.rcParams[\"figure.figsize\"] = [48,15]\n",
    "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "      for kplot in range (0,2):\n",
    "        if NumberValTrainLoops == 2:\n",
    "          iValTrain = kplot\n",
    "        else:\n",
    "          iValTrain = 0\n",
    "        if iValTrain == 0:\n",
    "          PlottedIndex +=1\n",
    "          if PlottedIndex > (selectednumplots-1):\n",
    "            PlottedIndex = selectednumplots-1          \n",
    "        Overalllabel = setValTrainlabel(iValTrain)\n",
    "        PredictedPos = PlottedIndex+selectedfield\n",
    "        ActualObservable = ActualQuantity[PlottedIndex]\n",
    "\n",
    "        eachplt = ax1\n",
    "        if kplot == 1:\n",
    "          eachplt = ax2\n",
    "        \n",
    "        Overalllabel = setValTrainlabel(iValTrain)\n",
    "\n",
    "        maxplot = np.float32(totallength)\n",
    "        if UseRealDatesonplots:\n",
    "          StartDate = np.datetime64(InitialDate).astype('datetime64[D]') + np.timedelta64(Tseq*Dailyunit + math.floor(Dailyunit/2),'D')\n",
    "          EndDate = StartDate + np.timedelta64(totallength*Dailyunit)\n",
    "          datemin, datemax = makeadateplot(figure,eachplt, datemin=StartDate, datemax=EndDate)\n",
    "          Dateplot = True\n",
    "          Dateaxis = np.empty(totallength, dtype = 'datetime64[D]')\n",
    "          Dateaxis[0] = StartDate\n",
    "          for idate in range(1,totallength):\n",
    "            Dateaxis[idate] = Dateaxis[idate-1] + np.timedelta64(Dailyunit,'D')\n",
    "        else:\n",
    "          Dateplot = False\n",
    "          datemin = 0.0\n",
    "          datemax = maxplot\n",
    "\n",
    "        sumreal = 0.0\n",
    "        sumerror = 0.0\n",
    "        for itime in range(0,Num_Seq):\n",
    "          sumreal += abs(real[PlottedIndex,iValTrain,itime])\n",
    "          sumerror += error[PlottedIndex,iValTrain,itime]\n",
    "        c_error = round(100.0*sumerror/sumreal,2)\n",
    "\n",
    "        if UseRealDatesonplots:\n",
    "          eachplt.plot(Dateaxis[0:real.shape[-1]],real[PlottedIndex,iValTrain,:], label=f'real')\n",
    "          eachplt.plot(Dateaxis,predict[PlottedIndex,iValTrain,:], label='prediction')            \n",
    "          eachplt.plot(Dateaxis[0:error.shape[-1]],error[PlottedIndex,iValTrain,:], label=f'error', color=\"red\")\n",
    "          for others in range (0,otherlen):\n",
    "            eachplt.plot(Dateaxis[0:otherpredict.shape[-1]],otherpredict[others,PlottedIndex,iValTrain,:], label=otherlabs[others])\n",
    "\n",
    "          if fill:\n",
    "            eachplt.fill_between(Dateaxis[0:predictsmall.shape[-1]], predictsmall[PlottedIndex,iValTrain,:], \n",
    "                                 real[PlottedIndex,iValTrain,:], alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(Dateaxis[0:error.shape[-1]], error[PlottedIndex,iValTrain,:], alpha=0.05, color=\"red\")\n",
    "\n",
    "        else:\n",
    "          eachplt.plot(real[PlottedIndex,iValTrain,:], label=f'real')\n",
    "          eachplt.plot(predict[PlottedIndex,iValTrain,:], label='prediction')\n",
    "          eachplt.plot(error[PlottedIndex,iValTrain,:], label=f'error', color=\"red\")\n",
    "          for others in range (0,otherlen):\n",
    "            eachplt.plot(otherpredict[others,PlottedIndex,iValTrain,:], label=otherlabs[others])\n",
    "\n",
    "          if fill:\n",
    "            eachplt.fill_between(xsmall, predictsmall[PlottedIndex,iValTrain,:], real[PlottedIndex,iValTrain,:], \n",
    "                                 alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(xsmall, error[PlottedIndex,iValTrain,:], alpha=0.05, color=\"red\")\n",
    "\n",
    "        \n",
    "        if Earthquake and AddSpecialstoSummedplots:\n",
    "          if NumberValTrainLoops == 2:\n",
    "            if iValTrain == 0:\n",
    "              Addfixedearthquakes(eachplt, datemin, datemax, quakecolor = 'black', Dateplot = Dateplot, \n",
    "                                  vetoquake = PrimaryTrainingvetoquake)\n",
    "              Addfixedearthquakes(eachplt, datemin, datemax,  quakecolor = 'purple', Dateplot = Dateplot, \n",
    "                                  vetoquake = SecondaryTrainingvetoquake)\n",
    "            else:\n",
    "              Addfixedearthquakes(eachplt, datemin, datemax,  quakecolor = 'black', Dateplot = Dateplot, \n",
    "                                  vetoquake = PrimaryValidationvetoquake)\n",
    "              Addfixedearthquakes(eachplt, datemin, datemax, quakecolor = 'purple', Dateplot = Dateplot, \n",
    "                                  vetoquake = SecondaryValidationvetoquake)\n",
    "          else:\n",
    "            vetoquake = np.full(numberspecialeqs,False, dtype = np.bool)\n",
    "            Addfixedearthquakes(eachplt, datemin, datemax,  quakecolor = 'black', Dateplot = Dateplot, \n",
    "                                vetoquake = vetoquake)\n",
    "          \n",
    "        extrastring = Overalllabel + current_time + ' ' + RunName + \" \" \n",
    "        extrastring += f\"Length={Num_Seq}, Location Summed Results {Predictionbasicname[ActualObservable]}, \"\n",
    "        yaxislabel = Predictionbasicname[ActualObservable]\n",
    "        if JournalSimplePrint:\n",
    "          extrastring = Overalllabel +  ' ' + RunName + \" \"\n",
    "        newyaxislabel = \"\"\n",
    "        if selectedfuture > 0:\n",
    "          yaxislabel = Predictionname[PredictionNameIndex[PredictedPos]]\n",
    "          extrastring += \" FUTURE \" + yaxislabel\n",
    "          if JournalSimplePrint:\n",
    "            newyaxislabel = yaxislabel.replace(\"Now\", \"at Now +\")\n",
    "            newyaxislabel = newyaxislabel.replace(\"Back\",\"Back from Now +\")\n",
    "          else:\n",
    "            newyaxislabel = yaxislabel.replace(\"Months\",\"Months\\n\")\n",
    "            newyaxislabel = newyaxislabel.replace(\"weeks\",\"weeks\\n\")\n",
    "            newyaxislabel = newyaxislabel.replace(\"year\",\"year\\n\")\n",
    "            eachplt.text(0.05,0.75,\"FUTURE \\n\" + newyaxislabel,transform=eachplt.transAxes, color=\"black\",fontsize=FONTSIZE, fontweight='bold')\n",
    "        else:\n",
    "          extrastring += Predictionbasicname[ActualObservable]\n",
    "        if  not JournalSimplePrint:\n",
    "          extrastring += extracomments[PredictedPos][iValTrain]\n",
    "        eachplt.set_title('\\n'.join(wrap(extrastring,130)),fontsize=FONTSIZE)\n",
    "        if Dateplot:\n",
    "          eachplt.set_xlabel('Years',fontsize=FONTSIZE)\n",
    "        else:\n",
    "          eachplt.set_xlabel(TimeIntervalUnitName+'s',fontsize=FONTSIZE)\n",
    "        eachplt.set_ylabel(yaxislabel, color=\"black\",fontweight='bold',fontsize=FONTSIZE)\n",
    "        if JournalSimplePrint:\n",
    "          eachplt.tick_params(axis='both', labelsize = FONTSIZE)\n",
    "          eachplt.tick_params('x', direction = 'in', length=15, width=3, which='major')\n",
    "          eachplt.tick_params('y', direction = 'in', length=10, width=3, which='major')\n",
    "          if ActualObservable >= 2:\n",
    "            loclabel = 'center right'\n",
    "            eachplt.legend(fontsize=FONTSIZE, loc = loclabel,bbox_to_anchor=(0.5, 0., 0.5, 0.5) )\n",
    "          else:\n",
    "            loclabel = 'center left'\n",
    "            eachplt.legend(fontsize=FONTSIZE, loc = loclabel)\n",
    "        else:\n",
    "          eachplt.legend()\n",
    "        eachplt.grid(False)\n",
    "\n",
    "      figure.tight_layout()\n",
    "      if Dumpplot and Dumpoutkeyplotsaspics:\n",
    "        VT = 'Both'\n",
    "        if NumberValTrainLoops == 1:\n",
    "          VT='Full'\n",
    "        plt.savefig(APPLDIR +'/Outputs/DLResults' + VT + str(PredictedPos) +RunName + '.png ',format='png')\n",
    "      mysavefig(\"\")\n",
    "      plt.show()\n",
    "\n",
    "# Produce more detailed plots in time\n",
    "# ONLY done for first quantity as needed in HYdrology\n",
    "    splitsize = Plotsplitsize\n",
    "    if splitsize <= 1:\n",
    "      return\n",
    "    Numpoints = math.floor((Num_Seq+0.001)/splitsize)\n",
    "    extraone = Num_Seq%Numpoints\n",
    "\n",
    "    neededrows = math.floor((splitsize*NumberValTrainLoops +1.1)/2)\n",
    "    iValTrain = -1\n",
    "    PlottedIndex = 0\n",
    "    iseqnew = 0\n",
    "    counttimes = 0\n",
    "    for rowloop in range(0,neededrows):\n",
    "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "      for kplot in range (0,2):\n",
    "        if NumberValTrainLoops == 2:\n",
    "          iValTrain = kplot\n",
    "        else:\n",
    "          iValTrain = 0\n",
    "        Overalllabel = setValTrainlabel(iValTrain)\n",
    "        eachplt = ax1\n",
    "        if kplot == 1:\n",
    "          eachplt = ax2\n",
    "        sumreal = 0.0\n",
    "        sumerror = 0.0\n",
    "\n",
    "        if iValTrain == 0:\n",
    "          iseqold = iseqnew\n",
    "          iseqnew = iseqold + Numpoints\n",
    "          if counttimes < extraone:\n",
    "            iseqnew +=1\n",
    "          counttimes += 1\n",
    "        for itime in range(iseqold,iseqnew):\n",
    "          sumreal += abs(real[PlottedIndex,iValTrain,itime])\n",
    "          sumerror += error[PlottedIndex,iValTrain,itime]\n",
    "        c_error = round(100.0*sumerror/sumreal,2)\n",
    "\n",
    "        eachplt.plot(xsmall[iseqold:iseqnew],predict[PlottedIndex,iValTrain,iseqold:iseqnew], label='prediction')\n",
    "        eachplt.plot(xsmall[iseqold:iseqnew],real[PlottedIndex,iValTrain,iseqold:iseqnew], label=f'real')\n",
    "        eachplt.plot(xsmall[iseqold:iseqnew],error[PlottedIndex,iValTrain,iseqold:iseqnew], label=f'error', color=\"red\")\n",
    "\n",
    "        if fill:\n",
    "            eachplt.fill_between(xsmall[iseqold:iseqnew], predictsmall[PlottedIndex,iValTrain,iseqold:iseqnew], real[PlottedIndex,iValTrain,iseqold:iseqnew], alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(xsmall[iseqold:iseqnew], error[PlottedIndex,iValTrain,iseqold:iseqnew], alpha=0.05, color=\"red\")\n",
    "\n",
    "        extrastring = Overalllabel + current_time + ' ' + RunName + \" \" + f\"Range={iseqold}, {iseqnew} Rel Error {c_error} Location Summed Results {Predictionbasicname[PredictedPos]}, \"\n",
    "        eachplt.set_title('\\n'.join(wrap(extrastring,70)))\n",
    "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
    "        eachplt.set_ylabel(Predictionbasicname[PredictedPos])\n",
    "        eachplt.grid(True)\n",
    "        eachplt.legend()\n",
    "      figure.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "def normalizeforplot(casesdeath,Locationindex,value):\n",
    "\n",
    "    if np.math.isnan(value):\n",
    "      return value\n",
    "    if Plotrealnumbers:\n",
    "      predaveragevaluespointer = PredictionAverageValuesPointer[casesdeath]\n",
    "      newvalue = value/QuantityStatistics[predaveragevaluespointer,2] + QuantityStatistics[predaveragevaluespointer,0]\n",
    "      rootflag = QuantityTakeroot[predaveragevaluespointer]\n",
    "      if rootflag == 2:\n",
    "        newvalue = newvalue**2\n",
    "      if rootflag == 3:\n",
    "        newvalue = newvalue**3\n",
    "    else:\n",
    "      newvalue = value\n",
    "    if PopulationNorm:\n",
    "      newvalue *= Locationpopulation[Locationindex]\n",
    "    return newvalue\n",
    "\n",
    "# PLOT individual city data\n",
    "def plot_by_fips(fips, Observations, FitPredictions, dots=True, fill=True):\n",
    "    Locationindex = FIPSintegerlookup[fips]\n",
    "    current_time = timenow()\n",
    "    print(startbold + startred + current_time + ' plot by location ' + str(Locationindex) + ' ' + str(fips) + ' ' + Locationname[Locationindex] + ' ' +RunName + ' ' + RunComment + resetfonts)\n",
    "\n",
    "    basiclength = Num_Seq\n",
    "    predictlength = LengthFutures\n",
    "    if not UseFutures:\n",
    "        predictlength = 0\n",
    "    totallength = basiclength + predictlength\n",
    "    real = np.zeros([NumpredbasicperTime,basiclength])\n",
    "    predictsmall = np.zeros([NumpredbasicperTime,basiclength])\n",
    "    predict = np.zeros([NumpredbasicperTime,totallength]) \n",
    "\n",
    "    for PredictedQuantity in range(0,NumpredbasicperTime):\n",
    "      for itime in range (0,Num_Seq):\n",
    "        if np.math.isnan(Observations[itime, Locationindex, PredictedQuantity]):\n",
    "          Observations[itime, Locationindex, PredictedQuantity] = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
    "        else:\n",
    "          real[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, Locationindex, Observations[itime, Locationindex, PredictedQuantity])\n",
    "          predict[PredictedQuantity,itime] = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, Locationindex, PredictedQuantity])\n",
    "      if FuturedPointer[PredictedQuantity] >= 0:\n",
    "        for ifuture in range(0,LengthFutures):\n",
    "          jfuture = NumpredbasicperTime + NumpredFuturedperTime*ifuture\n",
    "          predict[PredictedQuantity,Num_Seq+ifuture] += normalizeforplot(PredictedQuantity,Locationindex, \n",
    "                                          FitPredictions[itime, Locationindex, FuturedPointer[PredictedQuantity] + jfuture])\n",
    "      for itime in range(0,basiclength):\n",
    "          predictsmall[PredictedQuantity,itime] = predict[PredictedQuantity,itime]\n",
    "        \n",
    "    error = np.absolute(real - predictsmall)\n",
    "    xsmall = np.arange(0,Num_Seq)\n",
    "\n",
    "    neededrows = math.floor((NumpredbasicperTime +1.1)/2)\n",
    "    iplot = -1\n",
    "    for rowloop in range(0,neededrows):\n",
    "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "      for kplot in range (0,2):\n",
    "        iplot +=1\n",
    "        if iplot > (NumpredbasicperTime-1):\n",
    "          iplot = NumpredbasicperTime-1\n",
    "        eachplt = ax1\n",
    "        if kplot == 1:\n",
    "          eachplt = ax2\n",
    "\n",
    "        sumreal = 0.0\n",
    "        sumerror = 0.0\n",
    "        for itime in range(0,Num_Seq):\n",
    "          sumreal += abs(real[iplot,itime])\n",
    "          sumerror += error[iplot,itime]\n",
    "        c_error = round(100.0*sumerror/sumreal,2)\n",
    "        RMSEstring = ''\n",
    "        if not Plotrealnumbers:\n",
    "          sumRMSE = 0.0\n",
    "          count = 0.0\n",
    "          for itime in range(0,Num_Seq):\n",
    "            sumRMSE += (real[iplot,itime] - predict[iplot,itime])**2\n",
    "            count += 1.0\n",
    "          RMSE_error = round(100.0*sumRMSE/count,4)\n",
    "          RMSEstring = ' RMSE ' + str(RMSE_error)\n",
    "\n",
    "        x = list(range(0, totallength))\n",
    "        if dots:\n",
    "            eachplt.scatter(x, predict[iplot])\n",
    "            eachplt.scatter(xsmall, real[iplot])\n",
    "\n",
    "        eachplt.plot(predict[iplot], label=f'{fips} prediction')\n",
    "        eachplt.plot(real[iplot], label=f'{fips} real')\n",
    "        eachplt.plot(error[iplot], label=f'{fips} error', color=\"red\")\n",
    "        if fill:\n",
    "            eachplt.fill_between(xsmall, predictsmall[iplot], real[iplot], alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(xsmall, error[iplot], alpha=0.05, color=\"red\")\n",
    "\n",
    "        name = Locationname[Locationindex]\n",
    "        if Plotrealnumbers:\n",
    "            name = \"Actual Numbers \" + name\n",
    "        stringpopulation = \" \"\n",
    "        if not Hydrology:\n",
    "          stringpopulation = \" Population \" +str(Locationpopulation[Locationindex])\n",
    "\n",
    "        titlestring = current_time + ' ' + RunName + f\" {name}, Label={fips}\" + stringpopulation + f\" Length={Num_Seq}, Abs Rel Error={c_error}%\" + RMSEstring + ' ' + RunName\n",
    "        eachplt.set_title('\\n'.join(wrap(titlestring,70)))\n",
    "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
    "        eachplt.set_ylabel(Predictionbasicname[iplot])\n",
    "        eachplt.grid(True)\n",
    "        eachplt.legend()\n",
    "\n",
    "      figure.tight_layout()\n",
    "      plt.show();\n",
    "\n",
    "\n",
    "def cumulative_error(real,predicted):\n",
    "  error = np.absolute(real-predicted).sum()\n",
    "  basevalue = np.absolute(real).sum()\n",
    "  return 100.0*error/basevalue\n",
    "\n",
    "# Plot summed results by Prediction Type\n",
    "# selectedfuture one more than usual future index\n",
    "def plot_by_futureindex(selectedfuture, Observations, FitPredictions, fill=True, extrastring=''):\n",
    "    current_time = timenow()\n",
    "    print(startbold + startred + current_time + ' plot by Future Index ' + str(selectedfuture) + ' ' + RunName + ' ' + RunComment + resetfonts)\n",
    "\n",
    "    selectedfield = NumpredbasicperTime + NumpredFuturedperTime*(selectedfuture-1)\n",
    "    if selectedfuture == 0:\n",
    "      selectedfield = 0\n",
    "    real = np.zeros([NumpredFuturedperTime,Num_Seq])\n",
    "    predictsmall = np.zeros([NumpredFuturedperTime,Num_Seq])\n",
    "    validdata = 0\n",
    "\n",
    "    for PredictedQuantity in range(0,NumpredFuturedperTime):\n",
    "      for iloc in range(0,Nloc):\n",
    "        for itime in range (0,Num_Seq):\n",
    "          real[PredictedQuantity,itime] += Observations[itime, iloc, selectedfield+PredictedQuantity]\n",
    "          predictsmall[PredictedQuantity,itime] += FitPredictions[itime, iloc, selectedfield+PredictedQuantity]\n",
    "      for itime in range (0,Num_Seq):\n",
    "        if np.math.isnan(real[PredictedQuantity,itime]):\n",
    "            real[PredictedQuantity,itime] = predictsmall[PredictedQuantity,itime]\n",
    "        else:\n",
    "            if PredictedQuantity == 0:\n",
    "              validdata += 1    \n",
    "\n",
    "    error = np.absolute(real - predictsmall)\n",
    "    xsmall = np.arange(0,Num_Seq)\n",
    "\n",
    "    neededrows = math.floor((NumpredFuturedperTime +1.1)/2)\n",
    "    iplot = -1\n",
    "    for rowloop in range(0,neededrows):\n",
    "      plt.rcParams[\"figure.figsize\"] = [16,6]\n",
    "      figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "      for kplot in range (0,2):\n",
    "        iplot +=1\n",
    "        if iplot > (NumpredbasicperTime-1):\n",
    "          iplot = NumpredbasicperTime-1\n",
    "        eachplt = ax1\n",
    "        if kplot == 1:\n",
    "          eachplt = ax2\n",
    "        sumreal = 0.0\n",
    "        sumerror = 0.0\n",
    "        for itime in range(0,Num_Seq):\n",
    "          sumreal += abs(real[iplot,itime])\n",
    "          sumerror += error[iplot,itime]\n",
    "        c_error = round(100.0*sumerror/sumreal,2)\n",
    "\n",
    "        eachplt.plot(predictsmall[iplot,:], label='prediction')\n",
    "        eachplt.plot(real[iplot,:], label=f'real')\n",
    "        eachplt.plot(error[iplot,:], label=f'error', color=\"red\")\n",
    "\n",
    "        if fill:\n",
    "            eachplt.fill_between(xsmall, predictsmall[iplot,:], real[iplot,:], alpha=0.1, color=\"grey\")\n",
    "            eachplt.fill_between(xsmall, error[iplot,:], alpha=0.05, color=\"red\")\n",
    "        errorstring= \" Error % \" + str(c_error)\n",
    "        printstring = current_time + \" Future Index \" + str(selectedfuture) + \" \" + RunName \n",
    "        printstring += \" \" + f\"Length={Num_Seq}, Location Summed Results {Predictionbasicname[iplot]}, \" + errorstring + \" \" + extrastring\n",
    "        eachplt.set_title('\\n'.join(wrap(printstring,70)))\n",
    "        eachplt.set_xlabel(TimeIntervalUnitName+'s')\n",
    "        eachplt.set_ylabel(Predictionbasicname[iplot])\n",
    "        eachplt.grid(True)\n",
    "        eachplt.legend()\n",
    "      figure.tight_layout()\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVxWS_-p5T_N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Calculate NNSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHr9p5LC5Z-k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y_true - y_pred)))\n",
    "\n",
    "def normalized_nash_sutcliffe_efficiencySTavg(y_true, y_pred): # axis 0 space 1 time\n",
    "    NSE = 1 - np.sum (np.square(y_true - y_pred) ) / np.sum( np.square(y_true - np.mean(y_true)) )\n",
    "    return 1 / ( 2 - NSE)\n",
    "\n",
    "def normalized_nash_sutcliffe_efficiencySsum(y_true, y_pred): # axis 0 space 1 time\n",
    "    NSE = 1 - np.sum (np.square(np.sum(y_true,axis=0) - np.sum(y_pred,axis=0) )) / np.sum( np.square(np.sum(y_true,axis=0) - np.mean(np.sum(y_true,axis=0)) ) )\n",
    "    return 1 / ( 2 - NSE)\n",
    "\n",
    "def symmetric_mean_absolute_percentage(y_true, y_pred):\n",
    "    value = 2*abs(y_true - y_pred) / (abs(y_true) + abs(y_pred))\n",
    "    # for cases when both ground truth and predicted value are zero\n",
    "    value = np.where(np.isnan(value), 0, value)   \n",
    "    return np.mean(value)\n",
    "    \n",
    "# Calculate NNSE\n",
    "# Sum (Obsevations - Mean)^2 / [Sum (Obsevations - Mean)^2 + Sum(Observations-Predictions)^2]\n",
    "def FindNNSE(Observations, FitPredictions, Label=''):\n",
    "\n",
    "  NNSEList = np.empty(NpredperseqTOT, dtype = np.int)\n",
    "  NumberNNSEcalc = 0\n",
    "  for ipred in range(0,NpredperseqTOT):\n",
    "    if CalculateNNSE[ipred]:\n",
    "      NNSEList[NumberNNSEcalc] = ipred\n",
    "      NumberNNSEcalc +=1\n",
    "  if NumberNNSEcalc == 0:\n",
    "    return\n",
    "  StoreNNSE = np.zeros([Nloc,NumberNNSEcalc], dtype = np.float64)\n",
    "  YTRUE = np.zeros([Nloc,Num_Seq,NumberNNSEcalc], dtype = np.float64)\n",
    "  YPRED = np.zeros([Nloc,Num_Seq,NumberNNSEcalc], dtype = np.float64)\n",
    "\n",
    "  current_time = timenow()\n",
    "  print(wraptotext(startbold + startred + current_time + ' Calculate NNSE ' + Label + ' ' +RunName + ' ' + RunComment + resetfonts))\n",
    "  for NNSEpredindex in range(0,NumberNNSEcalc):\n",
    "    PredictedQuantity = NNSEList[NNSEpredindex]\n",
    "    averageNNSE = 0.0\n",
    "    averageNNSETraining = 0.0\n",
    "    averageNNSEValidation = 0.0\n",
    "    line = ''\n",
    "    for Locationindex in range(0, Nloc):\n",
    "      QTObssq = 0.0\n",
    "      QTDiffsq = 0.0\n",
    "      QTObssum = 0.0\n",
    "      for itime in range (0,Num_Seq):\n",
    "        Observed = Observations[itime, Locationindex, PredictedQuantity]\n",
    "        if np.math.isnan(Observed):\n",
    "          Observed = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
    "        real = normalizeforplot(PredictedQuantity, Locationindex, Observed)\n",
    "        predict = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, \n",
    "                                    Locationindex, PredictedQuantity])\n",
    "        YTRUE[Locationindex, itime, NNSEpredindex] = real\n",
    "        YPRED[Locationindex, itime, NNSEpredindex] = predict\n",
    "        QTObssq += real**2\n",
    "        QTDiffsq += (real-predict)**2\n",
    "        QTObssum += real\n",
    "      Obsmeasure = QTObssq - (QTObssum**2 / Num_Seq )\n",
    "      StoreNNSE[Locationindex,NNSEpredindex] =  Obsmeasure / (Obsmeasure +QTDiffsq )\n",
    "      if MappingtoTraining[Locationindex] >= 0:\n",
    "        averageNNSETraining += StoreNNSE[Locationindex,NNSEpredindex]\n",
    "      if MappingtoValidation[Locationindex] >= 0:\n",
    "        averageNNSEValidation += StoreNNSE[Locationindex,NNSEpredindex]\n",
    "      averageNNSE += StoreNNSE[Locationindex,NNSEpredindex]\n",
    "      line += str(round(StoreNNSE[Locationindex,NNSEpredindex],3)) + ' '\n",
    "    \n",
    "    if ValidationNloc > 0:\n",
    "      averageNNSEValidation = averageNNSEValidation / ValidationNloc\n",
    "    averageNNSETraining = averageNNSETraining / TrainingNloc\n",
    "    averageNNSE = averageNNSE / Nloc\n",
    "\n",
    "# Location Summed    \n",
    "    QTObssq = 0.0\n",
    "    QTDiffsq = 0.0\n",
    "    QTObssum = 0.0\n",
    "    QTObssqT = 0.0\n",
    "    QTDiffsqT = 0.0\n",
    "    QTObssumT = 0.0\n",
    "    QTObssqV = 0.0\n",
    "    QTDiffsqV = 0.0\n",
    "    QTObssumV = 0.0\n",
    "    for itime in range (0,Num_Seq):\n",
    "      real = 0.0\n",
    "      predict = 0.0\n",
    "      realT = 0.0\n",
    "      predictT = 0.0\n",
    "      realV = 0.0\n",
    "      predictV = 0.0\n",
    "      for Locationindex in range(0, Nloc):\n",
    "        Observed = Observations[itime, Locationindex, PredictedQuantity]\n",
    "        if np.math.isnan(Observed):\n",
    "          Observed = FitPredictions[itime, Locationindex, PredictedQuantity]\n",
    "        localreal = normalizeforplot(PredictedQuantity, Locationindex, Observed)\n",
    "        localpredict = normalizeforplot(PredictedQuantity, Locationindex, FitPredictions[itime, \n",
    "                                    Locationindex, PredictedQuantity])\n",
    "        real += localreal\n",
    "        predict += localpredict\n",
    "        if MappingtoTraining[Locationindex] >= 0:\n",
    "          realT += localreal\n",
    "          predictT += localpredict\n",
    "        if MappingtoValidation[Locationindex] >= 0:\n",
    "          realV  += localreal\n",
    "          predictV += localpredict\n",
    "\n",
    "      QTObssq += real**2\n",
    "      QTDiffsq += (real-predict)**2\n",
    "      QTObssum += real\n",
    "      QTObssqT += realT**2\n",
    "      QTDiffsqT += (realT-predictT)**2\n",
    "      QTObssumT += realT\n",
    "      QTObssqV += realV**2\n",
    "      QTDiffsqV += (realV-predictV)**2\n",
    "      QTObssumV += realV\n",
    "    Obsmeasure = QTObssq - (QTObssum**2 / Num_Seq )\n",
    "    SummedNNSE =  Obsmeasure / (Obsmeasure +QTDiffsq )\n",
    "    ObsmeasureT = QTObssqT - (QTObssumT**2 / Num_Seq )\n",
    "    SummedNNSET =  ObsmeasureT / (ObsmeasureT +QTDiffsqT )\n",
    "    ObsmeasureV = QTObssqV - (QTObssumV**2 / Num_Seq )\n",
    "    if ValidationNloc > 0:\n",
    "      SummedNNSEV =  ObsmeasureV / (ObsmeasureV +QTDiffsqV )\n",
    "    else:\n",
    "      SummedNNSEV =  0.0\n",
    "\n",
    "    line = ''\n",
    "    if PredictedQuantity >= NumpredbasicperTime:\n",
    "      line = startred + 'Future ' + resetfonts\n",
    "    print(wraptotext(line + 'NNSE ' + startbold + Label + ' ' + str(PredictedQuantity) + ' ' + Predictionname[PredictionNameIndex[PredictedQuantity]] + startred + ' Averaged ' +\n",
    "          str(round(averageNNSE,3)) + resetfonts + ' Training ' + str(round(averageNNSETraining,3)) +\n",
    "          ' Validation ' + str(round(averageNNSEValidation,3)) + startred + startbold + ' Summed ' +  \n",
    "          str(round(SummedNNSE,3)) + resetfonts + ' Training ' + str(round(SummedNNSET,3)) +\n",
    "          ' Validation ' + str(round(SummedNNSEV,3)), size=200))\n",
    "\n",
    "  for NNSEpredindex in range(0,NumberNNSEcalc):\n",
    "    PredictedQuantity = NNSEList[NNSEpredindex]\n",
    "    MAE = mean_absolute_error(YTRUE[:,:,NNSEpredindex], YPRED[:,:,NNSEpredindex])\n",
    "    RMS = root_mean_squared_error(YTRUE[:,:,NNSEpredindex], YPRED[:,:,NNSEpredindex])\n",
    "    NNSE1 = normalized_nash_sutcliffe_efficiencySsum(YTRUE[:,:,NNSEpredindex], YPRED[:,:,NNSEpredindex])\n",
    "    NNSE2 = normalized_nash_sutcliffe_efficiencySTavg(YTRUE[:,:,NNSEpredindex], YPRED[:,:,NNSEpredindex])\n",
    "    SMAP = symmetric_mean_absolute_percentage(YTRUE[:,:,NNSEpredindex], YPRED[:,:,NNSEpredindex])\n",
    "\n",
    "    line = ''\n",
    "    if PredictedQuantity >= NumpredbasicperTime:\n",
    "      line = startred + 'Future ' + resetfonts\n",
    "    print(wraptotext(line + startbold + Label + ' ' + str(PredictedQuantity) + ' ' + Predictionname[PredictionNameIndex[PredictedQuantity]] + resetfonts + ' MAE ' + \n",
    "                     str(round(MAE,5)) + ' RMS '  + str(round(RMS,5)) + ' NNSE Space Sum ' + str(round(NNSE1,5)) + ' NNSE Space-Time Avg ' + \n",
    "                     str(round(NNSE2,5)) + ' SMAP ' + str(round(SMAP,5)), size=200 ))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOO6uzE1FUa1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def weightedcustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
    "    tupl = np.shape(y_actual)\n",
    "\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "\n",
    "def numpycustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
    "    tupl = np.shape(y_actual)\n",
    "\n",
    "    flagGCF = np.isnan(y_actual)\n",
    "    y_actual = y_actual[np.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[np.logical_not(flagGCF)]\n",
    "    sw = sample_weight[np.logical_not(flagGCF)]\n",
    "    tensordiff = np.sum(np.multiply(np.square(y_actual-y_pred),sw))\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "\n",
    "def weightedcustom_lossGCF1(y_actual, y_pred, sample_weight):\n",
    "    tupl = np.shape(y_actual)\n",
    "\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeDyzoVynCHL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJylkkL9AvsV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def custom_lossGCF1(y_actual,y_pred):\n",
    "    tupl = np.shape(y_actual)\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.math.square(y_actual-y_pred))\n",
    "\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def custom_lossGCF1spec(y_actual,y_pred):\n",
    "    global tensorsw\n",
    "    tupl = np.shape(y_actual)\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    sw = tensorsw[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
    "\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "\n",
    "def custom_lossGCF1A(y_actual,y_pred):\n",
    "    print(np.shape(y_actual), np.shape(y_pred))\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.square(y_actual-y_pred)\n",
    "    return tf.math.reduce_mean(tensordiff)\n",
    "\n",
    "# Basic TF does NOT supply sample_weight\n",
    "def custom_lossGCF1B(y_actual,y_pred,sample_weight=None):\n",
    "    tupl = np.shape(y_actual)\n",
    "\n",
    "    flagGCF = tf.math.is_nan(y_actual)\n",
    "    y_actual = y_actual[tf.math.logical_not(flagGCF)]\n",
    "    y_pred = y_pred[tf.math.logical_not(flagGCF)]\n",
    "    sw = sample_weight[tf.math.logical_not(flagGCF)]\n",
    "    tensordiff = tf.math.reduce_sum(tf.multiply(tf.math.square(y_actual-y_pred),sw))\n",
    "    if len(tupl) >= 2:\n",
    "      tensordiff /= tupl[0]\n",
    "    if len(tupl) >= 3:\n",
    "      tensordiff /= tupl[1]\n",
    "    if len(tupl) >= 4:\n",
    "      tensordiff /= tupl[2]\n",
    "    return tensordiff\n",
    "    \n",
    "def custom_lossGCF4(y_actual,y_pred):\n",
    "    tensordiff = y_actual-y_pred\n",
    "    newtensordiff = tf.where(tf.math.is_nan(tensordiff), tf.zeros_like(tensordiff), tensordiff)\n",
    "    return tf.math.reduce_mean(tf.math.square(newtensordiff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIWDP9I8myNQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Utility: Shuffle, Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIB3yMlo7kFI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SetSpacetime(BasicTimes):\n",
    "  global GlobalTimeMask\n",
    "  Time = None\n",
    "  if (MaskingOption == 0) or (not GlobalSpacetime):\n",
    "    return Time\n",
    "  NumTOTAL = BasicTimes.shape[1]\n",
    "  BasicTimes = BasicTimes.astype(np.int16)\n",
    "  BasicTimes = np.reshape(BasicTimes,(BasicTimes.shape[0],NumTOTAL,1))\n",
    "  addons = np.arange(0,Tseq,dtype =np.int16)\n",
    "  addons = np.reshape(addons,(1,1,Tseq))\n",
    "  Time = BasicTimes+addons\n",
    "  Time = np.reshape(Time,(BasicTimes.shape[0], NumTOTAL*Tseq))\n",
    "  BasicPureTime = np.arange(0,Tseq,dtype =np.int16) \n",
    "  BasicPureTime = np.reshape(BasicPureTime,(Tseq,1))\n",
    "  GlobalTimeMask = tf.where( (BasicPureTime-np.transpose(BasicPureTime))>0, 0.0,1.0)\n",
    "  GlobalTimeMask = np.reshape(GlobalTimeMask,(1,1,1,Tseq,Tseq))\n",
    "  return Time\n",
    "\n",
    "def shuffleDLinput(Xin,yin,AuxiliaryArray=None, Spacetime=None):\n",
    " # Auxiliary array could be weight or location/time tracker\n",
    " # These are per batch so sorted axis is first\n",
    "  \n",
    "  np.random.seed(int.from_bytes(os.urandom(4), byteorder='little'))\n",
    "  trainingorder = list(range(0, len(Xin)))\n",
    "  random.shuffle(trainingorder)\n",
    "\n",
    "  Xinternal = list()\n",
    "  yinternal = list()\n",
    "  if AuxiliaryArray is not None:\n",
    "    AuxiliaryArrayinternal = list()\n",
    "  if Spacetime is not None:\n",
    "    Spacetimeinternal = list()\n",
    "  for i in trainingorder:\n",
    "    Xinternal.append(Xin[i])\n",
    "    yinternal.append(yin[i])\n",
    "    if AuxiliaryArray is not None:\n",
    "      AuxiliaryArrayinternal.append(AuxiliaryArray[i])\n",
    "    if Spacetime is not None:\n",
    "      Spacetimeinternal.append(Spacetime[i])\n",
    "  X = np.array(Xinternal)\n",
    "  y = np.array(yinternal)\n",
    "  if (AuxiliaryArray is None) and (Spacetime is None):\n",
    "    return X, y\n",
    "  if (AuxiliaryArray is not None) and (Spacetime is None):\n",
    "    AA = np.array(AuxiliaryArrayinternal)\n",
    "    return X,y,AA\n",
    "  if (AuxiliaryArray is None) and (Spacetime is not None):\n",
    "    St = np.array(Spacetimeinternal)\n",
    "    return X,y,St\n",
    "  AA = np.array(AuxiliaryArrayinternal)\n",
    "  St = np.array(Spacetimeinternal)\n",
    "  return X,y,AA,St\n",
    "\n",
    "# Simple Plot of Loss from history\n",
    "def finalizeDL(ActualModel, recordtrainloss, recordvalloss, validationfrac, X_in, y_in, modelflag, LabelFit =''):\n",
    "\n",
    "# Ouput Loss v Epoch\n",
    "  histlen = len(recordtrainloss)\n",
    "  trainloss = recordtrainloss[histlen-1]\n",
    "  plt.rcParams[\"figure.figsize\"] = [8,6]\n",
    "  plt.plot(recordtrainloss)\n",
    "  if (validationfrac > 0.001) and len(recordvalloss) > 0:\n",
    "    valloss = recordvalloss[histlen-1]\n",
    "    plt.plot(recordvalloss)\n",
    "  else:\n",
    "    valloss = 0.0\n",
    "  \n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + current_time + ' ' + RunName + ' finalizeDL ' + RunComment +resetfonts)\n",
    "  plt.title(LabelFit + ' ' + RunName+' model loss ' + str(round(trainloss,7)) + ' Val ' + str(round(valloss,7)))\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.yscale(\"log\")\n",
    "  plt.grid(True)\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "# Setup TFT  \n",
    "  if modelflag == 2:\n",
    "    global SkipDL2F, IncreaseNloc_sample, DecreaseNloc_sample\n",
    "    SkipDL2F = True\n",
    "    IncreaseNloc_sample = 1\n",
    "    DecreaseNloc_sample = 1\n",
    "    TFToutput_map = TFTpredict(TFTmodel,TFTtest_datacollection)\n",
    "    VisualizeTFT(TFTmodel, TFToutput_map)\n",
    "  else:\n",
    "    FitPredictions = DLprediction(X_in, y_in,ActualModel,modelflag, LabelFit = LabelFit)\n",
    "    for debugfips in ListofTestFIPS:\n",
    "      if debugfips != '': \n",
    "        debugfipsoutput(debugfips, FitPredictions, X_in, y_in)\n",
    "  return\n",
    "\n",
    "def debugfipsoutput(debugfips, FitPredictions, Xin, Observations):\n",
    "\n",
    "  print(startbold + startred + 'debugfipsoutput for ' + str(debugfips) + RunName + ' ' + RunComment +resetfonts)\n",
    "# Set Location Number in Arrays\n",
    "  LocationNumber = FIPSstringlookup[debugfips]\n",
    "\n",
    "  # Sequences to look at\n",
    "  Seqcount = 5\n",
    "  Seqnumber =  np.empty(Seqcount, dtype = np.int)\n",
    "  Seqnumber[0] = 0\n",
    "  Seqnumber[1] = int(Num_Seq/4)-1\n",
    "  Seqnumber[2] = int(Num_Seq/2)-1\n",
    "  Seqnumber[3] = int((3*Num_Seq)/4) -1\n",
    "  Seqnumber[4] = Num_Seq-1\n",
    "\n",
    "  # Window Positions to look at\n",
    "  Wincount = 5\n",
    "  Winnumber = np.empty(Wincount, dtype = np.int)\n",
    "  Winnumber[0] = 0\n",
    "  Winnumber[1] = int(Tseq/4)-1\n",
    "  Winnumber[2] = int(Tseq/2)-1\n",
    "  Winnumber[3] = int((3*Tseq)/4) -1\n",
    "  Winnumber[4] = Tseq-1\n",
    "\n",
    "  if SymbolicWindows:\n",
    "    InputSequences = np.empty([Seqcount,Wincount, NpropperseqTOT], dtype=np.float32)\n",
    "    for jseq in range(0,Seqcount):\n",
    "      iseq = Seqnumber[jseq]\n",
    "      for jwindow in range(0,Wincount):\n",
    "        window = Winnumber[jwindow]\n",
    "        InputSequences[jseq,jwindow] = Xin[LocationNumber,iseq+jseq]\n",
    "  else:\n",
    "    InputSequences = Xin \n",
    "\n",
    "  # Location Info\n",
    " \n",
    "  print('\\n' + startbold + startred + debugfips + ' # ' + str(LocationNumber) + ' ' +\n",
    "        Locationname[LocationNumber] + ' ' + Locationstate[LocationNumber] + ' Pop '\n",
    "        + str(Locationpopulation[LocationNumber]) + resetfonts)\n",
    "  plot_by_fips(int(debugfips), Observations, FitPredictions)\n",
    "\n",
    "  \n",
    "    \n",
    "  # Print Input Data to Test\n",
    "  # Static Properties\n",
    "  print(startbold + startred + 'Static Properties ' + debugfips + ' ' +\n",
    "         Locationname[LocationNumber] + resetfonts)\n",
    "  line = ''\n",
    "  for iprop in range(0,NpropperTimeStatic):\n",
    "    if SymbolicWindows:\n",
    "      val = InputSequences[0,0,iprop]\n",
    "    else:\n",
    "      val = InputSequences[0,LocationNumber,0,iprop]\n",
    "    line += startbold + InputPropertyNames[PropertyNameIndex[iprop]] + resetfonts + ' ' + str(round(val,3)) + ' '\n",
    "  print('\\n'.join(wrap(line,200)))\n",
    "\n",
    " # Dynamic Properties\n",
    "  for iprop in range(NpropperTimeStatic, NpropperTime):\n",
    "    print('\\n')\n",
    "    for jwindow in range(0,Wincount):\n",
    "      window = Winnumber[jwindow]\n",
    "      line = startbold + InputPropertyNames[PropertyNameIndex[iprop]] + ' W= '+str(window) +resetfonts\n",
    "      for jseq in range(0,Seqcount):\n",
    "        iseq = Seqnumber[jseq]\n",
    "        line += startbold + startred + ' ' + str(iseq) + ')' +resetfonts\n",
    "        if SymbolicWindows:\n",
    "          val = InputSequences[jseq,jwindow,iprop]\n",
    "        else:\n",
    "          val = InputSequences[iseq,LocationNumber,window,iprop]\n",
    "        line +=   ' ' + str(round(val,3))\n",
    "      print('\\n'.join(wrap(line,200)))\n",
    "  \n",
    "\n",
    "  # Total Input\n",
    "  print('\\n')\n",
    "  line = startbold + 'Props: ' + resetfonts \n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    if iprop%5 == 0:\n",
    "      line += startbold + startred + ' ' + str(iprop) + ')' + resetfonts     \n",
    "    line += ' ' + InputPropertyNames[PropertyNameIndex[iprop]]\n",
    "  print('\\n'.join(wrap(line,200)))\n",
    "  for jseq in range(0,Seqcount):\n",
    "    iseq = Seqnumber[jseq]\n",
    "    for jwindow in range(0,Wincount):\n",
    "      window = Winnumber[jwindow]\n",
    "      line = startbold + 'Input: All in Seq ' + str(iseq) + ' W= ' + str(window) + resetfonts\n",
    "      for iprop in range(0,NpropperseqTOT):\n",
    "        if iprop%5 == 0:\n",
    "          line += startbold + startred + ' ' + str(iprop) + ')' +resetfonts\n",
    "        if SymbolicWindows:\n",
    "          val = InputSequences[jseq,jwindow,iprop]\n",
    "        else:\n",
    "          val = InputSequences[iseq,LocationNumber,window,iprop]\n",
    "        result = str(round(val,3))\n",
    "        line += ' ' + result\n",
    "      print('\\n'.join(wrap(line,200)))\n",
    "\n",
    "  # Total Prediction\n",
    "  print('\\n')\n",
    "  line = startbold + 'Preds: ' + resetfonts \n",
    "  for ipred in range(0,NpredperseqTOT):\n",
    "    if ipred%5 == 0:\n",
    "      line += startbold + startred + ' ' + str(ipred) + ')' + resetfonts     \n",
    "    line += ' ' + Predictionname[PredictionNameIndex[ipred]]\n",
    "  for jseq in range(0,Seqcount):\n",
    "    iseq = Seqnumber[jseq]\n",
    "    line = startbold + 'Preds: All in Seq ' + str(iseq) + resetfonts\n",
    "    for ipred in range(0,NpredperseqTOT):\n",
    "      fred = Observations[iseq,LocationNumber,ipred]\n",
    "      if np.math.isnan(fred):\n",
    "        result = 'NaN'\n",
    "      else:\n",
    "        result = str(round(fred,3))\n",
    "      if ipred%5 == 0:\n",
    "          line += startbold + startred + ' ' + str(ipred) + ')' + resetfonts     \n",
    "      line += ' ' + result\n",
    "    print('\\n'.join(wrap(line,200)))   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPM9420zCDSO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###DLPrediction2F Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bv4pFAeCXZu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def printloss(name,mean,var,SampleSize, lineend =''):\n",
    "  mean /= SampleSize\n",
    "  var /= SampleSize\n",
    "  std = math.sqrt(var - mean**2)\n",
    "  print(name + ' Mean ' + str(round(mean,5)) + ' Std Deviation ' + str(round(std,7)) + ' ' + lineend)\n",
    "  \n",
    "def DLprediction2F(Xin, yin, DLmodel, modelflag):\n",
    "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
    "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
    "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
    "  # Label Array is always [Num_Seq][Nloc] [0=Window(first sequence)#, 1=Location]\n",
    "\n",
    "  if SkipDL2F:\n",
    "    return\n",
    "  if GarbageCollect:\n",
    "    gc.collect()\n",
    "  global  OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
    "\n",
    "  SensitivityAnalyze = np.full((NpropperseqTOT), False, dtype = np.bool)\n",
    "  SensitivityChange = np.zeros ((NpropperseqTOT), dtype = np.float32)\n",
    "  SensitvitybyPrediction = False\n",
    "  if ReadApril2021Covid:\n",
    "    for iprop in range(0,NpropperseqTOT):\n",
    "      if iprop >=15:\n",
    "        continue\n",
    "      if modelflag==2:\n",
    "        SensitivityAnalyze[iprop] = DLmodel.CheckProperty(iprop)\n",
    "        continue\n",
    "      SensitivityAnalyze[iprop] = True\n",
    "  if RunName == 'EARTHQ-EMA1LR7':\n",
    "    for iprop in range(0,NpropperseqTOT):\n",
    "      if (iprop > 21) or (iprop < 4):\n",
    "        continue\n",
    "      if modelflag==2:\n",
    "        SensitivityAnalyze[iprop] = DLmodel.CheckProperty(iprop)\n",
    "        continue\n",
    "      SensitivityAnalyze[iprop] = True\n",
    "  if RunName == 'EARTHQ-EMA1LR8':\n",
    "    for iprop in range(0,NpropperseqTOT):\n",
    "      if (iprop > 13) or (iprop < 4):\n",
    "        continue\n",
    "      if modelflag==2:\n",
    "        SensitivityAnalyze[iprop] = DLmodel.CheckProperty(iprop)\n",
    "        continue\n",
    "      SensitivityAnalyze[iprop] = True\n",
    "\n",
    "  something = 0\n",
    "  SensitivityList = []\n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    if SensitivityAnalyze[iprop]:\n",
    "      something +=1\n",
    "      SensitivityList.append(iprop)\n",
    "  if something == 0:\n",
    "    return\n",
    "  ScaleProperty = 0.99\n",
    "  SampleSize = 1\n",
    "\n",
    "\n",
    "  SensitivityFitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT, 1 + something], dtype =np.float32)\n",
    "  FRanges = np.full((NpredperseqTOT), 1.0, dtype = np.float32)\n",
    "  current_time = timenow()\n",
    "  print(wraptotext(startbold+startred+ 'DLPrediction2F ' +current_time + ' ' + RunName + RunComment +  resetfonts))\n",
    "\n",
    "  sw = np.empty_like(yin, dtype=np.float32)\n",
    "  for i in range(0,sw.shape[0]):\n",
    "    for j in range(0,sw.shape[1]):\n",
    "      for k in range(0,NpredperseqTOT):\n",
    "        sw[i,j,k] = Predictionwgt[k] \n",
    "  labelarray =np.empty([Num_Seq, Nloc, 2], dtype = np.int32)\n",
    "  for iseq in range(0, Num_Seq):\n",
    "    for iloc in range(0,Nloc):\n",
    "      labelarray[iseq,iloc,0] = iseq\n",
    "      labelarray[iseq,iloc,1] = iloc\n",
    "\n",
    "  Totaltodo = Num_Seq*Nloc\n",
    "  Nloc_sample = Nloc # default\n",
    "\n",
    "  if IncreaseNloc_sample > 1:\n",
    "    Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
    "  elif DecreaseNloc_sample > 1:\n",
    "    Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
    "\n",
    "  if Totaltodo%Nloc_sample != 0:\n",
    "    printexit('Invalid Nloc_sample ' + str(Nloc_sample) + \" \" + str(Totaltodo))\n",
    "  d_sample = Tseq * Nloc_sample        \n",
    "  max_d_sample = d_sample\n",
    "  OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
    "  print(' Predict with ' +str(Nloc_sample) + ' sequences per sample and batch size ' + str(OuterBatchDimension))\n",
    "\n",
    "  print(startbold+startred+ 'Sensitivity using Property ScaleFactor ' + str(round(ScaleProperty,3)) + resetfonts)\n",
    "  for Sensitivities in range(0,1+something):\n",
    "    if Sensitivities == 0: # BASIC unmodified run\n",
    "      iprop = -1\n",
    "      print(startbold+startred+ 'Basic Predictions' + resetfonts)\n",
    "      if SymbolicWindows:\n",
    "        ReshapedSequencesTOTmodified = ReshapedSequencesTOT # NOT used if modelflag == 2\n",
    "        if modelflag == 2:\n",
    "          DLmodel.MakeMapping()\n",
    "      else:\n",
    "        Xinmodified = Xin\n",
    "    else:\n",
    "      iprop = SensitivityList[Sensitivities-1]\n",
    "      maxminplace = PropertyNameIndex[iprop]\n",
    "      lastline = ''\n",
    "      if iprop < Npropperseq:\n",
    "        lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
    "      print(startbold+startred+ 'Property ' + str(iprop) + ' ' + InputPropertyNames[maxminplace] + resetfonts + lastline)\n",
    "      if SymbolicWindows:\n",
    "        if modelflag == 2:\n",
    "          DLmodel.SetupProperty(iprop)\n",
    "          DLmodel.ScaleProperty(ScaleProperty)\n",
    "          DLmodel.MakeMapping()\n",
    "        else:\n",
    "          ReshapedSequencesTOTmodified = np.copy(ReshapedSequencesTOT)\n",
    "          ReshapedSequencesTOTmodified[:,:,iprop] = ScaleProperty * ReshapedSequencesTOTmodified[:,:,iprop]\n",
    "      else:\n",
    "        Xinmodified = np.copy(Xin)\n",
    "        Xinmodified[:,:,:,iprop] = ScaleProperty*Xinmodified[:,:,:,iprop]\n",
    "    CountFitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "    meanvalue2 = 0.0\n",
    "    meanvalue3 = 0.0\n",
    "    meanvalue4 = 0.0\n",
    "    variance2= 0.0\n",
    "    variance3= 0.0\n",
    "    variance4= 0.0\n",
    "\n",
    "    samplebar = notebook.trange(SampleSize,  desc='Full Samples', unit  = 'sample')\n",
    "    bbar = notebook.trange(OuterBatchDimension,  desc='Batch    loop', unit  = 'sample')\n",
    "    for shuffling in range (0,SampleSize):\n",
    "      if GarbageCollect:\n",
    "        gc.collect()\n",
    "      yuse = yin\n",
    "      labeluse = labelarray\n",
    "      y2= np.reshape(yuse, (-1, NpredperseqTOT)).copy()\n",
    "      labelarray2 = np.reshape(labeluse, (-1,2))\n",
    "\n",
    "      if SymbolicWindows:\n",
    "        # Xin X2 X3 not used rather ReshapedSequencesTOT\n",
    "        labelarray2, y2 = shuffleDLinput(labelarray2, y2)\n",
    "        ReshapedSequencesTOTuse = ReshapedSequencesTOTmodified\n",
    "      else:\n",
    "        Xuse = Xinmodified\n",
    "        X2 = np.reshape(Xuse, (-1, Tseq, NpropperseqTOT)).copy()\n",
    "        X2, y2, labelarray2 = shuffleDLinput(X2, y2,labelarray2)\n",
    "        X3 = np.reshape(X2, (-1, d_sample, NpropperseqTOT))\n",
    "        \n",
    "      y3 = np.reshape(y2, (-1, Nloc_sample, NpredperseqTOT))\n",
    "      sw = np.reshape(sw, (-1, Nloc_sample, NpredperseqTOT))\n",
    "      labelarray3 = np.reshape(labelarray2, (-1, Nloc_sample, 2))\n",
    "\n",
    "      quan2 = 0.0\n",
    "      quan3 = 0.0\n",
    "      quan4 = 0.0\n",
    "      for Batchindex in range(0, OuterBatchDimension):\n",
    "        if GarbageCollect:\n",
    "          gc.collect()\n",
    "\n",
    "        if SymbolicWindows:\n",
    "          if modelflag == 2: # Note first index of InputVector Location, Second is sequence number; labelarray3 is opposite\n",
    "            InputVector = np.empty((Nloc_sample,2), dtype = np.int32)\n",
    "            for iloc_sample in range(0,Nloc_sample):\n",
    "              InputVector[iloc_sample,0] = labelarray3[Batchindex, iloc_sample,1]\n",
    "              InputVector[iloc_sample,1] = labelarray3[Batchindex, iloc_sample,0]\n",
    "          else:\n",
    "            X3local = list()\n",
    "            for iloc_sample in range(0,Nloc_sample):\n",
    "              LocLocal = labelarray3[Batchindex, iloc_sample,1]\n",
    "              SeqLocal = labelarray3[Batchindex, iloc_sample,0]\n",
    "              X3local.append(ReshapedSequencesTOTuse[LocLocal,SeqLocal:SeqLocal+Tseq])\n",
    "            InputVector = np.array(X3local)\n",
    "        else:\n",
    "          InputVector = X3[Batchindex]\n",
    "\n",
    "        Labelsused = labelarray3[Batchindex]\n",
    "        Time = None\n",
    "        if modelflag == 0:\n",
    "          InputVector = np.reshape(InputVector,(-1,Tseq,NpropperseqTOT))\n",
    "        elif modelflag == 1:\n",
    "          Time = SetSpacetime(np.reshape(Labelsused[:,0],(1,-1)))\n",
    "          InputVector = np.reshape(InputVector,(1,Tseq*Nloc_sample,NpropperseqTOT))\n",
    "        PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time )\n",
    "        PredictedVector = np.reshape(PredictedVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "        swbatched = sw[Batchindex,:,:]\n",
    "        if LocationBasedValidation:\n",
    "          swT = np.zeros([1,Nloc_sample,NpredperseqTOT],dtype = np.float32)\n",
    "          swV = np.zeros([1,Nloc_sample,NpredperseqTOT],dtype = np.float32)\n",
    "          for iloc_sample in range(0,Nloc_sample):\n",
    "            fudgeT = Nloc/TrainingNloc\n",
    "            fudgeV = Nloc/ValidationNloc\n",
    "            iloc = Labelsused[iloc_sample,1]\n",
    "            if MappingtoTraining[iloc] >= 0:\n",
    "              swT[0,iloc_sample,:] = swbatched[iloc_sample,:]*fudgeT\n",
    "            else:\n",
    "              swV[0,iloc_sample,:] = swbatched[iloc_sample,:]*fudgeV\n",
    "        TrueVector = y3[Batchindex]\n",
    "        TrueVector = np.reshape(TrueVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "        swbatched = np.reshape(swbatched,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "        losspercall = numpycustom_lossGCF1(TrueVector,PredictedVector,swbatched)\n",
    "        quan2 += losspercall\n",
    "        bbar.update(1)\n",
    "        if LocationBasedValidation:\n",
    "          losspercallTr = numpycustom_lossGCF1(TrueVector,PredictedVector,swT)\n",
    "          quan3 += losspercallTr\n",
    "          losspercallVl = numpycustom_lossGCF1(TrueVector,PredictedVector,swV)\n",
    "          quan4 += losspercallVl\n",
    "        \n",
    "        for iloc_sample in range(0,Nloc_sample):\n",
    "          LocLocal = Labelsused[iloc_sample,1]\n",
    "          SeqLocal = Labelsused[iloc_sample,0]\n",
    "          yyhat = PredictedVector[0,iloc_sample]\n",
    "          CountFitPredictions [SeqLocal,LocLocal,:] += FRanges\n",
    "          SensitivityFitPredictions [SeqLocal,LocLocal,:,Sensitivities] += yyhat\n",
    "\n",
    "        fudge = 1.0/(1.0 + Batchindex)\n",
    "        mean2 = quan2 * fudge \n",
    "        if LocationBasedValidation:\n",
    "          mean3 = quan3 * fudge\n",
    "          mean4 = quan4 * fudge\n",
    "          bbar.set_postfix(AvLoss = mean2, AvTr = mean3, AvVl = mean4, Loss = losspercall, Tr = losspercallTr, Vl = losspercallVl)\n",
    "        else:\n",
    "          bbar.set_postfix(Loss = losspercall, AvLoss = mean2 ) \n",
    "\n",
    "  # Processing at the end of Sampling Loop\n",
    "      fudge = 1.0/OuterBatchDimension\n",
    "      quan2 *= fudge\n",
    "      quan3 *= fudge\n",
    "      quan4 *= fudge\n",
    "      meanvalue2 += quan2\n",
    "      variance2 += quan2**2\n",
    "      variance3 += quan3**2\n",
    "      variance4 += quan4**2\n",
    "      if LocationBasedValidation:\n",
    "        meanvalue3 += quan3\n",
    "        meanvalue4 += quan4        \n",
    "      samplebar.update(1)\n",
    "      if LocationBasedValidation:\n",
    "        samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Tr = quan3, Val = quan4)\n",
    "      else:\n",
    "        samplebar.set_postfix(Shuffle=shuffling, Loss = quan2)\n",
    "      bbar.reset()\n",
    "  # End Shuffling loop\n",
    "\n",
    "    if Sensitivities == 0:\n",
    "      iprop = -1\n",
    "      lineend = startbold+startred+ 'Basic Predictions' + resetfonts\n",
    "    else:\n",
    "      iprop = SensitivityList[Sensitivities-1]\n",
    "      nameplace = PropertyNameIndex[iprop]\n",
    "      maxminplace = PropertyAverageValuesPointer[iprop]\n",
    "      lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
    "      lineend= startbold+startred + 'Property ' + str(iprop) + ' ' + InputPropertyNames[nameplace] + resetfonts + lastline\n",
    "      if modelflag == 2:\n",
    "        DLmodel.ResetProperty()\n",
    "\n",
    "    meanvalue2 /= SampleSize \n",
    "\n",
    "    global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
    "    printloss(' Full Loss ',meanvalue2,variance2,SampleSize, lineend = lineend)\n",
    "    meanvalue2 /= SampleSize\n",
    "    GlobalLoss = meanvalue2\n",
    "    GlobalTrainingLoss = 0.0\n",
    "    GlobalValidationLoss = 0.0\n",
    "    \n",
    "    if LocationBasedValidation:\n",
    "      printloss(' Training Loss ',meanvalue3,variance3,SampleSize, lineend = lineend)\n",
    "      printloss(' Validation Loss ',meanvalue4,variance4,SampleSize, lineend = lineend)\n",
    "      meanvalue3 /= SampleSize\n",
    "      meanvalue4 /= SampleSize\n",
    "      GlobalTrainingLoss = meanvalue3\n",
    "      GlobalValidationLoss = meanvalue4\n",
    "    \n",
    "    if PlotinDL2F:\n",
    "      label = 'Sensitivity ' + str(Sensitivities)\n",
    "      extracomments =[]\n",
    "      for PredictedPos in range(0,NpredperseqTOT):\n",
    "        labelfull = label + ' Pred ' +str(PredictedPos)\n",
    "        extracomments.append([labelfull,labelfull])\n",
    "      Location_summed_plot(0, yin, SensitivityFitPredictions[:,:,:,Sensitivities] , extracomments = extracomments, Dumpplot = False)\n",
    "\n",
    "# Sequence Location Predictions\n",
    "    SensitivityFitPredictions[:,:,:,Sensitivities] = np.divide(SensitivityFitPredictions[:,:,:,Sensitivities],CountFitPredictions[:,:,:])\n",
    "    if Sensitivities == 0:\n",
    "      Goldstandard = np.sum(np.abs(SensitivityFitPredictions[:,:,:,Sensitivities]), axis =(0,1))\n",
    "      TotalGS = np.sum(Goldstandard)\n",
    "      continue\n",
    "    Change = np.sum(np.abs(np.subtract(SensitivityFitPredictions[:,:,:,Sensitivities],SensitivityFitPredictions[:,:,:,0])), axis =(0,1))\n",
    "    TotalChange = np.sum(Change)\n",
    "    SensitivityChange[iprop] = TotalChange\n",
    "    print(str(round(TotalChange,5)) +  ' GS ' + str(round(TotalGS,5)) + ' ' +lineend)\n",
    "    if SensitvitybyPrediction:\n",
    "      for ipred in range(0,NpredperseqTOT):\n",
    "        print(str(round(Change[ipred],5)) +  ' GS ' + str(round(Goldstandard[ipred],5)) \n",
    "        + ' ' + str(ipred) + ' ' + Predictionname[PredictionNameIndex[ipred]] + ' wgt ' + str(round(Predictionwgt[ipred],3)))\n",
    "    \n",
    "  print(startbold+startred+ '\\nSummarize Changes Total ' + str(round(TotalGS,5))+ ' Property ScaleFactor ' + str(round(ScaleProperty,3)) + resetfonts )\n",
    "  for Sensitivities in range(1,1+something):\n",
    "    iprop = SensitivityList[Sensitivities-1]\n",
    "    nameplace = PropertyNameIndex[iprop]\n",
    "    maxminplace = PropertyAverageValuesPointer[iprop]\n",
    "    \n",
    " \n",
    "    lastline = ' Normed Mean ' +str(round(QuantityStatistics[maxminplace,5],4))\n",
    "    lastline += ' Normed Std ' +str(round(QuantityStatistics[maxminplace,6],4))\n",
    "    TotalChange = SensitivityChange[iprop] \n",
    "    NormedChange = TotalChange/((1-ScaleProperty)*TotalGS)\n",
    "    stdmeanratio = 0.0\n",
    "    stdchangeratio = 0.0   \n",
    "    if np.abs(QuantityStatistics[maxminplace,5]) > 0.0001:\n",
    "      stdmeanratio = QuantityStatistics[maxminplace,6]/QuantityStatistics[maxminplace,5]\n",
    "    if np.abs(QuantityStatistics[maxminplace,6]) > 0.0001:\n",
    "      stdchangeratio = NormedChange/QuantityStatistics[maxminplace,6]\n",
    "\n",
    "    lratios =  ' Normed Change '+ str(round(NormedChange,5)) + ' /std ' + str(round(stdchangeratio,5))\n",
    "    lratios += ' Std/Mean ' + str(round(stdmeanratio,5))\n",
    "    print(str(iprop) + ' Change '+ str(round(TotalChange,2)) + startbold + lratios\n",
    "          + ' ' + InputPropertyNames[nameplace] + resetfonts + lastline)\n",
    "\n",
    "  current_time = timenow()  \n",
    "  print(startbold+startred+ '\\nEND DLPrediction2F ' + current_time + ' ' + RunName + RunComment +resetfonts) \n",
    "  return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23a1qMuO_yVT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### General DL Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awPqzc4H_yVh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model_summary(model):  \n",
    "  stream = io.StringIO()\n",
    "  model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
    "  summary_string = stream.getvalue()\n",
    "  stream.close()\n",
    "  return summary_string\n",
    "\n",
    "def setDLinput(Spacetime = True):\n",
    "  # Initial data is Flatten([Num_Seq][Nloc]) [Tseq] with values [Nprop-Sel + Nforcing + Add(ExPosEnc-Selin)] starting with   RawInputSequencesTOT\n",
    "  # Predictions are Flatten([Num_Seq] [Nloc]) [Predvals=Npred+ExPosEnc-Selout] [Predtimes = Forecast-time range] starting with RawInputPredictionsTOT\n",
    "  # No assumptions as to type of variables here\n",
    "  if SymbolicWindows:\n",
    "    X_predict = SymbolicInputSequencesTOT.reshape(OuterBatchDimension,1,1)\n",
    "  else:\n",
    "    X_predict = RawInputSequencesTOT.reshape(OuterBatchDimension,Tseq,NpropperseqTOT)\n",
    "  y_predict = RawInputPredictionsTOT.reshape(OuterBatchDimension,NpredperseqTOT)\n",
    "  if Spacetime:\n",
    "     SpacetimeforMask_predict =  SpacetimeforMask.reshape(OuterBatchDimension,1,1).copy()\n",
    "     return X_predict, y_predict, SpacetimeforMask_predict\n",
    "  return X_predict, y_predict\n",
    "\n",
    "def setSeparateDLinput(model, Spacetime = False):\n",
    "  # Initial data is Flatten([Num_Seq][Nloc]) [Tseq] with values [Nprop-Sel + Nforcing + Add(ExPosEnc-Selin)] starting with   RawInputSequencesTOT\n",
    "  # Predictions are Flatten([Num_Seq] [Nloc]) [Predvals=Npred+ExPosEnc-Selout] [Predtimes = Forecast-time range] starting with RawInputPredictionsTOT\n",
    "  # No assumptions as to type of variables here\n",
    "  # model = 0 LSTM =1 transformer\n",
    "  if model == 0: \n",
    "    Spacetime = False\n",
    "  X_val = None\n",
    "  y_val = None\n",
    "  Spacetime_val = None\n",
    "  Spacetime_train = None\n",
    "  if SymbolicWindows:\n",
    "    InputSequences = np.empty([Num_Seq, TrainingNloc], dtype = np.int32)\n",
    "    for iloc in range(0,TrainingNloc):   \n",
    "      InputSequences[:,iloc] = SymbolicInputSequencesTOT[:,ListofTrainingLocs[iloc]]\n",
    "    if model == 0:\n",
    "      X_train = InputSequences.reshape(Num_Seq*TrainingNloc,1,1)\n",
    "    else:\n",
    "      X_train = InputSequences\n",
    "    if Spacetime:\n",
    "      Spacetime_train = X_train.copy()\n",
    "\n",
    "    if LocationValidationFraction > 0.001:\n",
    "      UsedValidationNloc = ValidationNloc\n",
    "      if FullSetValidation:\n",
    "        UsedValidationNloc = Nloc\n",
    "      ValInputSequences = np.empty([Num_Seq, UsedValidationNloc], dtype = np.int32)\n",
    "      if FullSetValidation:\n",
    "        for iloc in range(0,Nloc):\n",
    "          ValInputSequences[:,iloc] = SymbolicInputSequencesTOT[:,iloc]\n",
    "      else:\n",
    "        for iloc in range(0,ValidationNloc):\n",
    "          ValInputSequences[:,iloc] = SymbolicInputSequencesTOT[:,ListofValidationLocs[iloc]]\n",
    "      if model == 0:\n",
    "        X_val = ValInputSequences.reshape(Num_Seq * UsedValidationNloc,1,1)\n",
    "      else:\n",
    "        X_val = ValInputSequences\n",
    "      if Spacetime:\n",
    "        Spacetime_val = X_val.copy()\n",
    "\n",
    "  else: # Symbolic Windows false Calculate Training\n",
    "    InputSequences = np.empty([Num_Seq, TrainingNloc,Tseq,NpropperseqTOT], dtype = np.float32)\n",
    "    for iloc in range(0,TrainingNloc): \n",
    "      InputSequences[:,iloc,:,:] = RawInputSequencesTOT[:,ListofTrainingLocs[iloc],:,:]\n",
    "    if model == 0:\n",
    "      X_train = InputSequences.reshape(Num_Seq*TrainingNloc,Tseq,NpropperseqTOT)\n",
    "    else:\n",
    "      X_train = InputSequences\n",
    "    if Spacetime:\n",
    "      Spacetime_train = np.empty([Num_Seq, TrainingNloc], dtype = np.int32)\n",
    "      for iloc in range(0,TrainingNloc):   \n",
    "        Spacetime_train[:,iloc] = SpacetimeforMask[:,ListofTrainingLocs[iloc]]\n",
    "\n",
    "    if LocationValidationFraction > 0.001: # Symbolic Windows false Calculate Validation\n",
    "      UsedValidationNloc = ValidationNloc\n",
    "      if FullSetValidation:\n",
    "        UsedValidationNloc = Nloc\n",
    "      ValInputSequences = np.empty([Num_Seq, UsedValidationNloc,Tseq,NpropperseqTOT], dtype = np.float32)\n",
    "      if FullSetValidation:\n",
    "        for iloc in range(0,Nloc):\n",
    "          ValInputSequences[:,iloc,:,:] = RawInputSequencesTOT[:,iloc,:,:]\n",
    "      else:\n",
    "        for iloc in range(0,ValidationNloc):\n",
    "          ValInputSequences[:,iloc,:,:] = RawInputSequencesTOT[:,ListofValidationLocs[iloc],:,:]\n",
    "      if model == 0:\n",
    "        X_val = ValInputSequences.reshape(Num_Seq * UsedValidationNloc,Tseq,NpropperseqTOT)\n",
    "      else:\n",
    "        X_val = ValInputSequences\n",
    "      if Spacetime:\n",
    "        Spacetime_val = np.empty([Num_Seq, UsedValidationNloc], dtype = np.int32)\n",
    "        if FullSetValidation:\n",
    "          for iloc in range(0,Nloc):\n",
    "            Spacetime_val[:,iloc] = SpacetimeforMask[:,iloc]\n",
    "        else:\n",
    "          for iloc in range(0,ValidationNloc):   \n",
    "            Spacetime_val[:,iloc] = SpacetimeforMask[:,ListofValidationLocs[iloc]]\n",
    "  \n",
    "  # Calculate training predictions \n",
    "  InputPredictions = np.empty([Num_Seq, TrainingNloc,NpredperseqTOT], dtype = np.float32)\n",
    "  for iloc in range(0,TrainingNloc):\n",
    "    InputPredictions[:,iloc,:] = RawInputPredictionsTOT[:,ListofTrainingLocs[iloc],:]\n",
    "  if model == 0:\n",
    "    y_train = InputPredictions.reshape(OuterBatchDimension,NpredperseqTOT)\n",
    "  else:\n",
    "    y_train = InputPredictions\n",
    "\n",
    "  # Calculate validation predictions \n",
    "  if LocationValidationFraction > 0.001:\n",
    "    ValInputPredictions = np.empty([Num_Seq, UsedValidationNloc,NpredperseqTOT], dtype = np.float32)\n",
    "    if FullSetValidation:\n",
    "      for iloc in range(0,Nloc):\n",
    "        ValInputPredictions[:,iloc,:] = RawInputPredictionsTOT[:,iloc,:]\n",
    "    else:\n",
    "      for iloc in range(0,ValidationNloc):\n",
    "        ValInputPredictions[:,iloc,:] = RawInputPredictionsTOT[:,ListofValidationLocs[iloc],:]\n",
    "    if model == 0:\n",
    "      y_val = ValInputPredictions.reshape(Num_Seq * ValidationNloc,NpredperseqTOT)\n",
    "    else:\n",
    "      y_val = ValInputPredictions\n",
    "\n",
    "  if Spacetime:\n",
    "    return X_train, y_train, Spacetime_train, X_val, y_val, Spacetime_val\n",
    "  else:    \n",
    "    return X_train, y_train,X_val,y_val\n",
    "\n",
    "def InitializeDLforTimeSeries(message,processindex,y_predict):\n",
    "  if( processindex == 0 ):\n",
    "      current_time = timenow()\n",
    "      line = (startbold + current_time + ' ' + message + resetfonts + \" Window Size \" + str(Tseq) + \n",
    "            \" Number of samples over time that sequence starts at and location:\" +str(OuterBatchDimension) + \n",
    "            \" Number input features per sequence:\" + str(NpropperseqTOT) +  \n",
    "            \" Number of predicted outputs per sequence:\" + str(NpredperseqTOT) + \n",
    "            \" Batch_size:\" + str(LSTMbatch_size) + \n",
    "            \" n_nodes:\" + str(number_LSTMnodes) + \n",
    "            \" epochs:\" + str(TFTTransformerepochs))\n",
    "      print(wraptotext(line))\n",
    "      checkNaN(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42UqTW0xDoKr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tensorflow  Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIocfMfZBjfa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TensorFlowTrainingMonitor():\n",
    "  def __init__(self):\n",
    "\n",
    "    # These OPERATIONAL variables control saving of best fits\n",
    "    self.lastsavedepoch = -1 # Epoch number where last saved fit done\n",
    "    self.BestLossValueSaved = NaN # Training Loss value of last saved fit\n",
    "    self.BestValLossValueSaved = NaN # Validation Loss value of last saved fit\n",
    "    self.Numsuccess = 0 # count little successes up to SuccessLimit\n",
    "    self.Numfailed = 0\n",
    "    self.LastLossValue = NaN # Loss on previous epoch\n",
    "    self.MinLossValue = NaN # Saved minimum loss value\n",
    "    self.LastValLossValue = NaN # Validation Loss on previous epoch\n",
    "    self.MinValLossValue = NaN # validation loss value at last save\n",
    "    self.BestLossSaved = False # Boolean to indicate that best Loss value saved\n",
    "    self.saveMinLosspath = '' # Checkpoint path for saved network \n",
    "    self.epochcount = 0\n",
    "    self.NumberTimesSaved = 0 # Number of Checkpointing steps for Best Loss\n",
    "    self.NumberTimesRestored = 0 # Number of Checkpointing Restores\n",
    "    self.LittleJumpdifference = NaN\n",
    "    self.LittleValJumpdifference = NaN\n",
    "    self.AccumulateSuccesses = 0\n",
    "    self.AccumulateFailures = np.zeros(5, dtype=np.int)\n",
    "    self.RestoreReasons = np.zeros(8, dtype = np.int)\n",
    "    self.NameofFailures = ['Success','Train Only Failed','Val Only Failed','Both Failed', 'NaN']\n",
    "    self.NameofRestoreReasons = ['Both Big Jump', 'Both Little Jump','Train Big Jump', 'Train Little Jump','Val Big Jump','Val Little Jump',' Failure Limit', ' NaN']\n",
    "# End OPERATIONAL Control set up for best fit checkpointing\n",
    "\n",
    "# These are parameters user can set\n",
    "    self.UseBestAvailableLoss = True\n",
    "    self.LittleJump = 2.0 # Multiplier for checking jump compared to recent changes\n",
    "    self.ValLittleJump = 2.0 # Multiplier for checking jump compared to recent changes\n",
    "    self.startepochs = -1 # Ignore this number of epochs to let system get started\n",
    "    self.SuccessLimit = 20 # Don't keep saving. Wait for this number of (little) successes\n",
    "    self.FailureLimit = 10 # Number of failures before restore\n",
    "    self.BadJumpfraction = 0.2 # This fractional jump will trigger attempt to go back to saved value\n",
    "    self.ValBadJumpfraction = 0.2 # This fractional jump will trigger attempt to go back to saved value\n",
    "    self.ValidationFraction = 0.0 # Must be used validation fraction\n",
    "    DownplayValidationIncrease = True\n",
    "\n",
    "# End parameters user can set\n",
    "\n",
    "    self.checkpoint = None\n",
    "    self.CHECKPOINTDIR = ''\n",
    "    self.RunName = ''\n",
    "\n",
    "    self.train_epoch = 0.0\n",
    "    self.val_epoch = 0.0\n",
    "    tfepochstep = None\n",
    "    recordtrainloss =[]\n",
    "    recordvalloss = []\n",
    "\n",
    "  def SetControlParms(self, UseBestAvailableLoss = None, LittleJump = None, startepochs = None, ValLittleJump = None,\n",
    "       ValBadJumpfraction = None, SuccessLimit = None, FailureLimit = None, BadJumpfraction = None, DownplayValidationIncrease=True):\n",
    "    if UseBestAvailableLoss is not None:\n",
    "      self.UseBestAvailableLoss = UseBestAvailableLoss\n",
    "    if LittleJump is not None:\n",
    "      self.LittleJump = LittleJump\n",
    "    if ValLittleJump is not None:\n",
    "      self.ValLittleJump = ValLittleJump\n",
    "    if startepochs is not None:\n",
    "      self.startepochs = startepochs\n",
    "    if SuccessLimit is not None:\n",
    "      self.SuccessLimit = SuccessLimit\n",
    "    if FailureLimit is not None:\n",
    "      self.FailureLimit = FailureLimit\n",
    "    if BadJumpfraction is not None:\n",
    "      self.BadJumpfraction = BadJumpfraction\n",
    "    if ValBadJumpfraction is not None:\n",
    "      self.ValBadJumpfraction = ValBadJumpfraction\n",
    "    if DownplayValidationIncrease:\n",
    "      self.ValBadJumpfraction = 200.0\n",
    "      self.ValLittleJump = 2000.0 \n",
    "    elif ValLittleJump is None:\n",
    "      self.ValLittleJump = 2.0\n",
    "    elif ValBadJumpfraction is None:\n",
    "      self.ValBadJumpfraction = 0.2\n",
    "      \n",
    "  def SetCheckpointParms(self,checkpointObject,CHECKPOINTDIR,RunName = '',Restoredcheckpoint= False, Restored_path = '',  \n",
    "                         ValidationFraction = 0.0, SavedTrainLoss = NaN, SavedValLoss = NaN):\n",
    "    self.ValidationFraction = ValidationFraction\n",
    "    self.checkpoint = checkpointObject\n",
    "    self.CHECKPOINTDIR = CHECKPOINTDIR\n",
    "    self.RunName = RunName\n",
    "    if Restoredcheckpoint:\n",
    "      self.BestLossSaved = True\n",
    "      self.saveMinLosspath = Restored_path # Checkpoint path for saved network \n",
    "      self.LastLossValue = SavedTrainLoss\n",
    "      self.LastValLossValue = SavedValLoss\n",
    "      self.BestLossValueSaved = SavedTrainLoss\n",
    "      self.BestValLossValueSaved = SavedValLoss\n",
    "      self.lastsavedepoch =  self.epochcount\n",
    "      self.MinLossValue = SavedTrainLoss\n",
    "      self.MinValLossValue = SavedValLoss\n",
    "\n",
    "  def EpochEvaluate(self, epochcount,train_epoch, val_epoch, tfepochstep, recordtrainloss, recordvalloss):\n",
    "    FalseReturn = 0\n",
    "    TrueReturn = 1\n",
    "    self.epochcount = epochcount\n",
    "    self.train_epoch = train_epoch\n",
    "    self.val_epoch = val_epoch\n",
    "    self.tfepochstep = tfepochstep\n",
    "    self.recordtrainloss = recordtrainloss\n",
    "    self.recordvalloss = recordvalloss\n",
    "\n",
    "    Needtorestore = False \n",
    "    Failreason = 5 # nonsense\n",
    "    LossChange = 0.0\n",
    "    ValLossChange = 0.0\n",
    "    if np.math.isnan(self.train_epoch) or np.math.isnan(self.val_epoch):\n",
    "      Restoreflag = 7\n",
    "      self.RestoreReasons[Restoreflag] += 1\n",
    "      Needtorestore = True\n",
    "      Failreason = 4\n",
    "      self.AccumulateFailures[Failreason] += 1\n",
    "      print(str(self.epochcount) + ' NAN Seen Reason ' + str(Failreason) + ' #succ ' + str(self.Numsuccess) + ' #fail ' + str(self.Numfailed) + ' ' + str(round(self.train_epoch,6)) + ' ' + str(round(self.val_epoch,6)), flush=True)\n",
    "      return TrueReturn, self.train_epoch, self.val_epoch\n",
    "\n",
    "    if self.epochcount  <= self.startepochs:\n",
    "      return FalseReturn, self.train_epoch, self.val_epoch\n",
    "\n",
    "    if not np.math.isnan(self.LastLossValue):\n",
    "      LossChange = self.train_epoch - self.LastLossValue\n",
    "      if self.ValidationFraction > 0.001:\n",
    "        ValLossChange = self.val_epoch - self.LastValLossValue\n",
    "    if LossChange <= 0:\n",
    "      if self.ValidationFraction > 0.001:\n",
    "# Quick Fix\n",
    "        self.Numsuccess +=1\n",
    "        self.AccumulateSuccesses += 1\n",
    "        if ValLossChange <= 0:\n",
    "          Failreason = 0\n",
    "        else:\n",
    "          Failreason = 2\n",
    "      else:\n",
    "        self.Numsuccess +=1\n",
    "        self.AccumulateSuccesses += 1\n",
    "        Failreason = 0\n",
    "    else:\n",
    "      Failreason = 1\n",
    "      if self.ValidationFraction > 0.001:\n",
    "        if ValLossChange > 0:\n",
    "          Failreason = 3          \n",
    "    if Failreason > 0:\n",
    "        self.Numfailed += 1\n",
    "    self.AccumulateFailures[Failreason] += 1\n",
    "\n",
    "    if (not np.math.isnan(self.LastLossValue)) and (Failreason > 0):\n",
    "      print(str(self.epochcount) + ' Reason ' + str(Failreason) + ' #succ ' + str(self.Numsuccess) + ' #fail ' + str(self.Numfailed) + ' ' + str(round(self.train_epoch,6)) \n",
    "        + ' ' + str(round(self.LastLossValue,6)) + ' '+ str(round(self.val_epoch,6))+ ' ' + str(round(self.LastValLossValue,6)), flush=True)\n",
    "    self.LastLossValue = self.train_epoch\n",
    "    self.LastValLossValue = self.val_epoch\n",
    "    \n",
    "    StoreMinLoss = False\n",
    "    if not np.math.isnan(self.MinLossValue):\n",
    "#      if (self.train_epoch < self.MinLossValue) and (self.val_epoch <= self.MinValLossValue):\n",
    "      if (self.train_epoch < self.MinLossValue):\n",
    "        if self.Numsuccess >= self.SuccessLimit:\n",
    "          StoreMinLoss = True\n",
    "    else:\n",
    "      StoreMinLoss = True\n",
    "    if StoreMinLoss:\n",
    "      self.Numsuccess = 0\n",
    "      extrastuff = ''\n",
    "      extrastuff_val = ' '\n",
    "      if not np.math.isnan(self.MinLossValue):\n",
    "        extrastuff = ' Previous ' + str(round(self.MinLossValue,7))\n",
    "        self.LittleJumpdifference  = self.MinLossValue - self.train_epoch\n",
    "        if self.ValidationFraction > 0.001:\n",
    "          if not np.math.isnan(self.MinValLossValue):\n",
    "            extrastuff_val = ' Previous ' + str(round(self.MinValLossValue,7))\n",
    "            LittleValJumpdifference = max(self.MinValLossValue - self.val_epoch, self.LittleJumpdifference)    \n",
    "      self.saveMinLosspath = self.checkpoint.save(file_prefix=self.CHECKPOINTDIR + self.RunName +'MinLoss')\n",
    "      if not self.BestLossSaved:\n",
    "        print('\\nInitial Checkpoint at ' + self.saveMinLosspath + ' from ' + self.CHECKPOINTDIR)\n",
    "      self.MinLossValue = self.train_epoch\n",
    "      self.MinValLossValue = self.val_epoch\n",
    "      if self.ValidationFraction > 0.001:\n",
    "        extrastuff_val = ' Val Loss ' + str(round(self.val_epoch,7)) + extrastuff_val\n",
    "      print(' Epoch ' + str(self.epochcount) + ' Loss ' + str(round(self.train_epoch,7)) + extrastuff + extrastuff_val+ ' Failed ' + str(self.Numfailed), flush = True)\n",
    "      self.Numfailed = 0\n",
    "      self.BestLossSaved = True\n",
    "      self.BestLossValueSaved = self.train_epoch\n",
    "      self.BestValLossValueSaved = self.val_epoch\n",
    "      self.lastsavedepoch = self.epochcount\n",
    "      self.NumberTimesSaved += 1\n",
    "      return FalseReturn, self.train_epoch, self.val_epoch\n",
    "  \n",
    "    RestoreTrainflag = -1\n",
    "    Trainrestore = False\n",
    "    if LossChange > 0.0:\n",
    "      if LossChange > self.BadJumpfraction * self.train_epoch:\n",
    "        Trainrestore = True\n",
    "        RestoreTrainflag = 0\n",
    "      if not np.math.isnan(self.LittleJumpdifference):\n",
    "        if LossChange > self.LittleJumpdifference * self.LittleJump:\n",
    "          Trainrestore = True\n",
    "          if RestoreTrainflag < 0:\n",
    "            RestoreTrainflag = 1\n",
    "      if self.BestLossSaved:  \n",
    "        if self.train_epoch < self.MinLossValue:\n",
    "          Trainrestore = False\n",
    "          RestoreTrainflag = -1\n",
    "    \n",
    "    RestoreValflag = -1\n",
    "    Valrestore = False\n",
    "    if ValLossChange > 0.0:\n",
    "      if ValLossChange > self.ValBadJumpfraction * self.val_epoch:\n",
    "        Valrestore = True\n",
    "        RestoreValflag = 0\n",
    "      if not np.math.isnan(self.LittleValJumpdifference):\n",
    "        if ValLossChange > self.LittleValJumpdifference * self.ValLittleJump:\n",
    "          Valrestore = True\n",
    "          if RestoreValflag < 0:\n",
    "            RestoreValflag = 1\n",
    "      if self.BestLossSaved:  \n",
    "        if self.val_epoch < self.MinValLossValue:\n",
    "          Valrestore = False\n",
    "          RestoreValflag = -1\n",
    "    Restoreflag = -1\n",
    "    if Trainrestore and Valrestore:\n",
    "      Needtorestore = True\n",
    "      if RestoreTrainflag == 0:\n",
    "        Restoreflag = 0\n",
    "      else:\n",
    "        Restoreflag = 1\n",
    "    elif Trainrestore:\n",
    "      Needtorestore = True\n",
    "      Restoreflag = RestoreTrainflag + 2\n",
    "    elif Valrestore:\n",
    "      Needtorestore = True\n",
    "      Restoreflag = RestoreValflag + 4\n",
    "    if (self.Numfailed >= self.FailureLimit) and (Restoreflag == -1):\n",
    "      Restoreflag = 6\n",
    "      Needtorestore = True\n",
    "    if Restoreflag >= 0:\n",
    "      self.RestoreReasons[Restoreflag] += 1\n",
    "    if Needtorestore and (not self.BestLossSaved):\n",
    "      print('bad Jump ' + str(round(LossChange,7)) + ' Epoch ' + str(self.epochcount) + ' But nothing saved')\n",
    "      return FalseReturn, self.train_epoch, self.val_epoch\n",
    "    if Needtorestore:\n",
    "      return TrueReturn, self.train_epoch, self.val_epoch\n",
    "    else:\n",
    "      return FalseReturn, self.train_epoch, self.val_epoch\n",
    "\n",
    "  def RestoreBestFit(self):\n",
    "    if self.BestLossSaved:\n",
    "      self.checkpoint.tfrecordvalloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "      self.checkpoint.tfrecordtrainloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "      self.checkpoint.restore(save_path=self.saveMinLosspath).expect_partial()\n",
    "      self.tfepochstep  = self.checkpoint.tfepochstep \n",
    "      self.recordvalloss = self.checkpoint.tfrecordvalloss.numpy().tolist()\n",
    "      self.recordtrainloss = self.checkpoint.tfrecordtrainloss.numpy().tolist()\n",
    "      trainlen = len(self.recordtrainloss)\n",
    "      self.Numsuccess = 0\n",
    "      extrastuff = ''\n",
    "      if self.ValidationFraction > 0.001:\n",
    "        vallen =len(self.recordvalloss)\n",
    "        if vallen > 0:\n",
    "          extrastuff = ' Replaced Val Loss ' + str(round(self.recordvalloss[vallen-1],7))+ ' bad val ' + str(round(self.val_epoch,7))\n",
    "        else:\n",
    "          extrastuff = ' No previous Validation Loss'\n",
    "      print(str(self.epochcount) + ' Failed ' + str(self.Numfailed) + ' Restored Epoch ' + str(trainlen-1) + ' Replaced Loss ' + str(round(self.recordtrainloss[trainlen-1],7))\n",
    "        + ' bad ' + str(round(self.train_epoch,7)) + extrastuff + ' Checkpoint at ' + self.saveMinLosspath)\n",
    "      self.train_epoch = self.recordtrainloss[trainlen-1]\n",
    "      self.Numfailed = 0\n",
    "      self.LastLossValue = self.train_epoch\n",
    "      self.NumberTimesRestored += 1\n",
    "      if self.ValidationFraction > 0.001:\n",
    "        vallen = len(self.recordvalloss)\n",
    "        if vallen > 0:\n",
    "          self.val_epoch = self.recordvalloss[vallen-1]\n",
    "        else:\n",
    "          self.val_epoch =  0.0\n",
    "      return self.tfepochstep, self.recordtrainloss, self.recordvalloss, self.train_epoch, self.val_epoch\n",
    "\n",
    "  def PrintEndofFit(self, Numberofepochs):\n",
    "      print(startbold + 'Number of Saves ' +  str(self.NumberTimesSaved) + ' Number of Restores ' + str(self.NumberTimesRestored))\n",
    "      print('Epochs Requested ' + str(Numberofepochs) + ' Actually Stored ' + str(len(self.recordtrainloss)) + ' ' + str(self.tfepochstep.numpy()) \n",
    "      + ' Successes ' +str(self.AccumulateSuccesses) + resetfonts)\n",
    "      trainlen = len(self.recordtrainloss)\n",
    "      train_epoch1 = self.recordtrainloss[trainlen-1]\n",
    "      lineforval = ''\n",
    "      if self.ValidationFraction > 0.001:\n",
    "        lineforval = ' Last val '+ str(round(self.val_epoch,7))\n",
    "      print(startbold + 'Last loss '+ str(round(self.train_epoch,7)) + ' Last loss in History ' + str(round(train_epoch1,7))+ ' Best Saved Loss '\n",
    "      + str(round(self.BestLossValueSaved,7)) + lineforval + resetfonts)\n",
    "      print(startbold + startred +\"\\nFailure Reasons\" + resetfonts)\n",
    "      for ireason in range(0,len(self.AccumulateFailures)):\n",
    "        print('Optimization Failure ' + str(ireason) + ' ' + self.NameofFailures[ireason] + ' ' + str(self.AccumulateFailures[ireason]))\n",
    "      print(startbold + startred +\"\\nRestore Reasons\" + resetfonts)\n",
    "      for ireason in range(0,len(self.RestoreReasons)):\n",
    "        print('Backup to earlier fit ' + str(ireason) + ' ' + self.NameofRestoreReasons[ireason] + ' ' + str(self.RestoreReasons[ireason]))\n",
    "\n",
    "  def BestPossibleFit(self): # Use Best Saved if appropriate\n",
    "    if self.UseBestAvailableLoss:\n",
    "      if self.BestLossSaved:\n",
    "        if self.BestLossValueSaved < self.train_epoch:\n",
    "          self.checkpoint.tfrecordvalloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "          self.checkpoint.tfrecordtrainloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "          self.checkpoint.restore(save_path=self.saveMinLosspath).expect_partial()\n",
    "          self.tfepochstep  = self.checkpoint.tfepochstep \n",
    "          self.recordvalloss = self.checkpoint.tfrecordvalloss.numpy().tolist()\n",
    "          self.recordtrainloss = self.checkpoint.tfrecordtrainloss.numpy().tolist()\n",
    "          trainlen = len(self.recordtrainloss)\n",
    "          Oldtraining = self.train_epoch\n",
    "          self.train_epoch = self.recordtrainloss[trainlen-1]\n",
    "          extrainfo = ''\n",
    "          if self.ValidationFraction > 0.001:\n",
    "            vallen = len(self.recordvalloss)\n",
    "            if vallen > 0:\n",
    "              extrainfo = '\\nVal Loss ' + str(round(self.recordvalloss[vallen-1],7)) + ' old Val ' + str(round(self.val_epoch,7))\n",
    "              self.val_epoch = self.recordvalloss[vallen-1] \n",
    "            else:\n",
    "              self.val_epoch = 0.0\n",
    "              extrainfo = '\\n no previous validation loss'\n",
    "          print(startpurple+ startbold + 'Switch to Best Saved Value. Restored Epoch ' + str(trainlen-1)\n",
    "          + '\\nNew Loss ' + str(round(self.recordtrainloss[trainlen-1],7)) + ' old ' + str(round(Oldtraining,7))\n",
    "          + extrainfo + '\\nCheckpoint at ' + self.saveMinLosspath + resetfonts)\n",
    "\n",
    "        else:\n",
    "          print(startpurple+ startbold + '\\nFinal fit is best: train ' + str(round(self.train_epoch,7)) + ' Val Loss ' + str(round(self.val_epoch,7)) + resetfonts)\n",
    "    return self.tfepochstep, self.recordtrainloss, self.recordvalloss, self.train_epoch, self.val_epoch   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXTXBkF_VtMW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Record Parameters Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SmcF5FYZJRM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def PrintLSTMandBasicStuff(model):\n",
    "  if SymbolicWindows:\n",
    "    print(' Tseq '+ str(Tseq) + startbold  + startred + ' Symbolic Windows used to save space'+resetfonts)\n",
    "  else:\n",
    "    print(' Tseq '+ str(Tseq) + startbold  + startred + ' Symbolic Windows NOT used'+resetfonts)\n",
    "  print('Training Locations ' + str(TrainingNloc) + ' Validation Locations ' + str(ValidationNloc) +\n",
    "        ' Sequences ' + str(Num_Seq))\n",
    "  if LocationBasedValidation:\n",
    "    print(startbold  + startred + \" Location Based Validation with fraction \" + str(LocationValidationFraction)+resetfonts)\n",
    "    if RestartLocationBasedValidation:\n",
    "      print(startbold  + startred + \" Using Validation set saved in \" + RestartValidationSetRunName+resetfonts)\n",
    "  print('\\nAre futures predicted ' + str(UseFutures) + ' Custom Loss Pointer ' + str(CustomLoss) + ' Class weights used ' + str(UseClassweights))\n",
    "  \n",
    "  print('\\nProperties per sequence ' + str(NpropperseqTOT))\n",
    "  print('\\n' + startbold +startpurple + 'Properties ' + resetfonts)\n",
    "  labelline = 'Name   '\n",
    "  for propval in range (0,7):\n",
    "    labelline += QuantityStatisticsNames[propval] + '    '\n",
    "  print('\\n' + startbold + labelline + resetfonts)\n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    line = startbold + startpurple + str(iprop) + ' ' + InputPropertyNames[PropertyNameIndex[iprop]] + resetfonts  \n",
    "    jprop = PropertyAverageValuesPointer[iprop]\n",
    "    line += ' Root ' + str(QuantityTakeroot[jprop])\n",
    "    for proppredval in range (0,7):\n",
    "      line += ' ' + str(round(QuantityStatistics[jprop,proppredval],3))\n",
    "    print(line)\n",
    "\n",
    "  print('\\nPredictions per sequence ' + str(NpredperseqTOT))\n",
    "  print('\\n' + startbold +startpurple + 'Predictions ' + resetfonts)\n",
    "  print('\\n' + startbold + labelline + resetfonts)\n",
    "  for ipred in range(0,NpredperseqTOT):\n",
    "    line = startbold + startpurple + str(ipred) + ' ' + Predictionname[PredictionNameIndex[ipred]] + ' wgt ' + str(round(Predictionwgt[ipred],3)) + resetfonts + ' '\n",
    "    jpred = PredictionAverageValuesPointer[ipred]\n",
    "    line += ' Root ' + str(QuantityTakeroot[jpred])\n",
    "    for proppredval in range (0,7):\n",
    "      line += ' ' + str(round(QuantityStatistics[jpred,proppredval],3))\n",
    "    print(line)\n",
    "  print('\\n')\n",
    "  print('Plotrealnumbers ' + str(Plotrealnumbers) + ' Root Cases Deaths ' + str(RootCasesDeaths) + ' JournalSimplePrint ' + \n",
    "        str(JournalSimplePrint) + ' UseRealDatesonplots ' + str(UseRealDatesonplots) + ' Plot in DLPrediction2F ' + str(PlotinDL2F))\n",
    "\n",
    "  if model == 0:\n",
    "    print('Number of LSTMworkers ' + str(number_of_LSTMworkers))\n",
    "    print('Number of epochs for each LSTMworker ' + str(LSTMepochs))\n",
    "    print('LSTM Validation Fraction ' +str(LSTMvalidationfrac) + ' Used LSTM Validation Fraction ' +str(UsedLSTMvalidationfrac))\n",
    "    print('Batch size for LSTM ' + str(LSTMbatch_size))\n",
    "    print('LSTM Optimizer ' + str(LSTMoptimizer))\n",
    "  else:\n",
    "    print('Number of epochs for Transformer ' + str(Transformerepochs))\n",
    "  \n",
    "  print('LSTM Activation Method ' + str(LSTMactivationvalue))\n",
    "  print('LSTM recurrent Activation method ' + str(LSTMrecurrent_activation)) \n",
    "  print('LSTM Dropout Layer 1 ' +str(LSTMdropout1) + ' LSTM Recurrent Dropout Layer 1 ' +str(LSTMrecurrent_dropout1) + ' LSTM Dropout Layer >= 2 ' +str(LSTMdropout2) + ' LSTM Recurrent Dropout Layer >=2 ' +str(LSTMrecurrent_dropout2))\n",
    "  print('Number of hidden LSTM nodes ' + str(number_LSTMnodes) + ' Is there a third LSTM layer? ' + str(LSTMThirdLayer))\n",
    "  print('LSTM Initial Embedding layer ' + str(LSTMInitialMLP) + ' Final LSTM Layer ' + str(LSTMFinalMLP))\n",
    "  print('LSTM Verbose Option ' + str(LSTMverbose))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXDzDn44WOQC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n1lxglrD3GJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LSTM Model and Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtHEHa7S_yVq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyLSTMmodel(tf.keras.Model):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(MyLSTMmodel, self).__init__(**kwargs)\n",
    "    self.fullLSTM = MyLSTMlayer()\n",
    "\n",
    "  def call(self, inputs):  \n",
    "    outputs = self.fullLSTM(inputs)\n",
    "    return outputs\n",
    "\n",
    "  def build_graph(self, shapes):\n",
    "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])\n",
    "    \n",
    "class MyLSTMlayer(tf.keras.Model):\n",
    "# Class for a simple multiple layer LSTM with FCN at start and end\n",
    "# All parameters defined externally\n",
    "# structured so MyLSTMlayer can be used standalone or in part of a transformer\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super(MyLSTMlayer, self).__init__(**kwargs)\n",
    "    if (LSTMInitialMLP > 0) and (not LSTMSkipInitial):\n",
    "      self.dense_1 = tf.keras.layers.Dense(LSTMInitialMLP, activation=LSTMactivationvalue)\n",
    "    self.LSTM_1 =tf.keras.layers.LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout1, dropout = LSTMdropout1,\n",
    "                  activation= LSTMactivationvalue , return_sequences=True, recurrent_activation= LSTMrecurrent_activation)\n",
    "    self.LSTM_2 =tf.keras.layers.LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout1, dropout = LSTMdropout1,\n",
    "        activation= LSTMactivationvalue , return_sequences=LSTMThirdLayer, recurrent_activation= LSTMrecurrent_activation)\n",
    "    if(LSTMThirdLayer):\n",
    "      self.LSTM_3 =tf.keras.layers.LSTM(number_LSTMnodes, recurrent_dropout= LSTMrecurrent_dropout1, dropout = LSTMdropout1,\n",
    "                    activation= LSTMactivationvalue , return_sequences=False, recurrent_activation= LSTMrecurrent_activation)\n",
    "    self.dense_2 = tf.keras.layers.Dense(LSTMFinalMLP, activation=LSTMactivationvalue)\n",
    "    self.dense_f = tf.keras.layers.Dense(NpredperseqTOT)\n",
    "\n",
    "  def call(self, inputs, training=None):\n",
    "    if (LSTMInitialMLP > 0) and (not LSTMSkipInitial):\n",
    "      Runningdata = self.dense_1(inputs)\n",
    "      Runningdata = self.LSTM_1(Runningdata, training=training)\n",
    "    else:\n",
    "      Runningdata = self.LSTM_1(inputs, training=training)\n",
    "    Runningdata = self.LSTM_2(Runningdata, training=training)\n",
    "    if(LSTMThirdLayer):\n",
    "      Runningdata = self.LSTM_3(Runningdata, training=training)\n",
    "    if(LSTMFinalMLP > 0):\n",
    "      Runningdata = self.dense_2(Runningdata)\n",
    "    Outputdata = self.dense_f(Runningdata)\n",
    "    return Outputdata\n",
    "\n",
    "  def build_graph(self, shapes):\n",
    "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d4IgddaMoZX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LSTM Class + Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAe2I1U8cujh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyLSTMcustommodel(tf.keras.Model):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(MyLSTMcustommodel, self).__init__(**kwargs)\n",
    "    self.fullLSTM = MyLSTMlayer()\n",
    "\n",
    "  def compile(self, optimizer,  loss, lr):\n",
    "      super(MyLSTMcustommodel, self).compile()\n",
    "      if optimizer == 'adam':\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "      else:\n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "      Dictopt = self.optimizer.get_config()\n",
    "      print(startbold+startred + 'Optimizer ' + resetfonts, Dictopt)\n",
    "      self.loss_object = loss\n",
    "      self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "      self.loss_tracker.reset_states()\n",
    "      self.val_tracker = tf.keras.metrics.Mean(name=\"val\")\n",
    "      self.val_tracker.reset_states()\n",
    "      return\n",
    "\n",
    "  def resetmetrics(self):\n",
    "      self.loss_tracker.reset_states()\n",
    "      self.val_tracker.reset_states()\n",
    "      return\n",
    "\n",
    "  def build_graph(self, shapes):\n",
    "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data, Time=None):\n",
    "    if len(data) == 3:\n",
    "      X_train, y_train, sw_train = data\n",
    "    else:\n",
    "      X_train, y_train = data\n",
    "      sw_train = []\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = self(X_train, training=True)\n",
    "      loss = self.loss_object(y_train, predictions, sw_train)\n",
    "\n",
    "    gradients = tape.gradient(loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "    self.loss_tracker.update_state(loss)\n",
    "    return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data, Time=None):\n",
    "    if len(data) == 3:\n",
    "      X_val, y_val, sw_val = data\n",
    "    else:\n",
    "      X_val, y_val = data\n",
    "      sw_val = []\n",
    "\n",
    "    predictions = self(X_val, training=False)\n",
    "    loss = self.loss_object(y_val, predictions, sw_val)\n",
    "\n",
    "    self.val_tracker.update_state(loss)\n",
    "    return {\"val_loss\": self.val_tracker.result()}\n",
    "\n",
    "  def call(self, inputs, training=None, Time=None):  \n",
    "    outputs = self.fullLSTM(inputs, training=training)\n",
    "    return outputs\n",
    "\n",
    "        \n",
    "def RunLSTMCustomVersion():\n",
    "  # Run the LSTM model defined by Model and Layer class with custom training\n",
    "  # Use Tensorflow datasets\n",
    "\n",
    "  garbagecollectcall = 0\n",
    "  global LSTMvalidationfrac\n",
    "  global UsedLSTMvalidationfrac\n",
    "\n",
    "  if LocationBasedValidation:\n",
    "    UsedLSTMvalidationfrac = LocationValidationFraction\n",
    "    X_predict, y_predict, X_val, y_val = setSeparateDLinput(0)\n",
    "    InitializeDLforTimeSeries('Class custom  Version with location-based validation ',processindex,y_predict)\n",
    "    epochsize = X_predict.shape[0]\n",
    "    if UsedLSTMvalidationfrac > 0.001:\n",
    "      epochsize = X_predict.shape[0] + X_val.shape[0]\n",
    "    if UseClassweights:     \n",
    "      sw = np.empty_like(y_predict, dtype=np.float32)\n",
    "      for j in range(0,sw.shape[0]):\n",
    "        for i in range(0,NpredperseqTOT):\n",
    "          sw[j,i] = Predictionwgt[i] \n",
    "      X_train, y_train, sw_train = shuffleDLinput(X_predict, y_predict, sw)\n",
    "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train, sw_train))\n",
    "    else:\n",
    "      X_train, y_train = shuffleDLinput(X_predict, y_predict)\n",
    "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "      sw_train =[]\n",
    "    \n",
    "    if UsedLSTMvalidationfrac > 0.001:\n",
    "      if UseClassweights:     \n",
    "        sw_val = np.empty_like(y_val, dtype=np.float32)\n",
    "        for j in range(0,sw_val.shape[0]):\n",
    "          for i in range(0,NpredperseqTOT):\n",
    "            sw_val[j,i] = Predictionwgt[i] \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val, sw_val))\n",
    "      else:\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        sw_val =[]\n",
    "\n",
    "# Dimensions are X_predict: OuterBatchDimension,Tseq,NpropperseqTOT\n",
    "# OR if SymbolicWindows OuterBatchDimension,1,1\n",
    "# y_predict OuterBatchDimension,NpredperseqTOT\n",
    "  else:\n",
    "    X_predict, y_predict = setDLinput(Spacetime = False)\n",
    "    InitializeDLforTimeSeries('Class custom  Version ',processindex,y_predict)\n",
    "    epochsize = X_predict.shape[0]\n",
    "\n",
    "    if UseClassweights:     \n",
    "      sw = np.empty_like(y_predict, dtype=np.float32)\n",
    "      for j in range(0,sw.shape[0]):\n",
    "        for i in range(0,NpredperseqTOT):\n",
    "          sw[j,i] = Predictionwgt[i] \n",
    "      X_train, y_train, sw_train = shuffleDLinput(X_predict, y_predict, sw)\n",
    "      print(X_predict.shape)\n",
    "      print(X_train.shape)\n",
    "      print(y_predict.shape)\n",
    "      print(y_train.shape)\n",
    "      print(sw.shape)\n",
    "      print(sw_train.shape)\n",
    "      print(Predictionwgt)\n",
    "\n",
    "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train, sw_train))\n",
    "    else:\n",
    "      X_train, y_train = shuffleDLinput(X_predict, y_predict)\n",
    "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "      sw_train =[]\n",
    "\n",
    "    val_dataset =[]\n",
    "    if UsedLSTMvalidationfrac > 0.001:\n",
    "      total = X_train.shape[0]\n",
    "      totval = int(UsedLSTMvalidationfrac*total)\n",
    "      print(\" Validation samples \", totval, \" Training samples \", total-totval)\n",
    "      if totval > 0:\n",
    "        val_dataset = train_dataset.take(totval)\n",
    "        train_dataset = train_dataset.skip(totval)\n",
    "      else:\n",
    "        UsedLSTMvalidationfrac = 0.0\n",
    "\n",
    "  train_dataset = train_dataset.shuffle(buffer_size = OuterBatchDimension, reshuffle_each_iteration=True)\n",
    "  train_dataset = train_dataset.batch(LSTMbatch_size)\n",
    "  if UsedLSTMvalidationfrac > 0.001:\n",
    "    val_dataset = val_dataset.batch(LSTMbatch_size)\n",
    "\n",
    "\n",
    "  myLSTMcustommodel = MyLSTMcustommodel(name ='myLSTMcustommodel')\n",
    "\n",
    "  myLSTMcustommodel.compile(loss= weightedcustom_lossGCF1, optimizer= LSTMoptimizer, lr= LSTMlearning_rate)\n",
    "\n",
    "  recordtrainloss = []\n",
    "  recordvalloss = []\n",
    "  tfrecordtrainloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "  tfrecordvalloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "  tfepochstep = tf.Variable(0, trainable = False)\n",
    "\n",
    "  usecustomfit = True\n",
    "  if usecustomfit and UseClassweights:\n",
    "\n",
    "# Set up checkpoints to read or write\n",
    "    mycheckpoint = tf.train.Checkpoint(optimizer=myLSTMcustommodel.optimizer, \n",
    "                                     model=myLSTMcustommodel, tfepochstep=tf.Variable(0),\n",
    "                                     tfrecordtrainloss=tfrecordtrainloss,tfrecordvalloss=tfrecordvalloss)\n",
    "    \n",
    "# This restores back up\n",
    "    if Restorefromcheckpoint:\n",
    "      save_path = inputCHECKPOINTDIR + inputRunName + inputCheckpointpostfix\n",
    "      mycheckpoint.restore(save_path=save_path).expect_partial()\n",
    "      tfepochstep  = mycheckpoint.tfepochstep \n",
    "      recordvalloss = mycheckpoint.tfrecordvalloss.numpy().tolist()\n",
    "      recordtrainloss = mycheckpoint.tfrecordtrainloss.numpy().tolist()\n",
    "      trainlen = len(recordtrainloss)\n",
    "      extrainfo = ''\n",
    "      vallen = len(recordvalloss)\n",
    "      SavedTrainLoss = recordtrainloss[trainlen-1]\n",
    "      SavedValLoss = 0.0\n",
    "      if vallen > 0:\n",
    "        extrainfo = ' Val Loss ' + str(round(recordvalloss[vallen-1],7))\n",
    "        SavedValLoss = recordvalloss[vallen-1]\n",
    "      print(startbold + 'Network restored from ' + save_path + '\\nLoss ' + str(round(recordtrainloss[trainlen-1],7)) \n",
    "       + extrainfo + ' Epochs ' + str(tfepochstep.numpy()) + resetfonts )\n",
    "      LSTMTFMonitor.SetCheckpointParms(mycheckpoint,CHECKPOINTDIR,RunName = RunName,Restoredcheckpoint= True, \n",
    "              Restored_path = save_path,  ValidationFraction = UsedLSTMvalidationfrac, SavedTrainLoss = SavedTrainLoss, \n",
    "              SavedValLoss =SavedValLoss)\n",
    "    else:\n",
    "      LSTMTFMonitor.SetCheckpointParms(mycheckpoint,CHECKPOINTDIR,RunName = RunName,Restoredcheckpoint= False, \n",
    "                                       ValidationFraction = UsedLSTMvalidationfrac)\n",
    "\n",
    "# This just does analysis      \n",
    "    if AnalysisOnly:\n",
    "      if OutputNetworkPictures:\n",
    "        outputpicture1 = APPLDIR +'/Outputs/Model_' +RunName + '1.png'\n",
    "        outputpicture2 = APPLDIR +'/Outputs/Model_' +RunName + '2.png'\n",
    "        tf.keras.utils.plot_model(myLSTMcustommodel.build_graph([Tseq,NpropperseqTOT]), \n",
    "                            show_shapes=True, to_file = outputpicture1,\n",
    "                            show_dtype=True, \n",
    "                            expand_nested=True)\n",
    "        tf.keras.utils.plot_model(myLSTMcustommodel.fullLSTM.build_graph([Tseq,NpropperseqTOT]), \n",
    "                            show_shapes=True, to_file = outputpicture2,\n",
    "                            show_dtype=True, \n",
    "                            expand_nested=True)\n",
    "      if SymbolicWindows:\n",
    "        finalizeDL(myLSTMcustommodel,recordtrainloss, recordvalloss,UsedLSTMvalidationfrac,\n",
    "              ReshapedSequencesTOT, RawInputPredictionsTOT,0,LabelFit = 'Non-sampled LSTM Fit')\n",
    "      else:\n",
    "        finalizeDL(myLSTMcustommodel,recordtrainloss, recordvalloss,UsedLSTMvalidationfrac,\n",
    "              RawInputSequencesTOT, RawInputPredictionsTOT,0,LabelFit = 'Non-sampled LSTM Fit')\n",
    "      SummarizeFullLSTMModel(myLSTMcustommodel)\n",
    "      return\n",
    "\n",
    "# Initialize progress bars\n",
    "    pbar = notebook.trange(LSTMepochs, desc='Training loop', unit ='epoch')\n",
    "    bbar = notebook.trange(epochsize,  desc='Batch    loop', unit  = 'sample')\n",
    "\n",
    "    train_epoch = 0.0 # Training Loss this epoch\n",
    "    val_epoch = 0.0 # Validation Loss this epoch\n",
    "\n",
    "    Ctime1 = 0.0\n",
    "    Ctime2 = 0.0\n",
    "    Ctime3 = 0.0\n",
    "    Ctime4 = 0.0\n",
    "    Ctime5 = 0.0\n",
    "    Ctime6 = 0.0\n",
    "    Ctime7 = 0.0\n",
    "    GarbageCollect = True\n",
    "\n",
    "    for e in pbar:\n",
    "      myLSTMcustommodel.resetmetrics()\n",
    "      train_lossoverbatch=[]\n",
    "      val_lossoverbatch=[]\n",
    "      \n",
    "      if batchperepoch:\n",
    "        qbar = notebook.trange(epochsize, desc='Batch loop epoch ' +str(e))\n",
    "      \n",
    "      \n",
    "      for batch, (X_train, y_train, sw_train) in enumerate(train_dataset.take(-1)):\n",
    "        StopWatch.start('label7')\n",
    "        Numinbatch = X_train.shape[0]\n",
    "        # SymbolicWindows X_train is indexed by Batch index, 1(replace by Window), 1 (replace by properties)\n",
    "        if SymbolicWindows:\n",
    "          StopWatch.start('label1')\n",
    "          X_train = X_train.numpy()          \n",
    "          X_train = np.reshape(X_train,Numinbatch)\n",
    "          iseqarray = np.right_shift(X_train,16)\n",
    "          ilocarray = np.bitwise_and(X_train, 0b1111111111111111)\n",
    "          StopWatch.stop('label1')\n",
    "          Ctime1 += StopWatch.get('label1', digits=4)\n",
    "          StopWatch.start('label3')\n",
    "          X_train_withSeq = list()\n",
    "          for iloc in range(0,Numinbatch):\n",
    "            X_train_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "#         X_train_withSeq=[ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq] for iloc in range(0,Numinbatch)]\n",
    "          StopWatch.stop('label3')\n",
    "          Ctime3 += StopWatch.get('label3', digits=5)\n",
    "          StopWatch.start('label2')\n",
    "          loss = myLSTMcustommodel.train_step((np.array(X_train_withSeq), y_train, sw_train))\n",
    "          StopWatch.stop('label2')\n",
    "          Ctime2 += StopWatch.get('label2', digits=4)\n",
    "\n",
    "        else:\n",
    "          StopWatch.start('label2')\n",
    "          loss = myLSTMcustommodel.train_step((X_train, y_train, sw_train))\n",
    "          StopWatch.stop('label2')\n",
    "          Ctime2 += StopWatch.get('label2', digits=4)\n",
    "\n",
    "        if GarbageCollect:\n",
    "          StopWatch.start('label4')\n",
    "\n",
    "          if SymbolicWindows:\n",
    "            X_train_withSeq = None\n",
    "          X_train = None\n",
    "          y_train = None\n",
    "          sw_train = None\n",
    "          if garbagecollectcall > GarbageCollectionLimit:\n",
    "            garbagecollectcall = 0\n",
    "            gc.collect()\n",
    "          garbagecollectcall += 1\n",
    "          StopWatch.stop('label4')\n",
    "          Ctime4 += StopWatch.get('label4', digits=5)\n",
    "\n",
    "        localloss = loss[\"loss\"].numpy()\n",
    "        train_lossoverbatch.append(localloss)\n",
    "\n",
    "        if batchperepoch:\n",
    "          qbar.update(LSTMbatch_size)\n",
    "          qbar.set_postfix(Loss = localloss, Epoch = e)\n",
    "        bbar.update(Numinbatch)\n",
    "        bbar.set_postfix(Loss = localloss, Epoch = e)\n",
    "        StopWatch.stop('label7')\n",
    "        Ctime7 += StopWatch.get('label7', digits=5)\n",
    "# End Training step for one batch\n",
    "\n",
    "# Start Validation \n",
    "      if UsedLSTMvalidationfrac > 0.001:\n",
    "        StopWatch.start('label5')\n",
    "        for batch, (X_val, y_val, sw_val) in enumerate(val_dataset.take(-1)):\n",
    "          Numinbatch = X_val.shape[0]\n",
    "          # SymbolicWindows X_val is indexed by Batch index, 1(replace by Window), 1 (replace by properties)\n",
    "          if SymbolicWindows:\n",
    "            StopWatch.start('label1')\n",
    "            X_val = X_val.numpy()          \n",
    "            X_val = np.reshape(X_val,Numinbatch)\n",
    "            iseqarray = np.right_shift(X_val,16)\n",
    "            ilocarray = np.bitwise_and(X_val, 0b1111111111111111)\n",
    "            StopWatch.stop('label1')\n",
    "            Ctime1 += StopWatch.get('label1', digits=4)\n",
    "            StopWatch.start('label3')\n",
    "            X_valFull = list()\n",
    "            for iloc in range(0,Numinbatch):\n",
    "              X_valFull.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "            StopWatch.stop('label3')\n",
    "            Ctime3 += StopWatch.get('label3', digits=5)\n",
    "            StopWatch.start('label2')\n",
    "            loss = myLSTMcustommodel.test_step((np.array(X_valFull), y_val, sw_val))\n",
    "            StopWatch.stop('label2')\n",
    "            Ctime2 += StopWatch.get('label2', digits=4)\n",
    "\n",
    "          else:\n",
    "            loss = myLSTMcustommodel.test_step((X_val, y_val, sw_val))\n",
    "\n",
    "          localval = loss[\"val_loss\"].numpy()\n",
    "          val_lossoverbatch.append(localval)\n",
    "          \n",
    "          bbar.update(X_val.shape[0])\n",
    "          bbar.set_postfix(Val_loss = localval, Epoch = e)\n",
    "        StopWatch.stop('label5')\n",
    "        Ctime5 += StopWatch.get('label5', digits=5)\n",
    "# End Batch\n",
    "\n",
    "      train_epoch = train_lossoverbatch[-1]\n",
    "      recordtrainloss.append(train_epoch)\n",
    "      mycheckpoint.tfrecordtrainloss = tf.Variable(recordtrainloss)\n",
    "\n",
    "      val_epoch = 0.0\n",
    "      if UsedLSTMvalidationfrac > 0.001:\n",
    "        val_epoch = val_lossoverbatch[-1]\n",
    "        recordvalloss.append(val_epoch)\n",
    "        mycheckpoint.tfrecordvalloss = tf.Variable(recordvalloss)\n",
    "\n",
    "      pbar.set_postfix(Loss = train_epoch, Val = val_epoch)\n",
    "      bbar.reset()\n",
    "      tfepochstep = tfepochstep + 1\n",
    "      mycheckpoint.tfepochstep.assign(tfepochstep)\n",
    "\n",
    "# Decide on best fit\n",
    "      StopWatch.start('label6')\n",
    "      MonitorResult, train_epoch, val_epoch = LSTMTFMonitor.EpochEvaluate(e,train_epoch, val_epoch, \n",
    "          tfepochstep, recordtrainloss, recordvalloss)\n",
    "      if MonitorResult==1:\n",
    "        tfepochstep, recordtrainloss, recordvalloss, train_epoch, val_epoch = LSTMTFMonitor.RestoreBestFit() # Restore Best Fit\n",
    "      StopWatch.stop('label6')\n",
    "      Ctime6 += StopWatch.get('label6', digits=5)\n",
    "      continue\n",
    "# *********************** End of Epoch Loop\n",
    "\n",
    "# Print Fit details\n",
    "    print(startbold + 'Times Symbolic-1 ' + str(round(Ctime1,5))  + ' Symbolic-2 ' + str(round(Ctime3,5)) + ' TF ' + str(round(Ctime2,5)) + ' GarbageC ' + str(round(Ctime4,5)) + resetfonts)\n",
    "    print(startbold + 'Times Training ' + str(round(Ctime7,5))  + ' Validation ' + str(round(Ctime5,5)) + ' Monitor ' + str(round(Ctime6,5))  + resetfonts)\n",
    "    LSTMTFMonitor.PrintEndofFit(LSTMepochs)\n",
    "\n",
    "# Set Best Possible Fit\n",
    "    tfepochstep, recordtrainloss, recordvalloss, train_epoch, val_epoch = LSTMTFMonitor.BestPossibleFit()\n",
    "\n",
    "    if Checkpointfinalstate:\n",
    "      savepath = mycheckpoint.save(file_prefix=CHECKPOINTDIR + RunName)\n",
    "      print('Checkpoint at ' + savepath + ' from ' + CHECKPOINTDIR)\n",
    "    trainlen = len(recordtrainloss)\n",
    "    extrainfo = ''\n",
    "    if UsedLSTMvalidationfrac > 0.001:\n",
    "      vallen = len(recordvalloss)\n",
    "      extrainfo = ' Val Epoch ' + str(vallen-1) + ' Val Loss ' + str(round(recordvalloss[vallen-1],7))\n",
    "    print('Train Epoch ' + str(trainlen-1) + ' Train Loss ' + str(round(recordtrainloss[trainlen-1],7)) + extrainfo)\n",
    "\n",
    "\n",
    "  else:\n",
    "    the_callbacks = [TqdmCallback()]\n",
    "    modelresult = myLSTMcustommodel.fit(train_dataset,\n",
    "          validation_data = val_dataset,\n",
    "          epochs=LSTMepochs,\n",
    "          batch_size=None,\n",
    "          verbose = LSTMverbose,\n",
    "          callbacks=the_callbacks\n",
    "          )\n",
    "    recordtrainloss = modelresult.history['loss']\n",
    "    recordvalloss = modelresult.history['val_loss']\n",
    "\n",
    "  SummarizeFullLSTMModel(myLSTMcustommodel)\n",
    "  if OutputNetworkPictures:\n",
    "    outputpicture1 = APPLDIR +'/Outputs/Model_' +RunName + '1.png'\n",
    "    outputpicture2 = APPLDIR +'/Outputs/Model_' +RunName + '2.png'\n",
    "    tf.keras.utils.plot_model(myLSTMcustommodel.build_graph([Tseq,NpropperseqTOT]), \n",
    "                        show_shapes=True, to_file = outputpicture1,\n",
    "                        show_dtype=True, \n",
    "                        expand_nested=True)\n",
    "    tf.keras.utils.plot_model(myLSTMcustommodel.fullLSTM.build_graph([Tseq,NpropperseqTOT]), \n",
    "                        show_shapes=True, to_file = outputpicture2,\n",
    "                        show_dtype=True, \n",
    "                        expand_nested=True)\n",
    "  if SymbolicWindows:\n",
    "    finalizeDL(myLSTMcustommodel,recordtrainloss,recordvalloss,UsedLSTMvalidationfrac,ReshapedSequencesTOT, RawInputPredictionsTOT,0)\n",
    "  else:\n",
    "    finalizeDL(myLSTMcustommodel,recordtrainloss,recordvalloss,UsedLSTMvalidationfrac,RawInputSequencesTOT, RawInputPredictionsTOT,0)\n",
    "  return\n",
    "\n",
    "def SummarizeFullLSTMModel(DLmodel):\n",
    "  DLmodel.fullLSTM.summary()\n",
    "  DLmodel.summary()\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHkeY-HYZJRM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##LSTM Run & Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0ug_91p0yTF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXDPdPioY9jm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run LSTM Only\n",
    "if UseLSTMModel:\n",
    "  AnalysisOnly = True\n",
    "  Dumpoutkeyplotsaspics = True\n",
    "  Restorefromcheckpoint = False\n",
    "  Checkpointfinalstate = True\n",
    "  if AnalysisOnly:\n",
    "    Restorefromcheckpoint = True\n",
    "    Checkpointfinalstate = False\n",
    "  if Restorefromcheckpoint:\n",
    "    inputRunName = RunName\n",
    "    inputCHECKPOINTDIR = CHECKPOINTDIR\n",
    "#    inputRunName = 'Hydroln3A'\n",
    "    inputCheckpointpostfix = 'MinLoss-54'\n",
    "    inputCHECKPOINTDIR = APPLDIR + \"/checkpoints/\" + inputRunName + \"dir/\"\n",
    "\n",
    "  batchperepoch = False # if True output a batch bar for each epoch\n",
    "  GlobalSpacetime = False\n",
    "  IncreaseNloc_sample = 1\n",
    "  DecreaseNloc_sample = 1\n",
    "\n",
    "  Plotrealnumbers = False\n",
    "  SkipDL2F = False\n",
    "  PlotinDL2F = False\n",
    "\n",
    "  # Run Pure LSTM\n",
    "  LSTMbatch_size = TrainingNloc # Total number of counties for Covid\n",
    "  LSTMbatch_size = 128\n",
    "  LSTMbatch_size = min(LSTMbatch_size, TrainingNloc)\n",
    "  LSTMepochs = 80\n",
    "  LSTMlearning_rate = 0.0003 *0.1\n",
    "  \n",
    "  number_LSTMnodes= 160\n",
    "  LSTMFinalMLP = 160\n",
    "  LSTMInitialMLP = 160\n",
    "  LSTMThirdLayer = False\n",
    "  \n",
    "  processindex = 0\n",
    "  standaloneLSTMrun = False\n",
    "  ClassLSTMrun = True\n",
    "  CustomTraining = True\n",
    "\n",
    "  if ClassLSTMrun and CustomTraining:\n",
    "    FullSetValidation = False\n",
    "    LSTMTFMonitor = TensorFlowTrainingMonitor()\n",
    "    if Hydrology:\n",
    "      LSTMTFMonitor.SetControlParms(SuccessLimit = 1,FailureLimit = 2)\n",
    "    if Earthquake:\n",
    "      LSTMTFMonitor.SetControlParms(SuccessLimit = 1,FailureLimit = 2)\n",
    "    if ReadJan2021Covid or ReadApril2021Covid:\n",
    "      LSTMTFMonitor.SetControlParms(SuccessLimit = 3,FailureLimit = 2)\n",
    "\n",
    "  current_time = timenow()\n",
    "  runtype = ''\n",
    "  if Restorefromcheckpoint:\n",
    "    runtype = 'Restarted '\n",
    "  if standaloneLSTMrun or ClassLSTMrun:\n",
    "    print(wraptotext(startbold + startred +  current_time + ' '  + runtype + RunName + ' ' + RunComment + resetfonts))\n",
    "    PrintLSTMandBasicStuff(0)\n",
    "\n",
    "  if ClassLSTMrun:\n",
    "    if SymbolicWindows:\n",
    "      CustomTraining = True\n",
    "    if CustomTraining:\n",
    "      RunLSTMCustomVersion()\n",
    "  if standaloneLSTMrun or ClassLSTMrun:\n",
    "    print(startbold + startpurple +  'LSTM run completed ' + runtype + RunName + ' ' + RunComment + resetfonts)\n",
    "    sys.exit(0)\n",
    "  print(startbold + startpurple +  current_time + ' UTC Start Hybrid Transformer run ' + RunName + ' ' + RunComment + resetfonts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c38Cul5oghJO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Science Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxl3pnzgs9re",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Important Parameters defining Transformer project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSP5_8-hs9sB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if UseScienceTransformerModel or UseTFTModel:\n",
    "  ActivateAttention = False\n",
    "  DoubleQKV = False\n",
    "  TimeShufflingOnly = False\n",
    "  Transformerbatch_size = 1\n",
    "  Transformervalidationfrac = 0.0\n",
    "  UsedTransformervalidationfrac = 0.0\n",
    "  Transformerepochs = 200\n",
    "  Transformeroptimizer ='adam'\n",
    "  Transformerverbose = 0\n",
    "  TransformerOnlyFullAttention = True\n",
    "  d_model =64\n",
    "  d_Attention = 2 * d_model\n",
    "  if TransformerOnlyFullAttention:\n",
    "    d_Attention = d_model\n",
    "  d_qk = d_model\n",
    "  d_intermediateqk = 2 * d_model\n",
    "  num_heads = 2\n",
    "  num_Encoderlayers = 2\n",
    "  EncoderDropout= 0.1\n",
    "  EncoderActivation = 'selu'\n",
    "  d_EncoderLayer = d_Attention\n",
    "  d_merge = d_model\n",
    "  d_ffn = 4*d_model\n",
    "  MaskingOption = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xm7X3Ww23_Ta",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***Num_Seq*** Number of Sequences\n",
    "\n",
    "***Nloc*** Number of locations\n",
    "\n",
    "***Tseq*** Length of each sequence\n",
    "\n",
    "***Npropperseq*** Number of internal properties per sequence including static or dynamic\n",
    "\n",
    "***NpredperseqTOT*** Total number of predictions per sequence\n",
    "\n",
    "***d_model*** Dimension of value embedding for every input [Model1] \n",
    "\n",
    "***num_heads*** Number of Attention Heads which must exactly divide ***d_model***\n",
    "\n",
    "***num_Encoderlayers*** Number of layers in Encoder stage\n",
    "\n",
    "***EncoderDropout*** Dropout in Encoder stage, \n",
    "\n",
    "***d_ffn*** Size of feedforward network in each encoder layer. It appears to bet to be 4 * ***d_model*** \n",
    "\n",
    "***MaskingOption*** defines masking used; = 0 none; =1 mask the future\n",
    "\n",
    "***Transformerbatch_size*** is batch size of stochastic gradient descent\n",
    "\n",
    "***Tseq*** is size of sequence window\n",
    "\n",
    "***Transformervalidationfrac*** is fraction used for a validation dataset\n",
    "\n",
    "***d_sample*** The number of units presented to the Transformer which could be dynamic. Each of these inputs is used to calculate attention and is Tseq times number of locations simultaneously presented\n",
    "\n",
    "***max_d_sample*** The maximum number of units presented to the Transformer which is fixed. It is Tseq times maximum number of locations simultaneously presented\n",
    "\n",
    "***TransformerOnlyFullAttention*** if True calculate classic attention over all d_sample members; if False calculate separate attentions in location and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCZ_vAQZnOSA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Describe the science data sets here\n",
    "\n",
    "Initial data is [Num_Seq][Nloc][Tseq] with values [Nforcing + ExPosEnc-Selin + Nprop-Sel]\n",
    "\n",
    "Predictions are [Num_Seq] [Nloc] [Predvals=Npred+ExPosEnc-Selout] [Predtimes = Forecast-time range]\n",
    "\n",
    "A subset is included in each transformer call. There are 3 options\n",
    "*   Simplest: (as in LSTM) samples are labelled by [Num_Seq] [Nloc] and have input length [Tseq] with multiple features [Nforcing + ExPosEnc-Selin + Nprop-Sel] mapped into a model for each input of length [Model1]. Predictions for each input are [Predvals] [Predtimes]\n",
    "*   Straightforward improvement:  Divide Nloc into sublocation groups giving Nloc/Nsub groups with Nloc-Nsub locations in each group. There are many choices of groups including fixed disjoint subgroups, overlapping groups (so that each epoch surveys each location twice and this should help spread attention). Then each sample is labelled by [Num_Seq] [Nloc/Nsub] and have input length [Nloc-Nsub][Tseq] with multiple features [Nforcing + ExPosEnc-Selin + Nprop-Sel] mapped into a model for each input of length [Model1]. Predictions for each input are [Predvals] [Predtimes]\n",
    "*   (Too) Clever: Use different selections for Encoder and Decoder steps. For example feed all Nloc locations into transformer but oinly use through multi-headed attention step. One only takes a subset of these through encoder and predictions. This ensures that attention covers all locations\n",
    "*   Extend \"Too clever\"  or \"Straightforward\" method for multiple initial time values in each transformer input i.e. divide [Num_Seq] into [Num_Seq/Ntsub] groups and input [Num_Seq-Ntsub] time sequences into a single transformer. This spreads attention over time. \n",
    "\n",
    "We can represent all the above cases by lasbelling each data sample by\n",
    "{[Num_Seq][Nloc]} [Tseq] where always members of sequences are complete and selection of sequences and location for a single data sample varies in options above. Each member of a data sample has [Nforcing + ExPosEnc-Selin + Nprop-Sel] internal degrees of freedom. These degrees of freedom will be mapped (embedded) in a model variable of length ***d_model***\n",
    "\n",
    "Size of input is ***d_sample*** = Tseq * size {[Num_Seq][Nloc]} in a single data sample. This is important throughout network whereas [Nforcing + ExPosEnc-Selin + Nprop-Sel] is immediately embedded and becomes of length ***d_model***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsYiyg_6c-U3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up data\n",
    "# Initial data is [For Batching][Nloc_sample] [Tseq] [NpropperseqTOT] starting with RawInputSequencesTOT\n",
    "# Predictions are [For Batching][Nloc_sample] [NpredperseqTOT] starting with RawInputPredictionsTOT\n",
    "#  For case Nloc_sample = Nloc, the Batching is identical to Time sequence label\n",
    "# dsample Tseq * Nloc\n",
    "\n",
    "if UseScienceTransformerModel:\n",
    "  Nloc_sample = Nloc\n",
    "  OuterBatchDimension = Num_Seq\n",
    "  if Nloc%Nloc_sample != 0:\n",
    "    print(\"Inconsistent location numbers \" + str(Nloc) + ' ' + str(Nloc_sample))\n",
    "    sys.exit()\n",
    "  d_sample = Tseq * Nloc\n",
    "  max_d_sample = d_sample\n",
    "  Transformermaximum_position_encoding = max_d_sample\n",
    "  if SymbolicWindows:\n",
    "    X_Transformerdetailed = np.copy(SymbolicInputSequencesTOT)\n",
    "  else:\n",
    "    X_Transformerdetailed = np.copy(RawInputSequencesTOT)\n",
    "\n",
    "  y_Transformerdetailed = np.copy(RawInputPredictionsTOT)\n",
    "\n",
    "  print(\"The maximum number of units presented to the Transformer which is fixed.It is Tseq times maximum number of locations simultaneously presented \", str(max_d_sample))\n",
    "  print(\"The actual number of units presented to the Transformer for this batch.It is Tseq times  number of locations simultaneously presented in  this batch \", str(max_d_sample))\n",
    "  print(\" Number of locations in each sample presented to the Transformer \", str(Nloc_sample))\n",
    "  print(\"Number of locations times sequence window length in each sample presented to the Transformer \", str(OuterBatchDimension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xluDl5cXYy4y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Scaled dot product attention for Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoaxZAH4ZQCJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This seems unchanged for science case unless ***d_sample*** is too large. One wastes time then if softmaxes too small. It could be useful just to select the largest (e.g. take top 10 or remove all probabilities < 0.001) softmax values and just process with these\n",
    "\n",
    "Below all vectors Q K V have size ***d_model/num_heads***. They are defined for each head and for each sample member of the ***d_sample*** members\n",
    "\n",
    "This could use the  mask described earlier but that is not used in initial version\n",
    "\n",
    "We have a hierarchial sequence label which limits number of softmaxes calculated although number of Q K V vectors are not reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at4BqdMkqJYb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scaled dot product attention: Q K V Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsxEE_-Wa1gF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LazzUq3bJ5SH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def CalculateFullAttention(q,k,v,num_heads, mask=None):\n",
    "\n",
    "  depth = tf.shape(k)[-1]\n",
    "  dk = tf.cast(depth, tf.float32) # dk is depth in all methods\n",
    "  d_qk  = num_heads * depth\n",
    "  if SeparateHeads: # Spread out to save space\n",
    "    HeadedAttentionVectorList = []\n",
    "    for ihead in range(0,num_heads):\n",
    "      matmul_qk = tf.matmul(q[:,ihead,:,:], k[:,ihead,:,:], transpose_b=True)  # [Batch, ihead, d_sample, depth] [Batch, ihead, (d_sample, depth)T ]\n",
    "      scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # [Batch, d_sample, d_sample]\n",
    "      matmul_qk = None\n",
    "      # add the mask to the scaled tensor.\n",
    "      if mask is not None: # batch,d_sample, d_sample\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "      # softmax is normalized on the last axis (seq_len_k = d_sample) so that the scores add up to 1.\n",
    "      # scaled_attention_logits and attention_weights [Batch,  d_sample, d_sample] \n",
    "      attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n",
    "      scaled_attention_logits = None\n",
    "      if mask is not None:\n",
    "        attention_weights = tf.where(mask > 0.2, 0.0, attention_weights)\n",
    "\n",
    "      # [Batch,  d_sample, d_sample] [Batch,  d_sample, depth]\n",
    "      OneheadAttentionVectorfull = tf.matmul(attention_weights, v[:,ihead,:,:])  \n",
    "      HeadedAttentionVectorList.append(OneheadAttentionVectorfull)# [num_heads, Batch,  d_sample,  depth]\n",
    "\n",
    "    AttentionVectorfull = tf.convert_to_tensor(HeadedAttentionVectorList)\n",
    "    # [Batch, num_heads,  d_sample,  depth]\n",
    "    AttentionVectorfull = tf.transpose(AttentionVectorfull, perm = [1,0,2,3])\n",
    "    HeadedAttentionVectorList = None\n",
    "    OneheadAttentionVectorfull = None\n",
    "\n",
    "  else:\n",
    "    if ChopupMatrix:\n",
    "      basicsize = math.floor((Nloc_sample+0.001)/ChopupNumber)\n",
    "      remainder = Nloc_sample%basicsize\n",
    "      Tensorlist =[]\n",
    "      upperlimit = 0\n",
    "      for choploop in range(0,ChopupNumber):\n",
    "        lowerlimit = upperlimit\n",
    "        upperlimit = lowerlimit + basicsize\n",
    "        if choploop < remainder:\n",
    "          upperlimit += 1\n",
    "  # q k v [Batch, num_heads, d_sample, depth]   \n",
    "        upperlimitSeq = upperlimit*Tseq\n",
    "        lowerlimitSeq = lowerlimit*Tseq       \n",
    "        matmul_qk = tf.matmul(q[:,:,lowerlimitSeq:upperlimitSeq,:], k[:,:,lowerlimitSeq:upperlimitSeq,:], transpose_b=True)  \n",
    "  # [Batch, num_heads, d_sampleCHOP, depth] [Batch, num_heads, (d_sampleCHOP, depth)T ]\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # [Batch, num_heads, d_sampleCHOP, d_sampleCHOP]\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None: # batch,d_sample, d_sample\n",
    "          smallmask = mask[:,lowerlimitSeq:upperlimitSeq,lowerlimitSeq:upperlimitSeq]\n",
    "          smallmask = tf.reshape(smallmask,[smallmask.shape[0],1,smallmask.shape[1],smallmask.shape[2]])\n",
    "          scaled_attention_logits += (smallmask * -1e9)\n",
    "          \n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # scaled_attention_logits [Batch, num_heads, d_sampleCHOP, d_sampleCHOP]\n",
    "        if mask is not None:\n",
    "          attention_weights = tf.where(smallmask > 0.2, 0.0, attention_weights)\n",
    "        Tensorlist.append(tf.matmul(attention_weights, v[:,:,lowerlimitSeq:upperlimitSeq,:]))\n",
    "      AttentionVectorfull = tf.concat(Tensorlist,axis=2)\n",
    "      Tensorlist = None\n",
    "      smallmask = None\n",
    "\n",
    "    else: # FULL ATTENTION Integrated all heads done together\n",
    "      matmul_qk = tf.matmul(q, k, transpose_b=True)  # [Batch, num_heads, d_sample, depth] [Batch, num_heads, (d_sample, depth)T ]\n",
    "      scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # [Batch, num_heads, d_sample, d_sample]\n",
    "\n",
    "      # add the mask to the scaled tensor.\n",
    "      if mask is not None: # batch,d_sample, d_sample\n",
    "        mask = tf.reshape(mask,[mask.shape[0],1,mask.shape[1],mask.shape[2]])\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "      # softmax is normalized on the last axis (seq_len_k = d_sample) so that the scores add up to 1.\n",
    "      # scaled_attention_logits [Batch, num_heads, d_sample, d_sample] \n",
    "      attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
    "      if mask is not None:\n",
    "        attention_weights = tf.where(mask > 0.2, 0.0, attention_weights)\n",
    "\n",
    "      AttentionVectorfull = tf.matmul(attention_weights, v)  # [Batch, num_heads, d_sample, d_sample] [Batch, num_heads, d_sample, d_v]\n",
    "\n",
    "  # Below done for FULL ATTENTION as only Attention whatever SeparateHeads\n",
    "  AttentionVectortemp = tf.transpose(AttentionVectorfull, perm = [0,2,1,3]) # [Batch,  d_sample, num_heads, d_v]\n",
    "  AttentionVector = tf.reshape(AttentionVectortemp, [q.shape[0],d_sample, num_heads*d_v]) # restore shape\n",
    "  AttentionVectortemp = None\n",
    "  scaled_attention_logits = None\n",
    "  attention_weights = None\n",
    "  matmul_qk = None\n",
    "  return AttentionVector\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  # Needs externally defined  num_heads, d_sample, Nloc_sample, Tseq, d_model\n",
    "  # Calculates depth\n",
    "  \"\"\"\n",
    "  Calculate the attention weights after Q K V found\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "In the science Transformer, Q K V are all Batch, num_heads, d_sample, depth\n",
    "where Q K V all have same number of samples d_sample\n",
    "depth is always d_model/num heads\n",
    "\n",
    "d_sample is really [Nloc_sample][Tseq] and calculates separate Location and Time Attention\n",
    "\n",
    "Original returned attention weights but ignored. We do NOT return but rather return two attention vectors in Location and Time\n",
    "If TransformerOnlyFullAttention specified, it returns  traditional full attention vector\n",
    "  Returns:\n",
    "    AttentionVector1 concatenated with AttentionVector2\n",
    "  \"\"\"\n",
    "  # To scale matmul_qk\n",
    "  depth = tf.shape(k)[-1]\n",
    "  dk = tf.cast(depth, tf.float32) # dk is depth in all methods\n",
    "  d_qk  = num_heads * depth\n",
    "\n",
    "# This executes JUST ONE ATTENTION -  Full\n",
    "  if TransformerOnlyFullAttention:\n",
    "    AttentionVector = CalculateFullAttention(q,k,v,num_heads,mask)\n",
    "    mask = None\n",
    "# End ONE WAY ATTENTION\n",
    "\n",
    "  else: # Two way Attention\n",
    "    # FIRST Calculate Attention in Time. This requires no reordering and so can be redone with shape\n",
    "    # Note matmul works for all number of indices and multiplication is only done on last 2 indices \n",
    "    # so using qtime means we look at attention with location fixed\n",
    "    qtime = tf.reshape(q, [-1, num_heads, Nloc_sample, Tseq, depth ])\n",
    "    ktime = tf.reshape(k, [-1, num_heads, Nloc_sample, Tseq, depth ])\n",
    "    vtime = tf.reshape(v, [-1, num_heads, Nloc_sample, Tseq, d_v ])\n",
    "    matmul_qk = tf.matmul(qtime, ktime, transpose_b=True)  # [Batch, num_heads, Nloc_sample, Tseq, depth] [Batch, num_heads, Nloc_sample, (Tseq, depth)T ]\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # [Batch, num_heads, Nloc_sample, Tseq, Tseq]\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "#      mask = tf.reshape(mask,[mask.shape[0],Nloc_sample,Tseq,Nloc_sample,Tseq]) Calulate from full mask\n",
    "#      timemask= tf.transpose(mask,perm = [0,2,4,1,3])\n",
    "#      timemask = tf.linalg.diag_part(timemask)\n",
    "#      timemask = tf.transpose(timemask,perm = [0,3,1,2])\n",
    "#      timemask = tf.reshape(timemask,[timemask.shape[0],1,Nloc_sample, Tseq, Tseq])\n",
    "# Global Time Mask is [1,1,1,Tseq,Tseq]\n",
    "      timemask =tf.convert_to_tensor(GlobalTimeMask, dtype = tf.float32) # [Batch, num_heads, Nloc_sample, Tseq, Tseq]\n",
    "      scaled_attention_logits += (timemask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (Tseq) so that the scores add up to 1 in time direction separately for each space.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # scaled_attention_logits [Batch, num_heads, Nloc_sample, Tseq, Tseq] \n",
    "    if mask is not None:\n",
    "      attention_weights = tf.where(timemask > 0.2, 0.0, attention_weights)\n",
    "                \n",
    "    AttentionVector1time = tf.matmul(attention_weights, vtime)  # [Batch, num_heads, Nloc_sample, Tseq, Tseq] [Batch, num_heads,  Nloc_sample, Tseq, d_v] becomes [Batch, num_heads, Nloc_sample, Tseq, depth]\n",
    "    AttentionVector1temp = tf.transpose(AttentionVector1time, perm = [0,2,3,1,4]) # [Batch,  Nloc_sample, Tseq, num_heads, d_v]\n",
    "    AttentionVector1 = tf.reshape(AttentionVector1temp, [q.shape[0],d_sample, d_v*num_heads]) # restore shape\n",
    "    AttentionVector1time = None\n",
    "    AttentionVector1temp = None\n",
    "    timemask = None\n",
    "\n",
    "# Purely Space-based attention as second of two attentions\n",
    "# Note we don't do Space and Full together so calculated mask is for Space and saves space\n",
    "    if SpacewiseSecondAttention:\n",
    "      # Code below rewrites arrays to have 5 indices exposing space and time separately\n",
    "      qspace = tf.transpose(qtime, perm = [1,0,3,2,4]) # [num_heads, Batch, Tseq, Nloc_sample,  depth]\n",
    "      kspace = tf.transpose(ktime, perm = [1,0,3,2,4]) # [num_heads, Batch, Tseq, Nloc_sample,  depth]\n",
    "      vspace = tf.transpose(vtime, perm = [1,0,3,2,4]) # [num_heads, Batch, Tseq, Nloc_sample,  d_v]\n",
    "      if SeparateHeads:\n",
    "        HeadedAttentionVectorList = []\n",
    "        for ihead in range(0,num_heads):\n",
    "          # [Batch,ihead, Tseq, Nloc_sample, depth] [Batch, ihead,Tseq, (Nloc_sample, depth)T ]\n",
    "          matmul_qk = tf.matmul(qspace[ihead,:,:,:,:], kspace[ihead,:,:,:,:], transpose_b=True)\n",
    "          scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # [Batch, Tseq, Nloc_sample, Nloc_sample] \n",
    "          if mask is not None:\n",
    "            spacemask = tf.reshape(mask,[mask.shape[0],1,Nloc_sample,Nloc_sample])\n",
    "            scaled_attention_logits += (spacemask * -1e9) \n",
    "          attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # scaled_attention_logits [Batch,  Tseq, Nloc_sample,  Nloc_sample] \n",
    "          if mask is not None:\n",
    "            attention_weights = tf.where(spacemask > 0.2, 0.0, attention_weights) \n",
    "          # [Batch,  Tseq, Nloc_sample, Nloc_sample] [Batch,  Tseq, Nloc_sample, depth]\n",
    "          OneheadAttentionVectorspace = tf.matmul(attention_weights, vspace[ihead,:,:,:,:])  \n",
    "          HeadedAttentionVectorList.append(OneheadAttentionVectorspace)# [num_heads,Batch,  Tseq, Nloc_sample,  depth]\n",
    "        AttentionVector2space = tf.convert_to_tensor(HeadedAttentionVectorList)\n",
    "        OneheadAttentionVectorspace = None\n",
    "        HeadedAttentionVectorList = None\n",
    "     \n",
    "      else: # Fully Integrated\n",
    "        matmul_qk = tf.matmul(qspace, kspace, transpose_b=True)  # [num_heads,Batch,  Tseq, Nloc_sample, depth] [num_heads, Batch, Tseq, (Nloc_sample, depth)T ]\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # [num_heads, Batch, Tseq, Nloc_sample, Nloc_sample]\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "          #mask = tf.reshape(mask,[mask.shape[0],Nloc_sample,Tseq,Nloc_sample,Tseq])\n",
    "          #spacemask= mask[:,:,0,:,0]\n",
    "          #spacemask = tf.reshape(spacemask,[mask.shape[0],1,1,Nloc_sample,Nloc_sample])\n",
    "          spacemask = tf.reshape(mask,[1,mask.shape[0],1,Nloc_sample,Nloc_sample])\n",
    "          scaled_attention_logits += (spacemask * -1e9)\n",
    "\n",
    "        # softmax is normalized on the last axis (Nloc_sample) so that the scores add up to 1 in location direction separately for each space.\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # scaled_attention_logits [num_heads, Batch, Tseq, Nloc_sample,  Nloc_sample] \n",
    "        if mask is not None:\n",
    "          attention_weights = tf.where(spacemask > 0.2, 0.0, attention_weights)  \n",
    "\n",
    "        AttentionVector2space = tf.matmul(attention_weights, vspace)\n",
    "  # [num_heads, Batch, Tseq, Nloc_sample,  Nloc_sample] [Batch, num_heads, Tseq, Nloc_sample,  d_v] becomes [Batch, num_heads, Tseq, Nloc_sample,  d_v]\n",
    "      \n",
    "      AttentionVector2temp = tf.transpose(AttentionVector2space, perm = [1,3,2,0,4]) # [Batch,  Nloc_sample, Tseq, num_heads, d_v]\n",
    "      AttentionVector2 = tf.reshape(AttentionVector2temp, [q.shape[0], d_sample, d_v*num_heads ]) # restore shape\n",
    "      AttentionVector2space = None\n",
    "      AttentionVector2temp = None\n",
    "      spacemask = None\n",
    "      qspace = None\n",
    "      kspace = None\n",
    "      vspace = None\n",
    "\n",
    "# Full time-space for second attention\n",
    "    else:\n",
    "      AttentionVector2 = CalculateFullAttention(q,k,v,num_heads,mask)\n",
    "\n",
    "# Combine two attentions\n",
    "    AttentionVector = tf.concat([AttentionVector1,AttentionVector2], -1) # [Batch, d_sample,2*d_v*num_heads] \n",
    "    AttentionVector1 = None\n",
    "    AttentionVector2 = None\n",
    "    qtime = None\n",
    "    ktime = None\n",
    "    vtime = None\n",
    "\n",
    "# Either one-way or two-way attention\n",
    "  scaled_attention_logits = None\n",
    "  attention_weights = None\n",
    "  mask = None\n",
    "  matmul_qk = None\n",
    "  return AttentionVector # [Batch,  d_sample,d_Attention] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MIKxBremkTX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Multi-head attention for Science**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgd3H0AVrueX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Multi-head attention should be identical between Science ad NLP \n",
    "\n",
    "Note assertion that ***num_heads*** divides ***d_model***\n",
    "\n",
    "The annotation has seq_len and similar notations which is ***d_sample***\n",
    "\n",
    "***depth*** is calculated. It is number of words in each instance of Q, K, V for one head. Note that Q, K, V are concatenated over heads for efficient computation\n",
    "\n",
    "We suggest possibility of doing attention not across all d_sample inputs but rather separately in time and in location. This is implemented in \"Scaled Dot Product Attention\"\n",
    "\n",
    "We also allow d_Attention final size to be different from input d_model. Further we put correction of splitting into \"Scaled Dot Product Attention\" as it is naturally combined with other tensor reshape/transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9TpHjS8sHup",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multi-head Attention Discussion and Science version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz5BMC8Kaoqo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPmbr6F1C-v_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSV3PPKsYecw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Active_QKV(tf.keras.layers.Layer):\n",
    "  def __init__(self,d_intermediateqkv, d_finalqkv):\n",
    "    super(Active_QKV, self).__init__()\n",
    "\n",
    "    self.d_intermediateqkv  = d_intermediateqkv\n",
    "    self.d_finalqkv = d_finalqkv\n",
    "\n",
    "    self.FirstDenseqkv =  tf.keras.layers.Dense(self.d_intermediateqkv, activation=EncoderActivation)  # (batch_size, d_qkv, d_intermediateqkv)\n",
    "    self.SecondDenseqkv =  tf.keras.layers.Dense(self.d_finalqkv)  # (batch_size, d_qk, d_finalqkv)\n",
    "    self.TheDropoutqkv =  tf.keras.layers.Dropout(EncoderDropout)\n",
    "\n",
    "  def call(self, qkv, training=None):\n",
    "  \n",
    "    Running = self.FirstDenseqkv(qkv)\n",
    "    Running = self.SecondDenseqkv(Running)  \n",
    "    ComplexQKV = self.TheDropoutqkv(Running,training=training)\n",
    "\n",
    "    return ComplexQKV\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # Feed in d_model, num_heads. Nothing assumed. Other sizes implied by tensors\n",
    "  # seq_len = seq_len_q = seq_len_k= seq_len_v below is d_sample\n",
    "\n",
    "  def __init__(self):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_intermediateqk = d_intermediateqk\n",
    "    self.d_intermediatev = d_intermediatev\n",
    "    self.d_qk = d_qk\n",
    "    self.d_v = d_v\n",
    "\n",
    "    assert self.d_qk % self.num_heads == 0\n",
    "    \n",
    "    self.depth = self.d_qk // self.num_heads\n",
    "    \n",
    "    if not ActivateAttention:\n",
    "      self.wq = tf.keras.layers.Dense(self.d_qk)\n",
    "      self.wk = tf.keras.layers.Dense(self.d_qk)\n",
    "      if not Takevasinput:\n",
    "        self.wv = tf.keras.layers.Dense(self.d_v*self.num_heads)  \n",
    "    if  ActivateAttention:   \n",
    "      if self.d_intermediateqk >0:\n",
    "        self.wq_aa = Active_QKV(self.d_intermediateqk, self.d_qk)\n",
    "        self.wk_aa = Active_QKV(self.d_intermediateqk, self.d_qk)\n",
    "      else:\n",
    "        self.wq_aa = tf.keras.layers.Dense(self.d_qk, activation=EncoderActivation)\n",
    "        self.wk_aa = tf.keras.layers.Dense(self.d_qk, activation=EncoderActivation)\n",
    "      if not Takevasinput:  \n",
    "        if self.d_intermediatev >0:    \n",
    "          self.wv_aa = Active_QKV(self.d_intermediatev, self.d_v)\n",
    "        else:\n",
    "          self.wv_aa = tf.keras.layers.Dense(self.d_v, activation=EncoderActivation)\n",
    "\n",
    "    self.finaldense = tf.keras.layers.Dense(d_Attention)\n",
    "\n",
    "  def summarize(self):\n",
    "    count_tot = self.count_params()\n",
    "    count_v = 0\n",
    "    if  ActivateAttention: \n",
    "      count_q = self.wq_aa.count_params()\n",
    "      count_k = self.wk_aa.count_params()\n",
    "      if not Takevasinput:\n",
    "        count_v = self.wv_aa.count_params()\n",
    "    else:\n",
    "      count_q = self.wq.count_params()\n",
    "      count_k = self.wk.count_params()\n",
    "      if not Takevasinput:\n",
    "        count_v = self.wv.count_params()\n",
    "    count_final = self.finaldense.count_params()\n",
    "    print('MHA Tot ' + str(count_tot) + ' Q K V ' + str(count_q) + ' ' + str(count_k) + ' ' + str(count_v)\n",
    "     + ' Dense ' + str(count_final))\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, Mappedv, mask=None, training=None):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "   \n",
    "    if ActivateAttention:\n",
    "      q = self.wq_aa(q, training=training)  # (batch_size, seq_len, d_qk)\n",
    "      k = self.wk_aa(k, training=training)  # (batch_size, seq_len, d_qk)\n",
    "      if not Takevasinput:\n",
    "        v = self.wv_aa(v, training=training)  # (batch_size, seq_len, d_v)\n",
    "    else:\n",
    "      q = self.wq(q)  # (batch_size, seq_len, d_qk)\n",
    "      k = self.wk(k)  # (batch_size, seq_len, d_qk)\n",
    "      if not Takevasinput:\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_v*self.num_heads)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "\n",
    "    if Takevasinput:\n",
    "      v=[]\n",
    "      for i in range(0,self.num_heads):\n",
    "        v.append(Mappedv)\n",
    "      v = tf.convert_to_tensor(v)\n",
    "      v = tf.reshape(v, (batch_size, -1, self.num_heads, Mappedv.shape[-1]))\n",
    "      v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
    "    else:   \n",
    "      v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth = D_v)\n",
    "\n",
    "# scaled_attention.shape == (batch_size,  seq_len_q, d_Attention)\n",
    "    concat_attention = scaled_dot_product_attention(q, k, v, mask)\n",
    "    output = self.finaldense(concat_attention)  # (batch_size, seq_len_q, d_Attention)\n",
    "  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdDqGayx67vv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Point wise feed forward network** (Original and Science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag3IZpbu5aPe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is used in encoder (each) layer and the decoder.  The activation layer could be 'relu' or 'selu'.\n",
    "\n",
    "dff in code is our parameter ***d_ffn*** (size of first layer in feef forward network) and defaults to 4 * ***d_model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBqzJXGfHK3X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a relu or selu activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET7xLt0yCT6Z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_EncoderLayer, d_ffn):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(d_ffn, activation=EncoderActivation),  # (batch_size, seq_len, d_ffn)\n",
    "      tf.keras.layers.Dense(d_EncoderLayer)  # (batch_size, seq_len, d_EncoderLayer)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Encoder and decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDs43KCW-t9k",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note the process starts with an ***Encoder*** and finishes with a ***Decoder***. These share components like multi-headed attention. We expect to look at different Decoders for science as we want floating point numbers and not as in NLP, members of a vocabulary We expect that decoder could be similar for science and NLP as it is looking for structure and that is a set of relationships which could be similar between science and NLP. ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yScbC0MUH8dS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfYJG-Kvgwy2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFv-FNYUmvpn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encoder layer (Original and Science)\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_EncoderLayer` (last) axis. There are N = ***num_Encoderlayers*** encoder layers in the transformer.\n",
    "\n",
    "\n",
    "I am not clear why x and sublayer(x) (input and output, out1 + ffn_output) are added. I would have concatenated. I had a similar comment on positional encoding which is added to rather than being appended to input. For science the numbers matter -- its not just patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncyS-Ms3i2x_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention()\n",
    "    self.ffn = point_wise_feed_forward_network(d_EncoderLayer, d_ffn)\n",
    "\n",
    "    if oldencoderversion:\n",
    "      self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "      self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(EncoderDropout)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(EncoderDropout)\n",
    " \n",
    "  def summarize(self):\n",
    "    self.mha.summarize()\n",
    "    print(startbold + startpurple + 'point_wise_feed_forward_network Sequential Net' +resetfonts)\n",
    "    self.ffn.summary()\n",
    "\n",
    "  def call(self, x, xmapped, training=None, mask=None):\n",
    "\n",
    "# mha adjusts to shape[-1] of x being d_model or d_EncoderLayer\n",
    "    attn_output = self.mha(x, x, x, xmapped, mask = mask, training=training)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    \n",
    "    if oldencoderversion:\n",
    "      if np.shape(x)[-1] == np.shape(attn_output)[-1]:\n",
    "        addtogether = x + attn_output\n",
    "      else:\n",
    "        doublex = tf.concat([x,x], -1)\n",
    "        if np.shape(doublex)[-1] != np.shape(attn_output)[-1]:\n",
    "          doublex = tf.concat([doublex,doublex], -1)\n",
    "        addtogether = doublex + attn_output \n",
    "      out1 = self.layernorm1(addtogether)  # (batch_size, input_seq_len, d_EncoderLayer)\n",
    "    else:\n",
    "      out1 = attn_output\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_EncoderLayer)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    if oldencoderversion:\n",
    "      out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_EncoderLayer)\n",
    "    else:\n",
    "      out2 = ffn_output\n",
    "\n",
    "    return out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE1H51Ajm0q1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-rBfJg5-YJx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encoder (Science modified Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpEox7gJ8FCI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Encoder, self).__init__()\n",
    "    \n",
    "   # self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.dense_1 = tf.keras.layers.Dense(d_model, activation=EncoderActivation)\n",
    "\n",
    "    self.enc_layers = [EncoderLayer() for _ in range(num_Encoderlayers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(EncoderDropout)\n",
    "        \n",
    "  def call(self, x, training =None, mask = None):\n",
    "  \n",
    "  # adding embedding and position encoding.\n",
    "  # x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "  # x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    xmapped = self.dense_1(x)\n",
    "    xrunning = xmapped \n",
    "\n",
    "    xrunning = self.dropout(xrunning, training=training)\n",
    "    for i in range(num_Encoderlayers):\n",
    "      xrunning = self.enc_layers[i](xrunning, xmapped, training=training, mask=mask)\n",
    "      \n",
    "    return xrunning, xmapped  # (batch_size, d_sample, d_EncoderLayer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_OZW9HM_43X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encoder for Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XkLhZ1dAmXd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The encoder for Science is very close to the NLP version\n",
    "Its output is TWO arrays\n",
    "* The result of self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model) before NLP position encoding applied\n",
    "* The same output of Encoder used in NLP\n",
    "\n",
    "Both are (***Transformerbatch_size***, ***d_sample***, ***d_model***)\n",
    "\n",
    "Note this is analogous to RESNET that adds input to output after several convolutional layers. We maybe incorrectly are concatenating not adding\n",
    "\n",
    "This interpreted as original embedded data which would have been fed into an LSTM in that model plus a second vector summarizing result of attention analysis -- a form of generalized history\n",
    "\n",
    "We will need to convert the ***d_sample*** index to two indices ***Tseq*** * size {[Num_Seq][Nloc]}\n",
    "\n",
    "We can run members of size {[Num_Seq][Nloc]} together in LSTM although this is not how it is done normally in pure LSTM. Alternatively we can run each sample member separately\n",
    "\n",
    "Note each member of sample has a separate encoder output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk2MABcvCkW4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Decoder for Science\n",
    "This is not like the NLP Decoder. Rather it will use the same two layer LSTM we have already tested extensively in COVID.\n",
    "\n",
    "There are two important changes\n",
    "1. In COVID the equivalent of ***d_sample*** held just ***Tseq*** entries -- a single window for one location. Now we feed in a window of length Tseq as in COVID but the input data is muliple cases of size {[Num_Seq][Nloc]}. As we expect to start with one  sequence per network input this is just the number of locations.\n",
    "2. For each presented case, we intend to use not ust the output of encoder but rather the concatenation of two vectors of length ***d_model***\n",
    "\n",
    "* The result of self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model) before NLP position encoding applied\n",
    "* The same output of Encoder used in NLP containing concatenated attention head results\n",
    "\n",
    "There are two possibilities to consider\n",
    "1. That described above with a single input to decoder of length 2 * ***d_model*** * ***d_sample/Tseq***\n",
    "2. ***d_sample/Tseq*** separate runs (in parallel or one after the other) each containing one case. This is safest\" approach as closely mimics that used in COVID\n",
    "\n",
    "Note this LSTM subsystem ends with a small FCN and so we don't need an additional linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWccxYzUqM7R",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformer Model for Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_caU47Rpt3HI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncodertoLSTMmerge(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(EncodertoLSTMmerge, self).__init__()\n",
    "    self.dense_merged = tf.keras.layers.Dense(d_merge, activation=EncoderActivation)\n",
    "\n",
    "  def call(self, Originalinput, EncoderOutput, training=None):\n",
    "\n",
    "    EncoderOutput = tf.reshape(EncoderOutput, [EncoderOutput.shape[0], Nloc_sample, Tseq, d_EncoderLayer])\n",
    "    \n",
    "    if ReuseInputinEncoder:\n",
    "      Originalinput = tf.reshape(Originalinput, [Originalinput.shape[0], Nloc_sample, Tseq, Originalinput.shape[-1]])\n",
    "      Merged = tf.concat([Originalinput,EncoderOutput], -1)\n",
    "    else:\n",
    "      Merged = EncoderOutput\n",
    "    Merged = self.dense_merged(Merged)\n",
    "\n",
    "    return Merged\n",
    "\n",
    "  def build_graph(self, shapes):\n",
    "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvbEjJgxlX6S",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prediction and Visualization Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrTvfdV9B0ss",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###DLprediction2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaflIIrbKjRJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###DLPrediction2E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vuo893BKprz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def DLprediction2E(Xin, yin, DLmodel, modelflag):\n",
    "  # Form restricted Attention separately over Training and Validation\n",
    "\n",
    "  if not LocationBasedValidation:\n",
    "    return\n",
    "  if UsedTransformervalidationfrac < 0.001 or ValidationNloc <= 0:\n",
    "    return\n",
    "  if SkipDL2E:\n",
    "    return\n",
    "  if GarbageCollect:\n",
    "    gc.collect()\n",
    "\n",
    "  SampleSize = 1\n",
    "  FitRanges_PartialAtt = np.zeros([Num_Seq, Nloc, NpredperseqTOT,5], dtype =np.float32)\n",
    "  FRanges = np.full((NpredperseqTOT), 1.0, dtype = np.float32)\n",
    "  # 0 count 1 mean 2 Standard Deviation 3 Min 4 Max\n",
    "\n",
    "  print(wraptotext(startbold+startred+ 'DLPrediction2E Partial Attention ' +current_time + ' ' + RunName + RunComment +  resetfonts))\n",
    "\n",
    "  global  OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
    "\n",
    "  global FullSetValidation\n",
    "  saveFullSetValidation = FullSetValidation\n",
    "  FullSetValidation = False\n",
    "  X_predict, y_predict, Spacetime_predict, X_val, y_val, Spacetime_val = setSeparateDLinput(1, Spacetime = True)\n",
    "  FullSetValidation = saveFullSetValidation\n",
    "\n",
    "  Nloc_sample = TrainingNloc\n",
    "  OuterBatchDimension = Num_Seq\n",
    "  d_sample = Tseq * TrainingNloc\n",
    "  max_d_sample = d_sample\n",
    "  UsedValidationNloc = ValidationNloc\n",
    "\n",
    "  if SymbolicWindows:\n",
    "    X_Transformertraining = np.reshape(X_predict, (OuterBatchDimension, Nloc_sample))\n",
    "  else:\n",
    "    X_Transformertraining = np.reshape(X_predict, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "  y_Transformertraining = np.reshape(y_predict, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "  Spacetime_Transformertraining = np.reshape(Spacetime_predict, (OuterBatchDimension, Nloc_sample))\n",
    "\n",
    "  if SymbolicWindows:\n",
    "    X_Transformerval = np.reshape(X_val, (OuterBatchDimension, UsedValidationNloc))\n",
    "  else:\n",
    "    X_Transformerval = np.reshape(X_val, (OuterBatchDimension, UsedValidationNloc*Tseq, NpropperseqTOT))\n",
    "  y_Transformerval = np.reshape(y_val, (OuterBatchDimension, UsedValidationNloc, NpredperseqTOT))\n",
    "  Spacetime_Transformerval = np.reshape(Spacetime_val, (OuterBatchDimension, UsedValidationNloc))\n",
    "\n",
    "  if UseClassweights:     \n",
    "    sw_Transformertraining = np.empty_like(y_predict, dtype=np.float32)\n",
    "    for i in range(0,sw_Transformertraining.shape[0]):\n",
    "      for j in range(0,sw_Transformertraining.shape[1]):\n",
    "        for k in range(0,NpredperseqTOT):\n",
    "          sw_Transformertraining[i,j,k] = Predictionwgt[k]\n",
    "    sw_Transformerval = np.empty_like(y_val, dtype=np.float32)\n",
    "    for i in range(0,sw_Transformerval.shape[0]):\n",
    "      for jloc in range(0,sw_Transformerval.shape[1]):\n",
    "        for k in range(0,NpredperseqTOT):\n",
    "          sw_Transformerval[i,jloc,k] = Predictionwgt[k]  \n",
    "  else:\n",
    "    sw_Transformertraining = []\n",
    "    sw_Transformerval = []\n",
    "\n",
    "  if SymbolicWindows:\n",
    "    X_Transformertrainingflat2 = np.reshape(X_Transformertraining, (-1, TrainingNloc))\n",
    "    X_Transformertrainingflat1 = np.reshape(X_Transformertrainingflat2, (-1))\n",
    "  else:\n",
    "    X_Transformertrainingflat2 = np.reshape(X_Transformertraining, (-1, TrainingNloc,Tseq, NpropperseqTOT))\n",
    "    X_Transformertrainingflat1 = np.reshape(X_Transformertrainingflat2, (-1, Tseq, NpropperseqTOT))\n",
    "  y_Transformertrainingflat1 = np.reshape(y_Transformertraining, (-1,NpredperseqTOT) )\n",
    "  Spacetime_Transformertrainingflat1 = np.reshape(Spacetime_Transformertraining,(-1))   \n",
    "  if UseClassweights: \n",
    "    sw_Transformertrainingflat1 = np.reshape(sw_Transformertraining, (-1,NpredperseqTOT) )\n",
    "  if SymbolicWindows:\n",
    "    X_Transformervalflat2 = np.reshape(X_Transformerval, (-1, UsedValidationNloc))\n",
    "    X_Transformervalflat1 = np.reshape(X_Transformervalflat2, (-1))\n",
    "  else:\n",
    "    X_Transformervalflat2 = np.reshape(X_Transformerval, (-1, UsedValidationNloc,Tseq, NpropperseqTOT))\n",
    "    X_Transformervalflat1 = np.reshape(X_Transformervalflat2, (-1, Tseq, NpropperseqTOT))\n",
    "  y_Transformervalflat1 = np.reshape(y_Transformerval, (-1,NpredperseqTOT) )\n",
    "  Spacetime_Transformervalflat1 = np.reshape(Spacetime_Transformerval,(-1))\n",
    "  if UseClassweights: \n",
    "    sw_Transformervalflat1 = np.reshape(sw_Transformerval, (-1,NpredperseqTOT) )\n",
    "\n",
    "  meanvalue2 = 0.0\n",
    "  meanvalue3 = 0.0\n",
    "  meanvalue4 = 0.0\n",
    "  variance2= 0.0\n",
    "  variance3= 0.0\n",
    "  variance4= 0.0\n",
    "\n",
    "# START LOOP OVER SAMPLES\n",
    "  samplebar = notebook.trange(SampleSize,  desc='Full Samples', unit  = 'sample')\n",
    "  epochsize = 2*OuterBatchDimension\n",
    "  if IncreaseNloc_sample > 1:\n",
    "    epochsize = int(epochsize/IncreaseNloc_sample)\n",
    "  elif DecreaseNloc_sample > 1:\n",
    "    epochsize = int(epochsize*DecreaseNloc_sample)\n",
    "  bbar = notebook.trange(epochsize,  desc='Batch    loop', unit  = 'sample')\n",
    "  for shuffling in range (0,SampleSize):\n",
    "    if GarbageCollect:\n",
    "      gc.collect()\n",
    "\n",
    "# TRAINING SET\n",
    "    if TimeShufflingOnly:\n",
    "      X_train, y_train, sw_train, Spacetime_train = shuffleDLinput(X_Transformertraining, \n",
    "            y_Transformertraining, sw_Transformertraining, Spacetime = Spacetime_Transformertraining)\n",
    "    else:\n",
    "      X_train, y_train, sw_train, Spacetime_train = shuffleDLinput(X_Transformertrainingflat1, \n",
    "            y_Transformertrainingflat1, sw_Transformertrainingflat1, Spacetime = Spacetime_Transformertrainingflat1)  \n",
    "\n",
    "    Nloc_sample = TrainingNloc\n",
    "    OuterBatchDimension = Num_Seq       \n",
    "    Totaltodo = Nloc_sample*OuterBatchDimension\n",
    "    if IncreaseNloc_sample > 1:\n",
    "      Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
    "    elif DecreaseNloc_sample > 1:\n",
    "      Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
    "    OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
    "    if OuterBatchDimension * Nloc_sample != Totaltodo:\n",
    "      printexit('Inconsistent Nloc_sample ' + str(Nloc_sample))\n",
    "    d_sample = Tseq * Nloc_sample\n",
    "    max_d_sample = d_sample\n",
    "\n",
    "    if SymbolicWindows:\n",
    "      X_train = np.reshape(X_train, (OuterBatchDimension, Nloc_sample))\n",
    "    else:\n",
    "      X_train = np.reshape(X_train, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "    y_train = np.reshape(y_train, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "    sw_train = np.reshape(sw_train, (OuterBatchDimension, Nloc_sample, NpredperseqTOT)) \n",
    "    Spacetime_train = np.reshape(Spacetime_train, (OuterBatchDimension, Nloc_sample))\n",
    "\n",
    "    quan3 = 0.0\n",
    "    quan4 = 0.0\n",
    "    losspercallVl = 0.0\n",
    "    losspercallTr = 0.0\n",
    "    TotalTr = 0.0\n",
    "    TotalVl = 0.0\n",
    "    for Trainingindex in range(0, OuterBatchDimension):\n",
    "      if GarbageCollect:\n",
    "        gc.collect()\n",
    "      X_trainlocal = X_train[Trainingindex] \n",
    "      if SymbolicWindows:\n",
    "        X_trainlocal = np.reshape(X_trainlocal,[1,X_trainlocal.shape[0]])\n",
    "      else:\n",
    "        X_trainlocal = np.reshape(X_trainlocal,[1,X_trainlocal.shape[0],X_trainlocal.shape[1]])\n",
    "      \n",
    "      Numinbatch = X_trainlocal.shape[0]\n",
    "      NuminAttention = X_trainlocal.shape[1]\n",
    "      NumTOTAL = Numinbatch*NuminAttention \n",
    "      # SymbolicWindows X_train is indexed by Batch index, Location List for Attention. Missing 1(replace by Window), 1 (replace by properties)\n",
    "      if SymbolicWindows:         \n",
    "        X_trainlocal = np.reshape(X_trainlocal,NumTOTAL)\n",
    "        iseqarray = np.right_shift(X_trainlocal,16)\n",
    "        ilocarray = np.bitwise_and(X_trainlocal, 0b1111111111111111)\n",
    "        X_train_withSeq = list()\n",
    "        for iloc in range(0,NumTOTAL):\n",
    "          X_train_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "        X_train_withSeq = np.array(X_train_withSeq)\n",
    "        X_train_withSeq = np.reshape(X_train_withSeq,(Numinbatch, d_sample, NpropperseqTOT))\n",
    "        Time = None\n",
    "        if modelflag==1: \n",
    "          Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
    "        PredictedVector = DLmodel(X_train_withSeq, training = PredictionTraining, Time=Time )\n",
    "      else:\n",
    "        Spacetime_trainlocal = Spacetime_train[Trainingindex]\n",
    "        iseqarray = np.right_shift(Spacetime_trainlocal,16)\n",
    "        ilocarray = np.bitwise_and(Spacetime_trainlocal, 0b1111111111111111)\n",
    "        Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
    "        PredictedVector = DLmodel(X_trainlocal, training = PredictionTraining, Time=Time )\n",
    "      PredictedVector = np.reshape(PredictedVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "      TrueVector = y_train[Trainingindex]\n",
    "      TrueVector = np.reshape(TrueVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "      sw_trainlocal = sw_train[Trainingindex] \n",
    "      sw_trainlocal = np.reshape(sw_trainlocal,[1,sw_trainlocal.shape[0],sw_trainlocal.shape[1]])\n",
    "      losspercallTr = numpycustom_lossGCF1(TrueVector,PredictedVector,sw_trainlocal)\n",
    "      quan3 += losspercallTr\n",
    "      \n",
    "      for iloc_sample in range(0,Nloc_sample):\n",
    "        LocLocal = ilocarray[iloc_sample]\n",
    "        SeqLocal = iseqarray[iloc_sample]\n",
    "        yyhat = PredictedVector[0,iloc_sample]\n",
    "        if (FitRanges_PartialAtt [SeqLocal,LocLocal,0,0] < 0.1):\n",
    "            FitRanges_PartialAtt [SeqLocal,LocLocal,:,3] = yyhat\n",
    "            FitRanges_PartialAtt [SeqLocal,LocLocal,:,4] = yyhat\n",
    "        else:\n",
    "          FitRanges_PartialAtt [SeqLocal,LocLocal,:,3] = np.maximum(FitRanges_PartialAtt[SeqLocal,LocLocal,:,3],yyhat)\n",
    "          FitRanges_PartialAtt [SeqLocal,LocLocal,:,4] = np.minimum(FitRanges_PartialAtt[SeqLocal,LocLocal,:,4],yyhat)\n",
    "        FitRanges_PartialAtt [SeqLocal,LocLocal,:,0] += FRanges\n",
    "        FitRanges_PartialAtt[SeqLocal,LocLocal,:,1] += yyhat\n",
    "        FitRanges_PartialAtt[SeqLocal,LocLocal,:,2] += np.square(yyhat)\n",
    "\n",
    "      fudge = 1.0/(1+Trainingindex)\n",
    "      TotalTr = quan3 *fudge\n",
    "      bbar.set_postfix(TotalTr = TotalTr, Tr = losspercallTr)\n",
    "      bbar.update(Transformerbatch_size)\n",
    "# END Training Batch Loop\n",
    "    TotalTr= quan3/OuterBatchDimension\n",
    "\n",
    "# VALIDATION SET\n",
    "    Nloc_sample = UsedValidationNloc\n",
    "    OuterBatchDimension = Num_Seq  \n",
    "    Totaltodo = Nloc_sample*OuterBatchDimension\n",
    "    if IncreaseNloc_sample > 1:\n",
    "      Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
    "    elif DecreaseNloc_sample > 1:\n",
    "      Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
    "    OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
    "    if OuterBatchDimension * Nloc_sample != Totaltodo:\n",
    "      printexit('Inconsistent Nloc_sample ' + str(Nloc_sample))\n",
    "    d_sample = Tseq * Nloc_sample\n",
    "    max_d_sample = d_sample        \n",
    "\n",
    "    if TimeShufflingOnly:\n",
    "      X_val, y_val, sw_val, Spacetime_val = shuffleDLinput(\n",
    "          X_Transformerval, y_Transformerval, sw_Transformerval, Spacetime_Transformerval)\n",
    "    else:\n",
    "      X_val, y_val, sw_val, Spacetime_val = shuffleDLinput(\n",
    "          X_Transformervalflat1, y_Transformervalflat1, sw_Transformervalflat1, Spacetime_Transformervalflat1)\n",
    "      if SymbolicWindows:\n",
    "        X_val = np.reshape(X_val, (OuterBatchDimension, Nloc_sample))\n",
    "      else:\n",
    "        X_val = np.reshape(X_val, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "    y_val = np.reshape(y_val, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "    sw_val = np.reshape(sw_val, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "    Spacetime_val = np.reshape(Spacetime_val, (OuterBatchDimension, Nloc_sample))\n",
    "\n",
    "# START VALIDATION Batch Loop\n",
    "    for Validationindex in range(0,OuterBatchDimension):\n",
    "      X_valbatch = X_val[Validationindex]\n",
    "      y_valbatch = y_val[Validationindex]\n",
    "      sw_valbatch = sw_val[Validationindex]\n",
    "      Spacetime_valbatch = Spacetime_val[Validationindex]\n",
    "      if SymbolicWindows:\n",
    "        X_valbatch = np.reshape(X_valbatch,[1,X_valbatch.shape[0]])\n",
    "      else:\n",
    "        X_valbatch = np.reshape(X_valbatch,[1,X_valbatch.shape[0],X_valbatch.shape[1]])\n",
    "      y_valbatch = np.reshape(y_valbatch,[1,y_valbatch.shape[0],y_valbatch.shape[1]])\n",
    "      sw_valbatch = np.reshape(sw_valbatch,[1,sw_valbatch.shape[0],sw_valbatch.shape[1]])\n",
    "      Numinbatch = X_valbatch.shape[0]\n",
    "      NuminAttention = X_valbatch.shape[1]\n",
    "      NumTOTAL = Numinbatch*NuminAttention\n",
    "\n",
    "      if SymbolicWindows:          \n",
    "        X_valbatch = np.reshape(X_valbatch,NumTOTAL)\n",
    "        iseqarray = np.right_shift(X_valbatch,16)\n",
    "        ilocarray = np.bitwise_and(X_valbatch, 0b1111111111111111)\n",
    "        X_valbatch_withSeq = list()\n",
    "        for iloc in range(0,NumTOTAL):\n",
    "          X_valbatch_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "        X_valbatch_withSeq = np.array(X_valbatch_withSeq)\n",
    "        X_valbatch_withSeq = np.reshape(X_valbatch_withSeq,(Numinbatch, d_sample, NpropperseqTOT))\n",
    "        Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
    "        PredictedVector = DLmodel(X_valbatch_withSeq, training = PredictionTraining, Time=Time )\n",
    "      else:\n",
    "        Spacetime_valbatch = np.reshape(Spacetime_valbatch,-1)\n",
    "        iseqarray = np.right_shift(Spacetime_valbatch,16)\n",
    "        ilocarray = np.bitwise_and(Spacetime_valbatch, 0b1111111111111111)\n",
    "        Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
    "        PredictedVector = DLmodel(X_valbatch, training = PredictionTraining, Time=Time )        \n",
    "      PredictedVector = np.reshape(PredictedVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "      TrueVector = np.reshape(y_valbatch,(1,Nloc_sample,NpredperseqTOT))\n",
    "      sw_valbatch = np.reshape(sw_valbatch,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "      losspercallVl = numpycustom_lossGCF1(TrueVector,PredictedVector,sw_valbatch)\n",
    "      quan4 += losspercallVl\n",
    "      \n",
    "      for iloc_sample in range(0,Nloc_sample):\n",
    "        LocLocal = ilocarray[iloc_sample]\n",
    "        SeqLocal = iseqarray[iloc_sample]\n",
    "        yyhat = PredictedVector[0,iloc_sample]\n",
    "        if (FitRanges_PartialAtt [SeqLocal,LocLocal,0,0] < 0.1):\n",
    "            FitRanges_PartialAtt [SeqLocal,LocLocal,:,3] = yyhat\n",
    "            FitRanges_PartialAtt [SeqLocal,LocLocal,:,4] = yyhat\n",
    "        else:\n",
    "          FitRanges_PartialAtt [SeqLocal,LocLocal,:,3] = np.maximum(FitRanges_PartialAtt[SeqLocal,LocLocal,:,3],yyhat)\n",
    "          FitRanges_PartialAtt [SeqLocal,LocLocal,:,4] = np.minimum(FitRanges_PartialAtt[SeqLocal,LocLocal,:,4],yyhat)\n",
    "        FitRanges_PartialAtt [SeqLocal,LocLocal,:,0] += FRanges\n",
    "        FitRanges_PartialAtt[SeqLocal,LocLocal,:,1] += yyhat\n",
    "        FitRanges_PartialAtt[SeqLocal,LocLocal,:,2] += np.square(yyhat)\n",
    "      TotalVl = quan4/(1+Validationindex)\n",
    "      losspercall = (TotalTr*TrainingNloc+TotalVl*ValidationNloc)/Nloc\n",
    "      bbar.update(Transformerbatch_size)\n",
    "      bbar.set_postfix(Loss = losspercall, TotalTr = TotalTr, TotalVl= TotalVl, Vl = losspercallVl)\n",
    "# END VALIDATION BATCH LOOP\n",
    "\n",
    "# Processing at the end of Sampling Loop\n",
    "    fudge = 1.0/OuterBatchDimension\n",
    "    quan2 = (quan3*TrainingNloc + quan4*ValidationNloc)/Nloc\n",
    "    quan2 *= fudge\n",
    "    meanvalue2 += quan2\n",
    "    variance2 += quan2**2\n",
    "    if LocationBasedValidation:\n",
    "      quan3 *= fudge\n",
    "      quan4 *= fudge\n",
    "      meanvalue3 += quan3\n",
    "      meanvalue4 += quan4 \n",
    "      variance3 += quan3**2\n",
    "      variance4 += quan4**2       \n",
    "    samplebar.update(1)\n",
    "    if LocationBasedValidation:\n",
    "      samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Tr = quan3, Val = quan4)\n",
    "    else:\n",
    "      samplebar.set_postfix(Shuffle=shuffling, Loss = quan2)\n",
    "    bbar.reset()\n",
    "# End Shuffling loop\n",
    "\n",
    "  printloss(' Full Loss ',meanvalue2,variance2,SampleSize)\n",
    "  printloss(' Training Loss ',meanvalue3,variance3,SampleSize)\n",
    "  printloss(' Validation Loss ',meanvalue4,variance4,SampleSize)\n",
    "  global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
    "  GlobalLoss = meanvalue2\n",
    "  GlobalTrainingLoss = meanvalue3\n",
    "  GlobalValidationLoss = meanvalue4\n",
    "\n",
    "  FitRanges_PartialAtt[:,:,:,1] = np.divide(FitRanges_PartialAtt[:,:,:,1],FitRanges_PartialAtt[:,:,:,0])\n",
    "  FitRanges_PartialAtt[:,:,:,2] = np.sqrt(np.maximum(np.divide(FitRanges_PartialAtt[:,:,:,2],FitRanges_PartialAtt[:,:,:,0]) -\n",
    "                                np.square(FitRanges_PartialAtt[:,:,:,1]), 0.0)) \n",
    "  FitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "  for iseq in range(0,Num_Seq):\n",
    "    for iloc in range(0,Nloc):    \n",
    "      FitPredictions[iseq,iloc,:] = FitRanges_PartialAtt[iseq,iloc,:,1]\n",
    "  DLprediction3(yin, FitPredictions, ' Separate Attention mean values')\n",
    "  FindNNSE(yin, FitPredictions, Label='Separate Attention' )\n",
    "    \n",
    "  print(startbold+startred+ 'END DLPrediction2E ' +current_time + ' ' + RunName + RunComment +resetfonts) \n",
    "  return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqJ3FkIpWebd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def DLprediction2D(Xin, yin, DLmodel):\n",
    "  # Only runs in Transformer mode\n",
    "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
    "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
    "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
    "  # Label Array is always [Num_Seq][Nloc] [0=Window(first sequence)#, 1=Location]\n",
    "\n",
    "  DLprediction2E(Xin, yin, DLmodel,1)\n",
    "  if SkipDL2D:\n",
    "    return\n",
    "  if GarbageCollect:\n",
    "    gc.collect()\n",
    "  global  OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
    "\n",
    "  SampleSize = 1\n",
    "\n",
    "  FitRanges_FullAtt = np.zeros([Num_Seq, Nloc, NpredperseqTOT,5], dtype =np.float32)\n",
    "  FRanges = np.full((NpredperseqTOT), 1.0, dtype = np.float32)\n",
    "  # 0 count 1 mean 2 Standard Deviation 3 Min 4 Max\n",
    "\n",
    "  print(wraptotext(startbold+startred+ 'DLPrediction2D ' +current_time + ' ' + RunName + RunComment +  resetfonts))\n",
    "\n",
    "  sw = np.empty_like(yin, dtype=np.float32)\n",
    "  for i in range(0,sw.shape[0]):\n",
    "    for j in range(0,sw.shape[1]):\n",
    "      for k in range(0,NpredperseqTOT):\n",
    "        sw[i,j,k] = Predictionwgt[k] \n",
    "  labelarray =np.empty([Num_Seq, Nloc, 2], dtype = np.int32)\n",
    "  for iseq in range(0, Num_Seq):\n",
    "    for iloc in range(0,Nloc):\n",
    "      labelarray[iseq,iloc,0] = iseq\n",
    "      labelarray[iseq,iloc,1] = iloc\n",
    "\n",
    "  Totaltodo = Num_Seq*Nloc\n",
    "  Nloc_sample = Nloc # default\n",
    "\n",
    "  if IncreaseNloc_sample > 1:\n",
    "    Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
    "  elif DecreaseNloc_sample > 1:\n",
    "    Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
    "\n",
    "  if Totaltodo%Nloc_sample != 0:\n",
    "    printexit('Invalid Nloc_sample ' + str(Nloc_sample) + \" \" + str(Totaltodo))\n",
    "  d_sample = Tseq * Nloc_sample        \n",
    "  max_d_sample = d_sample\n",
    "  OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
    "  print(' Predict with ' +str(Nloc_sample) + ' sequences per sample and batch size ' + str(OuterBatchDimension))\n",
    "\n",
    "  meanvalue2 = 0.0\n",
    "  meanvalue3 = 0.0\n",
    "  meanvalue4 = 0.0\n",
    "  variance2= 0.0\n",
    "  variance3= 0.0\n",
    "  variance4= 0.0\n",
    "\n",
    "  samplebar = notebook.trange(SampleSize,  desc='Full Samples', unit  = 'sample')\n",
    "  bbar = notebook.trange(OuterBatchDimension,  desc='Batch    loop', unit  = 'sample')\n",
    "  for shuffling in range (0,SampleSize):\n",
    "    if GarbageCollect:\n",
    "      gc.collect()\n",
    "    startvalue = 0\n",
    "    endvalue = Num_Seq\n",
    "    Xuse = Xin[startvalue:endvalue]\n",
    "    yuse = yin[startvalue:endvalue]\n",
    "    labeluse = labelarray[startvalue:endvalue]\n",
    "    y2= np.reshape(yuse, (-1, NpredperseqTOT)).copy()\n",
    "    labelarray2 = np.reshape(labeluse, (-1,2))\n",
    "\n",
    "    if SymbolicWindows:\n",
    "      # Xin X2 X3 not used rather ReshapedSequencesTOT\n",
    "      labelarray2, y2 = shuffleDLinput(labelarray2, y2)\n",
    "    else:\n",
    "      X2 = np.reshape(Xuse, (-1, Tseq, NpropperseqTOT)).copy()\n",
    "      X2, y2, labelarray2 = shuffleDLinput(X2, y2,labelarray2)\n",
    "      X3 = np.reshape(X2, (-1, d_sample, NpropperseqTOT))\n",
    "    y3 = np.reshape(y2, (-1, Nloc_sample, NpredperseqTOT))\n",
    "    sw = np.reshape(sw, (-1, Nloc_sample, NpredperseqTOT))\n",
    "    labelarray3 = np.reshape(labelarray2, (-1, Nloc_sample, 2))\n",
    "\n",
    "    quan2 = 0.0\n",
    "    quan3 = 0.0\n",
    "    quan4 = 0.0\n",
    "    for Batchindex in range(0, OuterBatchDimension):\n",
    "      if GarbageCollect:\n",
    "        gc.collect()\n",
    "\n",
    "      if SymbolicWindows:\n",
    "        X3local = list()\n",
    "        for iloc_sample in range(0,Nloc_sample):\n",
    "          LocLocal = labelarray3[Batchindex, iloc_sample,1]\n",
    "          SeqLocal = labelarray3[Batchindex, iloc_sample,0]\n",
    "          X3local.append(ReshapedSequencesTOT[LocLocal,SeqLocal:SeqLocal+Tseq])\n",
    "        InputVector = np.array(X3local)\n",
    "      else:\n",
    "        InputVector = X3[Batchindex]\n",
    "\n",
    "      Labelsused = labelarray3[Batchindex]\n",
    "      Time = SetSpacetime(np.reshape(Labelsused[:,0],(1,-1)))\n",
    "      InputVector = np.reshape(InputVector,(1,Tseq*Nloc_sample,NpropperseqTOT))\n",
    "      PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time )\n",
    "      PredictedVector = np.reshape(PredictedVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "      swbatched = sw[Batchindex,:,:]\n",
    "      if LocationBasedValidation:\n",
    "        swT = np.zeros([1,Nloc_sample,NpredperseqTOT],dtype = np.float32)\n",
    "        swV = np.zeros([1,Nloc_sample,NpredperseqTOT],dtype = np.float32)\n",
    "        for iloc_sample in range(0,Nloc_sample):\n",
    "          fudgeT = Nloc/TrainingNloc\n",
    "          fudgeV = Nloc/ValidationNloc\n",
    "          iloc = Labelsused[iloc_sample,1]\n",
    "          if MappingtoTraining[iloc] >= 0:\n",
    "            swT[0,iloc_sample,:] = swbatched[iloc_sample,:]*fudgeT\n",
    "          else:\n",
    "            swV[0,iloc_sample,:] = swbatched[iloc_sample,:]*fudgeV\n",
    "      TrueVector = y3[Batchindex]\n",
    "      TrueVector = np.reshape(TrueVector,(1,Nloc_sample,NpredperseqTOT))\n",
    "      swbatched = np.reshape(swbatched,(1,Nloc_sample,NpredperseqTOT))\n",
    "\n",
    "      losspercall = numpycustom_lossGCF1(TrueVector,PredictedVector,swbatched)\n",
    "      quan2 += losspercall\n",
    "      bbar.update(1)\n",
    "      if LocationBasedValidation:\n",
    "        losspercallTr = numpycustom_lossGCF1(TrueVector,PredictedVector,swT)\n",
    "        quan3 += losspercallTr\n",
    "        losspercallVl = numpycustom_lossGCF1(TrueVector,PredictedVector,swV)\n",
    "        quan4 += losspercallVl\n",
    "      \n",
    "      for iloc_sample in range(0,Nloc_sample):\n",
    "        LocLocal = Labelsused[iloc_sample,1]\n",
    "        SeqLocal = Labelsused[iloc_sample,0]\n",
    "        yyhat = PredictedVector[0,iloc_sample]\n",
    "        if (FitRanges_FullAtt [SeqLocal,LocLocal,0,0] < 0.1):\n",
    "            FitRanges_FullAtt [SeqLocal,LocLocal,:,3] = yyhat\n",
    "            FitRanges_FullAtt [SeqLocal,LocLocal,:,4] = yyhat\n",
    "        else:\n",
    "          FitRanges_FullAtt [SeqLocal,LocLocal,:,3] = np.maximum(FitRanges_FullAtt [SeqLocal,LocLocal,:,3],yyhat)\n",
    "          FitRanges_FullAtt [SeqLocal,LocLocal,:,4] = np.minimum(FitRanges_FullAtt [SeqLocal,LocLocal,:,4],yyhat)\n",
    "        FitRanges_FullAtt [SeqLocal,LocLocal,:,0] += FRanges\n",
    "        FitRanges_FullAtt [SeqLocal,LocLocal,:,1] += yyhat\n",
    "        FitRanges_FullAtt [SeqLocal,LocLocal,:,2] += np.square(yyhat)\n",
    "\n",
    "      fudge = 1.0/(1.0 + Batchindex)\n",
    "      mean2 = quan2 * fudge \n",
    "      if LocationBasedValidation:\n",
    "        mean3 = quan3 * fudge\n",
    "        mean4 = quan4 * fudge\n",
    "        bbar.set_postfix(AvLoss = mean2, AvTr = mean3, AvVl = mean4, Loss = losspercall, Tr = losspercallTr, Vl = losspercallVl)\n",
    "      else:\n",
    "        bbar.set_postfix(Loss = losspercall, AvLoss = mean2 ) \n",
    "\n",
    "# Processing at the end of Sampling Loop\n",
    "    fudge = 1.0/OuterBatchDimension\n",
    "    quan2 *= fudge\n",
    "    quan3 *= fudge\n",
    "    quan4 *= fudge\n",
    "    meanvalue2 += quan2\n",
    "    variance2 += quan2**2\n",
    "    variance3 += quan3**2\n",
    "    variance4 += quan4**2\n",
    "    if LocationBasedValidation:\n",
    "      meanvalue3 += quan3\n",
    "      meanvalue4 += quan4        \n",
    "    samplebar.update(1)\n",
    "    if LocationBasedValidation:\n",
    "      samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Tr = quan3, Val = quan4)\n",
    "    else:\n",
    "      samplebar.set_postfix(Shuffle=shuffling, Loss = quan2)\n",
    "    bbar.reset()\n",
    "# End Shuffling loop\n",
    "\n",
    "  meanvalue2 /= SampleSize \n",
    "\n",
    "  global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
    "  printloss(' Full Loss ',meanvalue2,variance2,SampleSize)\n",
    "  meanvalue2 /= SampleSize\n",
    "  GlobalLoss = meanvalue2\n",
    "  GlobalTrainingLoss = 0.0\n",
    "  GlobalValidationLoss = 0.0\n",
    "  \n",
    "  if LocationBasedValidation:\n",
    "    printloss(' Training Loss ',meanvalue3,variance3,SampleSize)\n",
    "    printloss(' Validation Loss ',meanvalue4,variance4,SampleSize)\n",
    "    meanvalue3 /= SampleSize\n",
    "    meanvalue4 /= SampleSize\n",
    "    GlobalTrainingLoss = meanvalue3\n",
    "    GlobalValidationLoss = meanvalue4\n",
    "\n",
    "  FitRanges_FullAtt[:,:,:,1] = np.divide(FitRanges_FullAtt[:,:,:,1],FitRanges_FullAtt[:,:,:,0])\n",
    "  FitRanges_FullAtt[:,:,:,2] = np.sqrt(np.maximum(np.divide(FitRanges_FullAtt[:,:,:,2],FitRanges_FullAtt[:,:,:,0]) -\n",
    "                                np.square(FitRanges_FullAtt[:,:,:,1]), 0.0)) \n",
    "  FitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "  for iseq in range(0,Num_Seq):\n",
    "    for iloc in range(0,Nloc):    \n",
    "      FitPredictions[iseq,iloc,:] = FitRanges_FullAtt[iseq,iloc,:,1]\n",
    "  DLprediction3(yin, FitPredictions, ' Full Attention mean values')\n",
    "  FindNNSE(yin, FitPredictions, Label='Full Attention' )\n",
    "    \n",
    "  print(startbold+startred+ 'END DLPrediction2D ' +current_time + ' ' + RunName + RunComment +resetfonts) \n",
    "  return \n",
    "\n",
    "def printloss(name,mean,var,SampleSize, lineend =''):\n",
    "  mean /= SampleSize\n",
    "  var /= SampleSize\n",
    "  std = math.sqrt(var - mean**2)\n",
    "  print(name + ' Mean ' + str(round(mean,5)) + ' Std Deviation ' + str(round(std,7)) + ' ' + lineend)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbNmfCkQB_z6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###DLprediction2 numpysimplepartloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tL3_CraKsBO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def numpysimplepartloss(y_actual, y_pred):\n",
    "\n",
    "    flagGCF = np.isnan(y_actual)\n",
    "    y_actual1 = y_actual[np.logical_not(flagGCF)]\n",
    "    y_pred1 = y_pred[np.logical_not(flagGCF)]\n",
    "    loss = np.sum(np.square(y_actual1-y_pred1))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def DLprediction2(Xin, yin, DLmodel):\n",
    "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
    "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
    "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
    "  # Label Array is always [Num_Seq][Nloc] [0=Window(first sequence)#, 1=Location]\n",
    "\n",
    "  # Calculate predictions when data selected in time windows of various sizes\n",
    "    if SkipDL2:\n",
    "      return\n",
    "    \n",
    "    if GarbageCollect:\n",
    "      gc.collect()\n",
    "\n",
    "    TimeScope  = [1,2,5,10,25,50,math.floor(Num_Seq/2),Num_Seq]\n",
    "    NumberScopes = len(TimeScope)\n",
    "    Timebest = []\n",
    "    Timemean = []\n",
    "    Timestd = []\n",
    "    SampleSize = 1000\n",
    "     \n",
    "    print(wraptotext(startbold+startred+ 'DLPrediction2 ' +current_time + ' ' + RunName + RunComment + ' ' + str(NumberScopes) + ' Time Scopes ' +resetfonts))\n",
    "    sw = np.empty([Nloc,NpredperseqTOT],dtype = np.float32)\n",
    "    for iloc in range(0,Nloc):\n",
    "      for k in range(0,NpredperseqTOT):\n",
    "        sw[iloc,k] = Predictionwgt[k]\n",
    "    labelarray =np.empty([Num_Seq, Nloc, 2], dtype = np.int32)\n",
    "    for iseq in range(0, Num_Seq):\n",
    "      for iloc in range(0,Nloc):\n",
    "        labelarray[iseq,iloc,0] = iseq\n",
    "        labelarray[iseq,iloc,1] = iloc\n",
    "\n",
    "    global  OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
    "    OuterBatchDimension = Num_Seq\n",
    "    Nloc_sample = Nloc\n",
    "    d_sample = Tseq * Nloc        \n",
    "    max_d_sample = d_sample\n",
    "\n",
    "# LOOP OVER TIME SCOPES\n",
    "    for TimeScopeLoop in range(0,NumberScopes):\n",
    "      localtimeinterval = TimeScope[TimeScopeLoop]\n",
    "      bestvalue2 = 0.0\n",
    "      meanvalue2 = 0.0\n",
    "      variance2= 0.0\n",
    "      gatherhist2 =[]\n",
    "      gatherhist3 =[]\n",
    "      gatherhist4 =[]\n",
    "      meanvalue3 = 0.0\n",
    "      meanvalue4 = 0.0\n",
    "      np.random.seed(int.from_bytes(os.urandom(4), byteorder='little'))\n",
    "      possible = Num_Seq - localtimeinterval +1\n",
    "\n",
    "      samplebar = notebook.trange(SampleSize,  desc='Time scope interval ' + str(localtimeinterval), unit  = 'sample')\n",
    "      for shuffling in range (0,SampleSize):\n",
    "        if GarbageCollect:\n",
    "          gc.collect()\n",
    "        startvalue = np.random.randint(possible) # Selected randomly\n",
    "        endvalue = startvalue + localtimeinterval\n",
    "        Xuse = Xin[startvalue:endvalue]\n",
    "        yuse = yin[startvalue:endvalue]\n",
    "        labeluse = labelarray[startvalue:endvalue]\n",
    "        y2= np.reshape(yuse, (-1, NpredperseqTOT)).copy()\n",
    "        labelarray2 = np.reshape(labeluse, (-1,2))\n",
    "\n",
    "        if SymbolicWindows:\n",
    "          # Xin X2 X3 not used rather ReshapedSequencesTOT\n",
    "          labelarray2, y2 = shuffleDLinput(labelarray2, y2)\n",
    "        else:\n",
    "          X2 = np.reshape(Xuse, (-1, Tseq, NpropperseqTOT)).copy()\n",
    "          X2, y2, labelarray2 = shuffleDLinput(X2, y2,labelarray2)\n",
    "          X3 = np.reshape(X2, (-1, d_sample, NpropperseqTOT))\n",
    "        y3 = np.reshape(y2, (-1, Nloc_sample, NpredperseqTOT))\n",
    "        labelarray3 = np.reshape(labelarray2, (-1, Nloc_sample, 2))\n",
    "\n",
    "        quan2 = 0.0\n",
    "        quan3 = 0.0\n",
    "        quan4 = 0.0\n",
    "        ct3 = 0.0\n",
    "        ct4 = 0.0\n",
    "        Batchsizeused = 1\n",
    "        \n",
    "        for Batchindex in range(0, Batchsizeused):\n",
    "          if SymbolicWindows:\n",
    "            X3local = list()\n",
    "            for iloc_sample in range(0,Nloc_sample):\n",
    "              LocLocal = labelarray3[Batchindex, iloc_sample,1]\n",
    "              SeqLocal = labelarray3[Batchindex, iloc_sample,0]\n",
    "              X3local.append(ReshapedSequencesTOT[LocLocal,SeqLocal:SeqLocal+Tseq])\n",
    "            InputVector = np.array(X3local)\n",
    "          else:\n",
    "            InputVector = X3[Batchindex]\n",
    "\n",
    "          Labelsused = labelarray3[Batchindex]\n",
    "          # Calculate Time so we can set FutureMask [i 0:NumTOTAL.t in 0:Tseq, j 0:NumTOTAL u in 0:Tseq]= 1 if Time[j] > Time[i] \n",
    "          # which Implies this case is vetoed\n",
    "          Time = SetSpacetime(np.reshape(Labelsused[:,0],(1,-1)))\n",
    "          InputVector = np.reshape(InputVector,(1,Tseq*Nloc,NpropperseqTOT))\n",
    "          PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time )\n",
    "          PredictedVector = np.reshape(PredictedVector,(1,Nloc,NpredperseqTOT))\n",
    "\n",
    "          swbatched = np.reshape(sw, (1,Nloc,NpredperseqTOT))\n",
    "          if LocationBasedValidation:\n",
    "            swT = np.zeros([1,Nloc,NpredperseqTOT],dtype = np.float32)\n",
    "            swV = np.zeros([1,Nloc,NpredperseqTOT],dtype = np.float32)\n",
    "            for iloc_sample in range(0,Nloc_sample):\n",
    "              fudgeT = Nloc/TrainingNloc\n",
    "              fudgeV = Nloc/ValidationNloc\n",
    "              iloc = Labelsused[iloc_sample,1]\n",
    "              if MappingtoTraining[iloc] >= 0:\n",
    "                swT[0,iloc_sample,:] = swbatched[0,iloc_sample,:]*fudgeT\n",
    "                ct3 += 1.0\n",
    "              else:\n",
    "                swV[0,iloc_sample,:] = swbatched[0,iloc_sample,:]*fudgeV\n",
    "                ct4 += 1.0\n",
    "            swT = np.reshape(swT, (1,Nloc,NpredperseqTOT))\n",
    "            swV = np.reshape(swV, (1,Nloc,NpredperseqTOT))\n",
    "          TrueVector = y3[Batchindex]\n",
    "          TrueVector = np.reshape(TrueVector,(1,Nloc,NpredperseqTOT))\n",
    "\n",
    "          quan2 += numpycustom_lossGCF1(TrueVector,PredictedVector,swbatched)\n",
    "          if LocationBasedValidation:\n",
    "            quan3 += numpycustom_lossGCF1(TrueVector,PredictedVector,swT)\n",
    "            quan4 += numpycustom_lossGCF1(TrueVector,PredictedVector,swV)\n",
    "\n",
    "        fudge = 1.0/Batchsizeused\n",
    "        quan2 *= fudge\n",
    "        meanvalue2 += quan2\n",
    "        variance2 += quan2**2\n",
    "        if LocationBasedValidation:\n",
    "          quan3 *= TrainingNloc/ct3\n",
    "          meanvalue3 += quan3\n",
    "          quan4 *= ValidationNloc/ct4\n",
    "          meanvalue4 += quan4        \n",
    "          gatherhist3.append(quan3)\n",
    "          gatherhist4.append(quan4)\n",
    "\n",
    "        gatherhist2.append(quan2)\n",
    "        if shuffling == 0:\n",
    "          bestvalue2 = quan2\n",
    "        else:\n",
    "          if bestvalue2 > quan2:\n",
    "            bestvalue2 = quan2\n",
    "\n",
    "        samplebar.update(1)\n",
    "        if LocationBasedValidation:\n",
    "          samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Bestloss = bestvalue2, Tr = quan3, Val = quan4)\n",
    "        else:\n",
    "          samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Bestloss = bestvalue2)\n",
    "        \n",
    "# End Shuffling loop\n",
    "\n",
    "      meanvalue2 /= SampleSize\n",
    "      variance2 /= SampleSize\n",
    "      std2 = math.sqrt(variance2 - meanvalue2**2)\n",
    "      Timemean.append(meanvalue2)\n",
    "      Timestd.append(std2)\n",
    "      Timebest.append(bestvalue2)\n",
    "      plt.hist(gatherhist2, 30,  facecolor='b', alpha=0.75)\n",
    "      plt.title(RunComment + ' ' + RunName + ' Time Scope ' + str(localtimeinterval) + ' Mean ' + str(round(meanvalue2,6)))\n",
    "      plt.xlabel('Sum over Training plus Validation samples given time interval')\n",
    "      plt.ylabel('Numbers')\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "      if LocationBasedValidation:\n",
    "        meanvalue3 /= SampleSize\n",
    "        plt.hist(gatherhist3, 30,  facecolor='b', alpha=0.75)\n",
    "        plt.title(RunComment + ' ' + RunName + ' Time Scope ' + str(localtimeinterval) + ' Mean ' + str(round(meanvalue3,6)))\n",
    "        plt.xlabel('Sum over Training samples given time interval')\n",
    "        plt.ylabel('Numbers')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        meanvalue4 /= SampleSize\n",
    "        plt.hist(gatherhist4, 30,  facecolor='b', alpha=0.75)\n",
    "        plt.title(RunComment + ' ' + RunName + ' Time Interval ' + str(localtimeinterval) + ' Mean ' + str(round(meanvalue4,6)))\n",
    "        plt.xlabel('Sum over Validation samples given time interval '  + str(localtimeinterval))\n",
    "        plt.ylabel('Numbers')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        corr = 0.0\n",
    "        v3 = 0.0\n",
    "        v4 = 0.0\n",
    "        for ipos in range(0,SampleSize):\n",
    "          tr = gatherhist3[ipos] - meanvalue3\n",
    "          val = gatherhist4[ipos] - meanvalue4\n",
    "          v3 += tr*tr\n",
    "          v4 += val*val\n",
    "          corr += tr*val\n",
    "        v3 = math.sqrt(v3/SampleSize)\n",
    "        v4 = math.sqrt(v4/SampleSize)\n",
    "        corr /= (v3*v4*SampleSize)\n",
    "        print(startbold+'Time Interval ' + str(localtimeinterval) + ' Train ' + str(round(meanvalue3,6)) + ' std ' + str(round(v3,6))\n",
    "              + ' Val ' + str(round(meanvalue4,6)) + ' std ' + str(round(v4,6)) + ' Correlation ' + str(round(corr,4))+resetfonts)\n",
    "\n",
    "        if LocationBasedValidation:\n",
    "          Train = np.asarray(gatherhist3)\n",
    "          Val = np.asarray(gatherhist4)\n",
    "          Trainorder = np.argsort(Train)\n",
    "          Train = np.take_along_axis(Train,Trainorder,axis=0)\n",
    "          Val = np.take_along_axis(Val,Trainorder,axis=0)\n",
    "          i1 = SampleSize/2\n",
    "          i2 = SampleSize/4\n",
    "          i3 = SampleSize/10\n",
    "          i4 = 10\n",
    "          LengthList = [1,i4,i3,i2,i1,SampleSize]\n",
    "          line =\"\"\n",
    "          for i in range(0,len(LengthList)):\n",
    "            ilength = int(LengthList[i])\n",
    "            Cuttrain = np.mean(Train[0:ilength])\n",
    "            Cutval = np.mean(Val[0:ilength])\n",
    "            line += str(ilength) + \" Tr \" + str(round(Cuttrain,5)) + \" Vl \" + str(round(Cutval,5)) + \" \"\n",
    "          print(line)\n",
    "# End TimeScope Loop\n",
    "\n",
    "    \n",
    "    for TimeScopeLoop in range(0,NumberScopes):\n",
    "      localtimeinterval = TimeScope[TimeScopeLoop]\n",
    "      print('Time Interval ' + str(localtimeinterval) + ' mean ' +str(round(Timemean[TimeScopeLoop],6)) + ' STD ' + \n",
    "            str(round(Timestd[TimeScopeLoop],6)) + ' best ' + str(round(Timebest[TimeScopeLoop],6)))\n",
    "      \n",
    "    print(startbold+startred+ 'END DLPrediction2 ' +current_time + ' ' + RunName + RunComment +resetfonts)\n",
    "    \n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXQgEl_6C2ut",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###DLPrediction2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7HE3cl4C7a_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def DLprediction2B(Xin, yin, DLmodel):\n",
    "  # Input is the windows [Num_Seq] [Nloc] [Tseq] [NpropperseqTOT] (SymbolicWindows False)\n",
    "  # Input is  the sequences [Nloc] [Num_Time-1] [NpropperseqTOT] (SymbolicWindows True)\n",
    "  # Input Predictions are always [Num_Seq] [NLoc] [NpredperseqTOT]\n",
    "\n",
    "  # Calculate Predictions with full sampling and record range of values\n",
    "    if SkipDL2B:\n",
    "      return  \n",
    "    current_time = timenow()\n",
    "    print(wraptotext(startbold+startpurple + current_time + ' ' + RunName + ' ' + RunComment + \n",
    "                     ' DLprediction2B Full Sampling Predictions Follow' + resetfonts))\n",
    "    if GarbageCollect:\n",
    "      gc.collect()\n",
    "    FitPredictions = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "    FitRanges = np.zeros([Num_Seq, Nloc, NpredperseqTOT,5], dtype =np.float32)\n",
    "    FitPredictions0 = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "    FitPredictions1 = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "    FitPredictions2 = np.zeros([Num_Seq, Nloc, NpredperseqTOT], dtype =np.float32)\n",
    "# 0 count 1 mean 2 Standard Deviation 3 Min 4 Max\n",
    "\n",
    "    global  OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
    "    Totaltodo = Num_Seq*Nloc\n",
    "    Nloc_sample = Nloc # default\n",
    "\n",
    "    if IncreaseNloc_sample > 1:\n",
    "      Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
    "    elif DecreaseNloc_sample > 1:\n",
    "      Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
    "\n",
    "    if Totaltodo%Nloc_sample != 0:\n",
    "      printexit('Invalid Nloc_sample ' + str(Nloc_sample) + \" \" + str(Totaltodo))\n",
    "    d_sample = Tseq * Nloc_sample        \n",
    "    max_d_sample = d_sample\n",
    "    OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
    "    print(' Predict with ' +str(Nloc_sample) + ' sequences per sample and batch size ' + str(OuterBatchDimension))\n",
    "\n",
    "    labelarray =np.empty([Num_Seq, Nloc, 2], dtype = np.int32)\n",
    "    for iseq in range(0, Num_Seq):\n",
    "      for iloc in range(0,Nloc):\n",
    "        labelarray[iseq,iloc,0] = iseq\n",
    "        labelarray[iseq,iloc,1] = iloc\n",
    "    FRanges = np.empty ([NpredperseqTOT], dtype = np.float32)\n",
    "    for k in range(0,NpredperseqTOT):\n",
    "      FRanges[k] = 1.0\n",
    "    numpyPredictionwgt = np.asarray(Predictionwgt)\n",
    "\n",
    "    RMSEbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    IntrinsicRMSEbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    MaxEbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    MinEbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    count = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    floss = 0.0\n",
    "\n",
    "    samplebar = notebook.trange(SampleSize,  desc='Predict loop', unit  = 'Complete Shuffle')\n",
    "    bbar = notebook.trange(OuterBatchDimension,  desc='Batch    loop', unit  = 'sample')\n",
    "\n",
    "    bestvalue1 = 0.0\n",
    "    bestvalue2 = 0.0\n",
    "    meanvalue1 = 0.0\n",
    "    meanvalue2 = 0.0\n",
    "    meanvalue3 = 0.0\n",
    "    meanvalue4 = 0.0\n",
    "    gatherhist1 =[]\n",
    "    gatherhist2 =[]\n",
    "    Ctime1 = 0.0\n",
    "    Ctime2 = 0.0\n",
    "    Ctime3 = 0.0\n",
    "    for shuffling in range (0,SampleSize):\n",
    "      if GarbageCollect:\n",
    "        gc.collect()\n",
    "      y2 = np.reshape(yin, (-1, NpredperseqTOT))\n",
    "      labelarray2 = np.reshape(labelarray, (-1,2))\n",
    "      if SymbolicWindows:\n",
    "        labelarray2, y2 = shuffleDLinput(labelarray2, y2)\n",
    "        Locarray = labelarray2[:,1]\n",
    "        Seqarray = labelarray2[:,0]\n",
    "        Locarray = np.reshape(Locarray, (OuterBatchDimension, Nloc_sample))\n",
    "        Seqarray = np.reshape(Seqarray, (OuterBatchDimension, Nloc_sample))\n",
    "      else:\n",
    "        X2 = np.reshape(Xin, (-1, Tseq, NpropperseqTOT))\n",
    "        X2, y2, labelarray2 = shuffleDLinput(X2, y2,labelarray2)\n",
    "        X3 = np.reshape(X2, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "      y3 = np.reshape(y2, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "      labelarray3 = np.reshape(labelarray2, (OuterBatchDimension, Nloc_sample, 2))\n",
    "\n",
    "      quan1 = 0.0\n",
    "      quan2 = 0.0\n",
    "      quan3 = 0.0\n",
    "      quan4 = 0.0\n",
    "      count3 = 0\n",
    "      count4 = 0\n",
    "      for Batchindex in range(0, OuterBatchDimension):\n",
    "        if GarbageCollect:\n",
    "          gc.collect()\n",
    "        StopWatch.start('label1')\n",
    "        if SymbolicWindows:\n",
    "          X3 = list()\n",
    "          for iloc in range(0,Nloc_sample):\n",
    "            LocLocal = Locarray[Batchindex, iloc]\n",
    "            SeqLocal = Seqarray[Batchindex, iloc]\n",
    "            X3.append(ReshapedSequencesTOT[LocLocal,SeqLocal:SeqLocal+Tseq])\n",
    "          InputVector = np.array(X3)\n",
    "        else:\n",
    "          InputVector = X3[Batchindex]\n",
    "        InputVector = np.reshape(InputVector,(1,Tseq*Nloc_sample,NpropperseqTOT))\n",
    "        # Calculate Time so we can set FutureMask [i 0:NumTOTAL.t in 0:Tseq, j 0:NumTOTAL u in 0:Tseq]= 1 if Time[j] > Time[i] \n",
    "        # which Implies this case is vetoed\n",
    "        Time = SetSpacetime(np.reshape(labelarray3[Batchindex,:,0],(1,-1)))\n",
    "        StopWatch.stop('label1')\n",
    "        Ctime1 += StopWatch.get('label1', digits=4)\n",
    "\n",
    "        StopWatch.start('label2')\n",
    "        PredictedVector = DLmodel(InputVector, training = PredictionTraining, Time=Time )\n",
    "        PredictedVector = np.reshape(PredictedVector,(Nloc_sample,NpredperseqTOT))\n",
    "        Time = None\n",
    "        StopWatch.stop('label2')\n",
    "        Ctime2 += StopWatch.get('label2', digits=4)\n",
    "        StopWatch.start('label3')\n",
    "        losspercall = 0.0\n",
    "        count5 = 0\n",
    "        for iloc_sample in range(0,Nloc_sample):\n",
    "          yyhat = PredictedVector[iloc_sample]\n",
    "\n",
    "          iseq = labelarray3[Batchindex,iloc_sample,0]\n",
    "          iloc = labelarray3[Batchindex,iloc_sample,1]\n",
    "          yy = yin[iseq,iloc]\n",
    "          FitPredictions0[iseq,iloc] = yyhat\n",
    "          yy1 = yy[0:Npredperseq]\n",
    "          yyhat1 = yyhat[0:Npredperseq]\n",
    "          quan1 += numpycustom_lossGCF1(yy1,yyhat1,numpyPredictionwgt[0:Npredperseq])\n",
    "          loss = numpycustom_lossGCF1(yy,yyhat,numpyPredictionwgt)\n",
    "          quan2 += loss\n",
    "          losspercall += loss\n",
    "          if MappingtoTraining[iloc] >=0:\n",
    "            quan3 += loss\n",
    "            count3 += 1\n",
    "            count5 += 1\n",
    "          else:\n",
    "            quan4 += loss\n",
    "            count4 += 1\n",
    "            \n",
    "          if (FitRanges [iseq,iloc,0,0] < 0.1):\n",
    "            FitRanges [iseq,iloc,:,3] = yyhat\n",
    "            FitRanges [iseq,iloc,:,4] = yyhat\n",
    "          else:\n",
    "            FitRanges [iseq,iloc,:,3] = np.maximum(FitRanges [iseq,iloc,:,3],yyhat)\n",
    "            FitRanges [iseq,iloc,:,4] = np.minimum(FitRanges [iseq,iloc,:,4],yyhat)\n",
    "          FitRanges [iseq,iloc,:,0] += FRanges\n",
    "          FitRanges [iseq,iloc,:,1] += yyhat\n",
    "          FitRanges [iseq,iloc,:,2] += np.square(yyhat)\n",
    "        StopWatch.stop('label3')\n",
    "        Ctime3 += StopWatch.get('label3', digits=4)\n",
    "        bbar.update(1)\n",
    "        losspercall /= float(Nloc_sample)\n",
    "        Tr = 0.0\n",
    "        if count3 > 0:\n",
    "          Tr = quan3/count3\n",
    "        if count4 > 0:\n",
    "          Vl = quan4/count4\n",
    "          bbar.set_postfix(Loss = losspercall, Tr = Tr, Vl = Vl, Cttr = count5 )\n",
    "        else:\n",
    "          bbar.set_postfix(Loss = losspercall, Tr = Tr, Cttr = count5 )          \n",
    "\n",
    "      fudge = 1.0/(OuterBatchDimension*Nloc_sample)\n",
    "      quan1 *= fudge\n",
    "      quan2 *= fudge\n",
    "      quan3 /= (Num_Seq*TrainingNloc)\n",
    "      if ValidationNloc > 0:\n",
    "        quan4 /= (Num_Seq*ValidationNloc)\n",
    "      gatherhist1.append(quan1)\n",
    "      gatherhist2.append(quan2)\n",
    "\n",
    "      if shuffling == 0:\n",
    "        FitPredictions1 = FitPredictions0.copy()\n",
    "        FitPredictions2 = FitPredictions0.copy()\n",
    "        bestvalue1 = quan1\n",
    "        bestvalue2 = quan2\n",
    "      if ((shuffling >0) and (quan1 < bestvalue1)):\n",
    "        FitPredictions1 = FitPredictions0.copy()\n",
    "        bestvalue1 = quan1\n",
    "      if ((shuffling >0) and (quan2 < bestvalue2)):\n",
    "        FitPredictions2 = FitPredictions0.copy()\n",
    "        bestvalue2 = quan2\n",
    "    \n",
    "      meanvalue1 += quan1\n",
    "      meanvalue2 += quan2\n",
    "      meanvalue3 += quan3\n",
    "      meanvalue4 += quan4\n",
    "\n",
    "      samplebar.update(1)\n",
    "      samplebar.set_postfix(Shuffle=shuffling, Loss = quan2, Bestloss = bestvalue2)\n",
    "      bbar.reset()\n",
    "# End Shuffling loop\n",
    "\n",
    "    current_time = timenow()\n",
    "    print(startbold+startpurple + current_time + RunName + ' ' + RunComment + ' DLprediction2B Summarize Full Sampling Results' + resetfonts)\n",
    "    print('Times ' + str(round(Ctime1,5))  + ' ' + str(round(Ctime3,5)) + ' TF ' + str(round(Ctime2,5)))\n",
    "    meanvalue1 /= SampleSize\n",
    "    meanvalue2 /= SampleSize\n",
    "    meanvalue3 /= SampleSize\n",
    "    meanvalue4 /= SampleSize\n",
    "    print(' Only observed Quantities Best ' + str(round(bestvalue1,5)) + '  Mean ' + str(round(meanvalue1,5)) )\n",
    "    print(' All predictions Best ' + str(round(bestvalue2,5)) + '  Mean ' + str(round(meanvalue2,5)) )\n",
    "    print(' All predictions Mean Training ' + str(round(meanvalue3,5)) + '  Mean Validation ' + str(round(meanvalue4,5)) )\n",
    "    \n",
    "    \n",
    "    global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
    "    GlobalLoss = meanvalue1\n",
    "    if LocationBasedValidation:\n",
    "      GlobalTrainingLoss = meanvalue3\n",
    "      GlobalValidationLoss = meanvalue4\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = [8,6]\n",
    "    gatherhist1 = np.asarray(gatherhist1, dtype=np.float32)\n",
    "    meanhist1 = np.mean(gatherhist1)\n",
    "    plt.hist(gatherhist1, 50, density=False, facecolor='g', alpha=0.75)\n",
    "    plt.title(RunComment + ' ' + RunName + ' Mean% ' + str(round(meanhist1,6)))\n",
    "    plt.xlabel('Sum over Basic Quantities mean%')\n",
    "    plt.ylabel('Numbers')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    gatherhist2 = np.asarray(gatherhist2, dtype=np.float32)\n",
    "    meanhist2 = np.mean(gatherhist2)\n",
    "    plt.hist(gatherhist2, 30,  facecolor='r', alpha=0.75)\n",
    "    plt.title(RunComment + ' ' + RunName + ' Mean% ' + str(round(meanhist2,6)))\n",
    "    plt.xlabel('Sum over all Predictions mean% ')\n",
    "    plt.ylabel('Numbers')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    DLprediction3(yin, FitPredictions1, ' Basic Quantities')\n",
    "    DLprediction3(yin, FitPredictions2, ' Best sum over predictions', Dumpplot = True)\n",
    "\n",
    "    FitRanges[:,:,:,1] = np.divide(FitRanges[:,:,:,1],FitRanges[:,:,:,0])\n",
    "    FitRanges[:,:,:,2] = np.sqrt(np.maximum(np.divide(FitRanges[:,:,:,2],FitRanges[:,:,:,0]) -\n",
    "                                 np.square(FitRanges[:,:,:,1]), 0.0))   \n",
    "    print(wraptotext(RunComment + ' ' + RunName+ '  DLprediction2B Best Basic Error% ' + str(round(100.0*bestvalue1,4)) + \n",
    "          ' Best Total Error% ' + str(round(100.0*bestvalue2,4)) ))\n",
    "\n",
    "\n",
    "    for iseq in range(0,Num_Seq):\n",
    "      for iloc in range(0,Nloc):    \n",
    "        FitPredictions[iseq,iloc,:] = FitRanges[iseq,iloc,:,1]\n",
    "        yy = yin[iseq,iloc]\n",
    "        for i in range(0,NpredperseqTOT):\n",
    "          if(math.isnan(yy[i])):\n",
    "            continue\n",
    "          mse = Predictionwgt[i] * (yy[i]-FitPredictions[iseq,iloc,i])**2\n",
    "          RMSEbyclass[i] += mse\n",
    "          if i < Npredperseq:\n",
    "            floss += mse\n",
    "          IntrinsicRMSEbyclass[i] += FitRanges[iseq,iloc,i,2]**2\n",
    "          MaxEbyclass[i] += np.subtract(FitRanges [iseq,iloc,i,3],FitRanges[iseq,iloc,i,1])\n",
    "          MinEbyclass[i] += np.subtract(FitRanges[iseq,iloc,i,1],FitRanges [iseq,iloc,i,4])\n",
    "          count[i] += 1.0\n",
    "\n",
    "    floss /= (Num_Seq*Nloc)\n",
    "    RMSEbyclass1 = np.divide(RMSEbyclass,count)\n",
    "    RMSEbyclass2 = np.sqrt(np.divide(IntrinsicRMSEbyclass,count))\n",
    "    MaxEbyclass1 = np.divide(MaxEbyclass, count)\n",
    "    MinEbyclass1 = np.divide(MinEbyclass, count)\n",
    "    extracomments = []\n",
    "    print('Total observed Loss ' + str(round(floss,6)))\n",
    "    for i in range(0,NpredperseqTOT):\n",
    "      print('RMSE % ' +str(i) + ' ' + Predictionname[PredictionNameIndex[i]] + ' ' + str(round(count[i],0)) + ' ' + \n",
    "            str(round(RMSEbyclass1[i],6))  + ' ' + str(round(100.0*RMSEbyclass2[i],4))\n",
    "      + ' ' + str(round(100.0*MaxEbyclass1[i],4)) + ' ' + str(round(100.0*MinEbyclass1[i],4)) )\n",
    "      extracomments.append(['Loss Coeff ' + str(round(RMSEbyclass1[i],5))\n",
    "       + ' Intrinsic RMSE% ' + str(round(100.0*RMSEbyclass2[i],4)), ' '])\n",
    "    \n",
    "    current_time = timenow()\n",
    "    print(wraptotext(startbold+startpurple + current_time + ' ' + RunName + ' These FULL plots with uncertainties from DLprediction2B '\n",
    "     + ' ' + RunComment + resetfonts))\n",
    "    global SeparateValandTrainingPlots\n",
    "    saveflag = SeparateValandTrainingPlots\n",
    "    SeparateValandTrainingPlots = False\n",
    "    Location_summed_plot(0,yin, FitPredictions, extracomments=extracomments)\n",
    "    otherlabs2=['Max','Min']\n",
    "    otherfits2 = np.array( [ np.subtract(FitRanges [:,:,:,3],FitRanges[:,:,:,1]),  np.subtract(FitRanges [:,:,:,4],FitRanges[:,:,:,1])])\n",
    "    Location_summed_plot(0,yin, FitPredictions, extracomments = extracomments, otherlabs=otherlabs2, otherfits=otherfits2)\n",
    "    SeparateValandTrainingPlots = saveflag\n",
    "    print(startbold+startpurple+'End DLprediction2B NNSE and Individual Plots Follow' + resetfonts)\n",
    "\n",
    "    FindNNSE(yin, FitPredictions, Label = 'DL2B Full Att')\n",
    "    if IndividualPlots:\n",
    "      ProduceIndividualPlots(yin, FitPredictions)\n",
    "    if Earthquake and EarthquakeImagePlots:\n",
    "      ProduceSpatialQuakePlot(yin, FitPredictions)\n",
    "\n",
    "    return FitPredictions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1rkJNo8DFok",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###DLPrediction3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRhxLB50DOdE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def DLprediction3( yin, FitPredictions, LabelFit, Dumpplot = False):\n",
    "# Input Predictions are [Num_Seq] [NLoc] [NpredperseqTOT]\n",
    "# Use NumericalCutoff and TimeCutLabel from DLPrediction for start and End Sections\n",
    "\n",
    "    current_time = timenow()\n",
    "    print(wraptotext(startbold+startpurple + current_time + ' ' + RunName + ' These plots from DLprediction3 '\n",
    "     + LabelFit  + ' ' + RunComment + resetfonts))\n",
    "    RMSEbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSETRAINbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSEVALbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    RMSVbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    AbsEbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    AbsVbyclass = np.zeros([NpredperseqTOT], dtype=np.float64)\n",
    "    ObsVbytimeandclass = np.zeros([Num_Seq, NpredperseqTOT], dtype=np.float64)\n",
    "    Predbytimeandclass = np.zeros([Num_Seq, NpredperseqTOT], dtype=np.float64)\n",
    "    countbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    countVALbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    countTRAINbyclass = np.zeros([NpredperseqTOT,3], dtype=np.float64)\n",
    "    totalcount = 0\n",
    "    overcount = 0\n",
    "    weightedcount = 0.0\n",
    "    weightedovercount = 0.0\n",
    "    weightedrmse1 = 0.0\n",
    "    weightedrmse1TRAIN = 0.0\n",
    "    weightedrmse1VAL = 0.0\n",
    "\n",
    "    for iseq in range(0, Num_Seq):\n",
    "      yyy = FitPredictions[iseq]\n",
    "      for iloc in range(0,Nloc):\n",
    "        yy = yin[iseq,iloc]\n",
    "        yyhat = yyy[iloc]\n",
    "\n",
    "        sum1 = 0.0\n",
    "        for i in range(0,NpredperseqTOT):\n",
    "          overcount += 1\n",
    "          weightedovercount += Predictionwgt[i]\n",
    "\n",
    "          if(math.isnan(yy[i])):\n",
    "            continue\n",
    "          weightedcount += Predictionwgt[i]\n",
    "          totalcount += 1\n",
    "          mse1 = (yy[i]-yyhat[i])**2\n",
    "          mse = mse1*Predictionwgt[i]\n",
    "          sum1 += mse\n",
    "          AbsEbyclass[i] += abs(yy[i] - yyhat[i])\n",
    "          RMSVbyclass[i] += yy[i]**2\n",
    "          AbsVbyclass[i] += abs(yy[i])\n",
    "          RMSEbyclass[i,0] += mse\n",
    "          countbyclass[i,0] += 1.0\n",
    "          if iseq < NumericalCutoff:\n",
    "            countbyclass[i,1] += 1.0\n",
    "            RMSEbyclass[i,1] += mse\n",
    "          else:\n",
    "            countbyclass[i,2] += 1.0\n",
    "            RMSEbyclass[i,2] += mse\n",
    "          ObsVbytimeandclass [iseq,i] += abs(yy[i])\n",
    "          Predbytimeandclass [iseq,i] += abs(yyhat[i])\n",
    "          if LocationBasedValidation:\n",
    "            if MappingtoTraining[iloc] >= 0:\n",
    "              RMSETRAINbyclass[i,0] += mse\n",
    "              countTRAINbyclass[i,0] += 1.0\n",
    "              if iseq < NumericalCutoff:\n",
    "                RMSETRAINbyclass[i,1] += mse\n",
    "                countTRAINbyclass[i,1] += 1.0\n",
    "              else:\n",
    "                RMSETRAINbyclass[i,2] += mse\n",
    "                countTRAINbyclass[i,2] += 1.0\n",
    "            if MappingtoValidation[iloc] >= 0:\n",
    "              RMSEVALbyclass[i,0] += mse\n",
    "              countVALbyclass[i,0] += 1.0\n",
    "              if iseq < NumericalCutoff:\n",
    "                RMSEVALbyclass[i,1] += mse\n",
    "                countVALbyclass[i,1] += 1.0\n",
    "              else:\n",
    "                RMSEVALbyclass[i,2] += mse\n",
    "                countVALbyclass[i,2] += 1.0\n",
    "\n",
    "        weightedrmse1 += sum1\n",
    "        if LocationBasedValidation:\n",
    "          if MappingtoTraining[iloc] >= 0:\n",
    "            weightedrmse1TRAIN += sum1\n",
    "          if MappingtoValidation[iloc] >= 0:\n",
    "            weightedrmse1VAL += sum1\n",
    "\n",
    "    weightedrmse1 *= 1.0/(Num_Seq*Nloc)\n",
    "    RMSEbyclass = np.divide(RMSEbyclass,countbyclass)\n",
    "    if LocationBasedValidation:\n",
    "      weightedrmse1TRAIN /= (Num_Seq * TrainingNloc)\n",
    "      for i in range(0,NpredperseqTOT):\n",
    "        RMSETRAINbyclass[i,0] /= countTRAINbyclass[i,0]\n",
    "        RMSETRAINbyclass[i,1] /= countTRAINbyclass[i,1]\n",
    "        RMSETRAINbyclass[i,2] /= countTRAINbyclass[i,2]\n",
    "      if ValidationNloc>0:\n",
    "        weightedrmse1VAL /= (Num_Seq * ValidationNloc)\n",
    "        for i in range(0,NpredperseqTOT):\n",
    "          RMSEVALbyclass[i,0] /= countVALbyclass[i,0]\n",
    "          RMSEVALbyclass[i,1] /= countVALbyclass[i,1]\n",
    "          RMSEVALbyclass[i,2] /= countVALbyclass[i,2]\n",
    "    \n",
    "    line = ''\n",
    "    global GlobalTrainingLoss, GlobalValidationLoss, GlobalLoss\n",
    "    GlobalLoss = weightedrmse1\n",
    "    if LocationBasedValidation:\n",
    "      line = ' Training ' + str(round(weightedrmse1TRAIN,6)) + ' Validation ' + str(round(weightedrmse1VAL,6))\n",
    "      GlobalTrainingLoss = weightedrmse1TRAIN\n",
    "      GlobalValidationLoss = weightedrmse1VAL\n",
    "\n",
    "    print(wraptotext(RunName + ' DLPrediction3 ' + LabelFit + ' Weighted sum over predicted values ' + str(round(weightedrmse1,7))\n",
    "         + line + ' ' + RunComment))\n",
    "    print('Count ignoring NaN ' +str(round(weightedcount,4))+ ' Counting NaN ' + str(round(weightedovercount,4)) )\n",
    "\n",
    "\n",
    "    ObsvPred = np.sum( np.abs(ObsVbytimeandclass-Predbytimeandclass) , axis=0)\n",
    "    TotalObs = np.sum( ObsVbytimeandclass , axis=0)\n",
    "    SummedEbyclass = np.divide(ObsvPred,TotalObs)\n",
    "    RMSEbyclass1 = RMSEbyclass # NO Sqrt\n",
    "    RMSEbyclass2 = np.sqrt(np.divide(RMSEbyclass[:,0],RMSVbyclass))\n",
    "    RelEbyclass = np.divide(AbsEbyclass, AbsVbyclass)\n",
    "    extracomments = []\n",
    "    for i in range(0,NpredperseqTOT):\n",
    "      line = startbold + startred + ' Loss Coeffs '\n",
    "      for timecut in range(0,3):\n",
    "        line += TimeCutLabel[timecut] + 'Full ' + str(round(RMSEbyclass1[i,timecut],6)) + resetfonts\n",
    "      if LocationBasedValidation:\n",
    "        RTRAIN = RMSETRAINbyclass[i]\n",
    "        RVAL = np.full(3,0.0, dtype =np.float32)\n",
    "        if ValidationNloc > 0:\n",
    "          RVAL = RMSEVALbyclass[i]\n",
    "        for timecut in range(0,3):\n",
    "          line += startbold + startpurple + TimeCutLabel[timecut] + 'TRAIN ' + resetfonts + str(round(RTRAIN[timecut],6))\n",
    "          line += startbold + ' VAL ' + resetfonts + str(round(RVAL[timecut],6))\n",
    "      else:\n",
    "        RTRAIN = RMSEbyclass1[i]\n",
    "        RVAL = np.full(3,0.0, dtype =np.float32)\n",
    "        for timecut in range(0,3):\n",
    "          line += TimeCutLabel[timecut] + 'FULL ' + str(round(RTRAIN[timecut],6))\n",
    "      print(wraptotext(str(i) + ' ' + startbold + Predictionname[PredictionNameIndex[i]] + resetfonts + ' All Counts ' + str(round(countbyclass[i,0],0))  + ' '\n",
    "       + str(round(100.0*RMSEbyclass2[i],4)) + ' ' + str(round(100.0*RelEbyclass[i],4)) + ' ' + str(round(100.0*SummedEbyclass[i],4)) +line ))\n",
    "      extracomments.append(['Loss Coeffs F=' + str(round(RTRAIN[0],5)) + ' S=' + str(round(RTRAIN[1],5))+ ' E=' + str(round(RTRAIN[2],5)),\n",
    "                              'Loss Coeffs F=' + str(round(RVAL[0],5)) + ' S=' + str(round(RVAL[1],5))+ ' E=' + str(round(RVAL[2],5))])\n",
    "  \n",
    "    print('\\nNext plots from DLPrediction3 ' + LabelFit)\n",
    "    Location_summed_plot(0,yin, FitPredictions, extracomments = extracomments, Dumpplot = Dumpplot)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlTDcrBYljeq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Science Transformer Control Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AbcpEQBtFxZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomScienceTransformermodel(tf.keras.Model):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(CustomScienceTransformermodel, self).__init__(**kwargs)\n",
    "\n",
    "    self.Scienceencoder = Encoder()\n",
    "    self.Sciencemerge = EncodertoLSTMmerge()\n",
    "    self.fullLSTM = MyLSTMlayer() # Identical to that used in standalone LSTM although we change golobal parameters to remove input layers\n",
    "\n",
    "  def call(self, inputs, training=None, Time=None): \n",
    "# Timemask has dimension [OuterBatchDimension, d_sample, d_sample] = 1 (implies ignore) if [b,i,j] has time of [b,i] < time [b,j]\n",
    "# So time is flattened to one domension and is typically sets of Tseq entries\n",
    "#  and entries are ending time of each entry\n",
    "# Time size is number of items in batch times Nloc_sample times Tseq\n",
    "# Only set if (MaskingOption > 0) and GlobalSpacetime:\n",
    "    mask = None\n",
    "    if GlobalSpacetime and (Time is not None):\n",
    "      zero = tf.constant(0.0, dtype=tf.float32)\n",
    "      one = tf.constant(1.0, dtype=tf.float32)\n",
    "\n",
    "      \n",
    "      if SpacewiseSecondAttention and (not TransformerOnlyFullAttention):  # Space mask\n",
    "        Timematrix = tf.reshape(Time,[Time.shape[0],-1,Tseq])\n",
    "        Spacematrix = Timematrix[:,:,0]\n",
    "        Spacematrix = tf.reshape(Spacematrix,[Spacematrix.shape[0],-1,1])\n",
    "        mask = tf.where( (Spacematrix-tf.transpose(Spacematrix, perm=[0,2,1]))>0, zero,one) \n",
    "      else: # Full time+space mask\n",
    "        Timematrix = tf.reshape(Time,[Time.shape[0],-1,1])\n",
    "        mask = tf.where( (Timematrix-tf.transpose(Timematrix, perm=[0,2,1]))>0, zero,one) \n",
    "\n",
    "    restoreinputs = inputs\n",
    "    EncoderOutput, Mappedinput = self.Scienceencoder(restoreinputs, training=training, mask = mask)\n",
    "\n",
    "    # Here we expose separate dimensions Nloc_sample and Tseq\n",
    "    if UseMappedinput:\n",
    "      Mappedinput = tf.reshape(Mappedinput,[1, Nloc_sample*Tseq, Mappedinput.shape[-1]] )\n",
    "      compositeinputs = self.Sciencemerge(Mappedinput,EncoderOutput, training = training)\n",
    "    else:\n",
    "      compositeinputs = self.Sciencemerge(restoreinputs,EncoderOutput, training = training)\n",
    "      \n",
    "    # -1 is compositeinputs.shape[0]*Nloc_sample\n",
    "    bigbatch = tf.reshape(compositeinputs, [-1, Tseq, compositeinputs.shape[3] ])\n",
    "    outputs = self.fullLSTM(bigbatch, training = training)\n",
    "    return outputs\n",
    "  \n",
    "  def compile(self, optimizer,  loss):\n",
    "      super(CustomScienceTransformermodel, self).compile()\n",
    "      self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "      self.loss_object = loss\n",
    "      self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "      self.loss_tracker.reset_states()\n",
    "      self.val_tracker = tf.keras.metrics.Mean(name=\"val\")\n",
    "      self.val_tracker.reset_states()\n",
    "      return\n",
    "\n",
    "  def resetmetrics(self):\n",
    "      self.loss_tracker.reset_states()\n",
    "      self.val_tracker.reset_states()\n",
    "      return\n",
    "\n",
    "  def build_graph(self, shapes):\n",
    "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data, Time=None):\n",
    "    if len(data) == 3:\n",
    "      X_train, y_train, sw_train = data\n",
    "    else:\n",
    "      X_train, y_train = data\n",
    "      sw_train = []\n",
    "\n",
    "    # Collapse first two dimensions -- same is done for LSTM input\n",
    "    #  X_train: OuterBatchDimension = Num_Seq batched to 1, d_sample = [Nloc_sample,Tseq], #properties]\n",
    "    sw_train = tf.reshape(sw_train,[-1,sw_train.shape[2]])\n",
    "    y_train = tf.reshape(y_train,[-1,y_train.shape[2]])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = self(X_train, training=True, Time=Time)\n",
    "      loss = self.loss_object(y_train, predictions, sw_train)     \n",
    "\n",
    "    variables = self.Scienceencoder.trainable_variables + self.Sciencemerge.trainable_variables + self.fullLSTM.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "    self.loss_tracker.update_state(loss)\n",
    "    return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data, Time=None):\n",
    "    if len(data) == 3:\n",
    "      X_val, y_val, sw_val = data\n",
    "    else:\n",
    "      X_val, y_val = data\n",
    "      sw_val = []\n",
    "\n",
    "    # Collapse first two dimensions -- same is done for LSTM input\n",
    "    sw_val = tf.reshape(sw_val,[-1,sw_val.shape[2]])\n",
    "    y_val = tf.reshape(y_val,[-1,y_val.shape[2]])\n",
    "\n",
    "    predictions = self(X_val, training=False, Time=Time)\n",
    "\n",
    "    loss = self.loss_object(y_val, predictions, sw_val)\n",
    "    self.val_tracker.update_state(loss)\n",
    "    return {\"val_loss\": self.val_tracker.result()}\n",
    "\n",
    "def RunScienceTransformer():\n",
    "# Run the Science Transformer model defined by Model and Encoder/LSTM classes\n",
    "# There are a lot  of reshapings. X_Transformerdetailed and y_Transformerdetailed are viewed as unchanged\n",
    "# Others are gennerated here for lifetime of this call\n",
    "# Note shuffling done for a) Choice of Validation set where ONLY start time shuffled\n",
    "# and b) for batching where both shuffling over time and location performed\n",
    "# X_Transformertrainingplusval = np.reshape(X_Transformerdetailed, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "# y_Transformertrainingplusval = np.reshape(y_Transformerdetailed, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "# Nloc Number of locations\n",
    "# d_sample is  Nloc*Tseq in order [Nloc_sample=Nloc] [Tseq]\n",
    "# OuterBatchDimension is Num_Seq and is batching dimension which is also shuffling dimension\n",
    "# It changes if conventional validation used\n",
    "# Note Transformer batching dimension smaller and different from LSTM\n",
    "\n",
    "  global UsedTransformervalidationfrac, Transformervalidationfrac, OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
    "  global processindex\n",
    "  OuterBatchDimension = Num_Seq\n",
    "\n",
    "  extrainitializationstring = 'Basic System'\n",
    "  if usecustomfit :\n",
    "    extrainitializationstring = 'Custom Class'\n",
    "    \n",
    "  if LocationBasedValidation:\n",
    "    UsedTransformervalidationfrac = LocationValidationFraction\n",
    "    X_predict, y_predict, Spacetime_predict, X_val, y_val, Spacetime_val = setSeparateDLinput(1, Spacetime = True)\n",
    "    y_predicttocount = np.reshape(y_Transformerdetailed, (-1,NpredperseqTOT))\n",
    "    InitializeDLforTimeSeries(extrainitializationstring + ' Class custom  Version with location-based validation ',\n",
    "                        processindex,y_predicttocount)\n",
    "    Nloc_sample = TrainingNloc\n",
    "    OuterBatchDimension = Num_Seq\n",
    "    d_sample = Tseq * TrainingNloc\n",
    "    max_d_sample = d_sample\n",
    "\n",
    "    Total_train = Num_Seq\n",
    "    Total_val = 0\n",
    "    UsedValidationNloc = ValidationNloc\n",
    "    if UsedTransformervalidationfrac > 0.001:\n",
    "      Total_val = Num_Seq\n",
    "      if FullSetValidation:\n",
    "        UsedValidationNloc = Nloc\n",
    "    Total_all = Total_train + Total_val\n",
    "\n",
    "    if SymbolicWindows:\n",
    "      X_Transformertraining = np.reshape(X_predict, (OuterBatchDimension, Nloc_sample))\n",
    "    else:\n",
    "      X_Transformertraining = np.reshape(X_predict, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "    y_Transformertraining = np.reshape(y_predict, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "    Spacetime_Transformertraining = np.reshape(Spacetime_predict, (OuterBatchDimension, Nloc_sample))\n",
    "    if UsedTransformervalidationfrac > 0.001:\n",
    "      if SymbolicWindows:\n",
    "        X_Transformerval = np.reshape(X_val, (OuterBatchDimension, UsedValidationNloc))\n",
    "      else:\n",
    "        X_Transformerval = np.reshape(X_val, (OuterBatchDimension, UsedValidationNloc*Tseq, NpropperseqTOT))\n",
    "      y_Transformerval = np.reshape(y_val, (OuterBatchDimension, UsedValidationNloc, NpredperseqTOT))\n",
    "      Spacetime_Transformerval = np.reshape(Spacetime_val, (OuterBatchDimension, UsedValidationNloc))\n",
    "\n",
    "    if UseClassweights:     \n",
    "      sw_Transformertraining = np.empty_like(y_predict, dtype=np.float32)\n",
    "      for i in range(0,sw_Transformertraining.shape[0]):\n",
    "        for j in range(0,sw_Transformertraining.shape[1]):\n",
    "          for k in range(0,NpredperseqTOT):\n",
    "            sw_Transformertraining[i,j,k] = Predictionwgt[k]\n",
    "      if UsedTransformervalidationfrac > 0.001:\n",
    "        fudge = Nloc/ValidationNloc\n",
    "        sw_Transformerval = np.empty_like(y_val, dtype=np.float32)\n",
    "        for i in range(0,sw_Transformerval.shape[0]):\n",
    "          for jloc in range(0,sw_Transformerval.shape[1]):\n",
    "            for k in range(0,NpredperseqTOT):\n",
    "              if FullSetValidation:\n",
    "                if MappingtoValidation[jloc] >= 0:\n",
    "                  sw_Transformerval[i,jloc,k] = Predictionwgt[k]*fudge\n",
    "                else:\n",
    "                  sw_Transformerval[i,jloc,k] = 0.0\n",
    "              else:\n",
    "                sw_Transformerval[i,jloc,k] = Predictionwgt[k]  \n",
    "      else:\n",
    "        sw_Transformerval = []\n",
    "    else:\n",
    "      sw_Transformertraining = []\n",
    "      sw_Transformerval = []\n",
    "  \n",
    "  else: # Traditional Validation or no validation -- first set up if no validation\n",
    "    OuterBatchDimension = Num_Seq\n",
    "    UsedTransformervalidationfrac = Transformervalidationfrac\n",
    "    UsedValidationNloc = Nloc\n",
    "    Nloc_sample = Nloc\n",
    "    d_sample = Tseq * Nloc\n",
    "    max_d_sample = d_sample\n",
    "    if SymbolicWindows:\n",
    "      X_Transformertrainingplusval = np.reshape(X_Transformerdetailed, (OuterBatchDimension, Nloc_sample))\n",
    "    else:\n",
    "      X_Transformertrainingplusval = np.reshape(X_Transformerdetailed, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "    Spacetime_Transformertrainingplusval = SpacetimeforMask.copy()\n",
    "    Spacetime_Transformertrainingplusval = np.reshape(Spacetime_Transformertrainingplusval, (OuterBatchDimension, Nloc_sample))\n",
    "    y_Transformertrainingplusval = np.reshape(y_Transformerdetailed, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "    X_Transformertrainingplusval, y_Transformertrainingplusval, Spacetime_Transformertrainingplusval = (\n",
    "                  shuffleDLinput(X_Transformertrainingplusval, y_Transformertrainingplusval, \n",
    "                  Spacetime = Spacetime_Transformertrainingplusval) ) # Shuffle for validation\n",
    "    Total_all = X_Transformertrainingplusval.shape[0]\n",
    "    Total_val = 0\n",
    "\n",
    "    # Decide on validation fraction based on batching dimension -- sequences\n",
    "    if UsedTransformervalidationfrac > 0.001:\n",
    "      Total_val = int(UsedTransformervalidationfrac*Total_all)\n",
    "      print(RunName + \" Validation samples \", Total_val, \" Training samples \", Total_all-Total_val,\" from sequences\")\n",
    "      if  Total_val==0:\n",
    "        UsedTransformervalidationfrac = 0.0\n",
    "    Total_train = Total_all - Total_val\n",
    "\n",
    "# set up if no validation\n",
    "    y_predict = np.reshape(y_Transformertrainingplusval, (OuterBatchDimension*Nloc_sample, NpredperseqTOT))\n",
    "    InitializeDLforTimeSeries(startbold + RunComment + ' ' + RunName + ' ' + extrainitializationstring + \n",
    "                        ' Transformer  Version ' + resetfonts,processindex,y_predict)\n",
    "\n",
    "    # Set Sample Weights for whole sample\n",
    "    if UseClassweights:     \n",
    "      sw = np.empty_like(y_Transformertrainingplusval, dtype=np.float32)\n",
    "      for i in range(0,sw.shape[0]):\n",
    "        for j in range(0,sw.shape[1]):\n",
    "          for k in range(0,NpredperseqTOT):\n",
    "            sw[i,j,k] = Predictionwgt[k] \n",
    "\n",
    "    if UsedTransformervalidationfrac > 0.001:\n",
    "      X_Transformerval = X_Transformertrainingplusval[0:Total_val]\n",
    "      y_Transformerval = y_Transformertrainingplusval[0:Total_val]\n",
    "      X_Transformertraining = X_Transformertrainingplusval[Total_val:]\n",
    "      y_Transformertraining = y_Transformertrainingplusval[Total_val:]\n",
    "      Spacetime_Transformerval = Spacetime_Transformertrainingplusval[0:Total_val]\n",
    "      Spacetime_Transformertraining = Spacetime_Transformertrainingplusval[Total_val:]\n",
    "      if UseClassweights:\n",
    "        sw_Transformerval = sw[0:Total_val]\n",
    "        sw_Transformertraining = sw[Total_val:]\n",
    "    else:\n",
    "      X_Transformertraining = X_Transformertrainingplusval\n",
    "      y_Transformertraining = y_Transformertrainingplusval\n",
    "      sw_Transformertraining = sw\n",
    "      Spacetime_Transformertraining = Spacetime_Transformertrainingplusval\n",
    "\n",
    "  epochsize = Total_all\n",
    "  if not(usecustomfit and UseClassweights):\n",
    "    printexit('Unsupported default fit')\n",
    "\n",
    "  if SymbolicWindows:\n",
    "    X_Transformertrainingflat2 = np.reshape(X_Transformertraining, (-1, TrainingNloc))\n",
    "    X_Transformertrainingflat1 = np.reshape(X_Transformertrainingflat2, (-1))\n",
    "  else:\n",
    "    X_Transformertrainingflat2 = np.reshape(X_Transformertraining, (-1, TrainingNloc,Tseq, NpropperseqTOT))\n",
    "    X_Transformertrainingflat1 = np.reshape(X_Transformertrainingflat2, (-1, Tseq, NpropperseqTOT))\n",
    "  y_Transformertrainingflat1 = np.reshape(y_Transformertraining, (-1,NpredperseqTOT) )\n",
    "  #    X_Transformertrainingflat1 = np.reshape(X_Transformertrainingflat2, (-1))\n",
    "  Spacetime_Transformertrainingflat1 = np.reshape(Spacetime_Transformertraining,(-1))   \n",
    "  if UseClassweights: \n",
    "    sw_Transformertrainingflat1 = np.reshape(sw_Transformertraining, (-1,NpredperseqTOT) )\n",
    "\n",
    "  if UsedTransformervalidationfrac > 0.001:\n",
    "    if SymbolicWindows:\n",
    "      X_Transformervalflat2 = np.reshape(X_Transformerval, (-1, UsedValidationNloc))\n",
    "      X_Transformervalflat1 = np.reshape(X_Transformervalflat2, (-1))\n",
    "    else:\n",
    "      X_Transformervalflat2 = np.reshape(X_Transformerval, (-1, UsedValidationNloc,Tseq, NpropperseqTOT))\n",
    "      X_Transformervalflat1 = np.reshape(X_Transformervalflat2, (-1, Tseq, NpropperseqTOT))\n",
    "    y_Transformervalflat1 = np.reshape(y_Transformerval, (-1,NpredperseqTOT) )\n",
    "    Spacetime_Transformervalflat1 = np.reshape(Spacetime_Transformerval,(-1))\n",
    "    if UseClassweights: \n",
    "      sw_Transformervalflat1 = np.reshape(sw_Transformerval, (-1,NpredperseqTOT) )\n",
    "\n",
    "  # FINALLY SET UP MODEL\n",
    "  myScienceTransformermodel = CustomScienceTransformermodel()\n",
    "\n",
    "  myScienceTransformermodel.compile(loss= weightedcustom_lossGCF1, optimizer= Transformeroptimizer)\n",
    "  recordtrainloss = []\n",
    "  recordvalloss = []\n",
    "  tfrecordtrainloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "  tfrecordvalloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "  tfepochstep = tf.Variable(0, trainable = False)\n",
    "\n",
    "  myScienceTransformermodel.compile(loss= weightedcustom_lossGCF1, optimizer= Transformeroptimizer)\n",
    "  Dictopt = myScienceTransformermodel.optimizer.get_config()\n",
    "  print(startbold+startred + 'Optimizer ' + resetfonts, Dictopt)\n",
    "\n",
    "  # Set up checkpoints to read or write\n",
    "  mycheckpoint = tf.train.Checkpoint(optimizer=myScienceTransformermodel.optimizer, \n",
    "                                    model=myScienceTransformermodel, tfepochstep=tf.Variable(0),\n",
    "                                    tfrecordtrainloss=tfrecordtrainloss,tfrecordvalloss=tfrecordvalloss)\n",
    "\n",
    "\n",
    "  if Restorefromcheckpoint:\n",
    "    save_path = inputCHECKPOINTDIR + inputRunName + inputCheckpointpostfix\n",
    "    mycheckpoint.restore(save_path=save_path).expect_partial()\n",
    "    tfepochstep  = mycheckpoint.tfepochstep \n",
    "    recordvalloss = mycheckpoint.tfrecordvalloss.numpy().tolist()\n",
    "    recordtrainloss = mycheckpoint.tfrecordtrainloss.numpy().tolist()\n",
    "    trainlen = len(recordtrainloss)\n",
    "    extrainfo = ''\n",
    "    vallen = len(recordvalloss)\n",
    "    SavedTrainLoss = recordtrainloss[trainlen-1]\n",
    "    SavedValLoss = 0.0\n",
    "    if vallen > 0:\n",
    "      extrainfo = ' Val Loss ' + str(round(recordvalloss[vallen-1],7))\n",
    "      SavedValLoss = recordvalloss[vallen-1]\n",
    "    print(startbold + 'Network restored from ' + save_path + '\\nLoss ' + str(round(recordtrainloss[trainlen-1],7)) \n",
    "      + extrainfo + ' Epochs ' + str(tfepochstep.numpy()) + resetfonts )\n",
    "    TransformerTFMonitor.SetCheckpointParms(mycheckpoint,CHECKPOINTDIR,RunName = RunName,Restoredcheckpoint= True, \n",
    "            Restored_path = save_path,  ValidationFraction = UsedTransformervalidationfrac, SavedTrainLoss = SavedTrainLoss, SavedValLoss =SavedValLoss)\n",
    "  else:\n",
    "    TransformerTFMonitor.SetCheckpointParms(mycheckpoint,CHECKPOINTDIR,RunName = RunName,Restoredcheckpoint= False, \n",
    "                                            ValidationFraction = UsedTransformervalidationfrac)\n",
    "\n",
    "  # This just does analysis\n",
    "  if AnalysisOnly:\n",
    "    finalanalysis(myScienceTransformermodel,recordtrainloss, recordvalloss,RunComment, True)\n",
    "    return myScienceTransformermodel\n",
    "\n",
    "  # Initialize progress bars\n",
    "  pbar = notebook.trange(Transformerepochs, desc='Training loop', unit ='epoch')\n",
    "  if IncreaseNloc_sample > 1:\n",
    "    epochsize = int(epochsize/IncreaseNloc_sample)\n",
    "  elif DecreaseNloc_sample > 1:\n",
    "    epochsize = int(epochsize*DecreaseNloc_sample)\n",
    "\n",
    "  bbar = notebook.trange(epochsize,  desc='Batch    loop', unit  = 'sample')\n",
    "\n",
    "  Ctime1 = 0.0\n",
    "  Ctime2 = 0.0\n",
    "  Ctime3 = 0.0\n",
    "  GarbageCollect = True\n",
    "  garbagecollectcall = 0\n",
    "\n",
    "  for e in pbar:\n",
    "    myScienceTransformermodel.resetmetrics()\n",
    "    train_lossoverbatch=[]\n",
    "    val_lossoverbatch=[]\n",
    "\n",
    "    if LocationBasedValidation:\n",
    "      Nloc_sample = TrainingNloc\n",
    "      OuterBatchDimension = Num_Seq\n",
    "      d_sample = Tseq * TrainingNloc\n",
    "    else:\n",
    "      Nloc_sample = Nloc\n",
    "      OuterBatchDimension = Total_train\n",
    "      d_sample = Tseq * Nloc        \n",
    "    max_d_sample = d_sample\n",
    "\n",
    "    if TimeShufflingOnly:\n",
    "      X_train, y_train, sw_train, Spacetime_train = shuffleDLinput(X_Transformertraining, \n",
    "            y_Transformertraining, sw_Transformertraining, Spacetime = Spacetime_Transformertraining)\n",
    "    else:\n",
    "      X_train, y_train, sw_train, Spacetime_train = shuffleDLinput(X_Transformertrainingflat1, \n",
    "            y_Transformertrainingflat1, sw_Transformertrainingflat1, Spacetime = Spacetime_Transformertrainingflat1)\n",
    "      \n",
    "      if LocationBasedValidation:\n",
    "        Nloc_sample = TrainingNloc\n",
    "        OuterBatchDimension = Num_Seq\n",
    "      else:\n",
    "        Nloc_sample = Nloc\n",
    "        OuterBatchDimension = Total_train        \n",
    "      Totaltodo = Nloc_sample*OuterBatchDimension\n",
    "      if IncreaseNloc_sample > 1:\n",
    "        Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
    "      elif DecreaseNloc_sample > 1:\n",
    "        Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
    "      OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
    "      if OuterBatchDimension * Nloc_sample != Totaltodo:\n",
    "        printexit('Inconsistent Nloc_sample ' + str(Nloc_sample))\n",
    "      d_sample = Tseq * Nloc_sample\n",
    "      max_d_sample = d_sample\n",
    "\n",
    "      if SymbolicWindows:\n",
    "        X_train = np.reshape(X_train, (OuterBatchDimension, Nloc_sample))\n",
    "      else:\n",
    "        X_train = np.reshape(X_train, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "      y_train = np.reshape(y_train, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "      sw_train = np.reshape(sw_train, (OuterBatchDimension, Nloc_sample, NpredperseqTOT)) \n",
    "      Spacetime_train = np.reshape(Spacetime_train, (OuterBatchDimension, Nloc_sample))\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train, sw_train, Spacetime_train))\n",
    "    train_dataset = train_dataset.batch(Transformerbatch_size)\n",
    "\n",
    "    if batchperepoch:\n",
    "      qbar = notebook.trange(epochsize, desc='Batch loop epoch ' +str(e))\n",
    "\n",
    "  # Start Training Batch Loop\n",
    "    for batch, (X_train, y_train, sw_train, Spacetime_train) in enumerate(train_dataset.take(-1)):\n",
    "      Numinbatch = X_train.shape[0]\n",
    "      NuminAttention = X_train.shape[1]\n",
    "      NumTOTAL = Numinbatch*NuminAttention\n",
    "      Time = None\n",
    "\n",
    "      # SymbolicWindows X_train is indexed by Batch index, Location List for Attention. Missing 1(replace by Window), 1 (replace by properties)\n",
    "      if SymbolicWindows:\n",
    "        StopWatch.start('label1')\n",
    "        X_train = X_train.numpy()          \n",
    "        X_train = np.reshape(X_train,NumTOTAL)\n",
    "        iseqarray = np.right_shift(X_train,16)\n",
    "        ilocarray = np.bitwise_and(X_train, 0b1111111111111111)\n",
    "        StopWatch.stop('label1')\n",
    "        Ctime1 += StopWatch.get('label1', digits=4)\n",
    "        StopWatch.start('label3')\n",
    "        X_train_withSeq = list()\n",
    "        for iloc in range(0,NumTOTAL):\n",
    "          X_train_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "  #         X_train_withSeq=[ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq] for iloc in range(0,Numinbatch)]\n",
    "        X_train_withSeq = np.array(X_train_withSeq)\n",
    "        X_train_withSeq = np.reshape(X_train_withSeq,(Numinbatch, d_sample, NpropperseqTOT))\n",
    "        #\n",
    "        # Calculate Time so we can set FutureMask [i 0:NumTOTAL.t in 0:Tseq, j 0:NumTOTAL u in 0:Tseq]= 1 if Time[j] > Time[i] \n",
    "        # which Implies this case is vetoed\n",
    "        Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
    "        StopWatch.stop('label3')\n",
    "        Ctime3 += StopWatch.get('label3', digits=5)\n",
    "\n",
    "        StopWatch.start('label2')\n",
    "        loss = myScienceTransformermodel.train_step((X_train_withSeq, y_train, sw_train), Time=Time)\n",
    "        StopWatch.stop('label2')\n",
    "        Ctime2 += StopWatch.get('label2', digits=4)\n",
    "\n",
    "      else:\n",
    "        iseqarray = np.right_shift(Spacetime_train,16)\n",
    "        Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
    "        loss =   myScienceTransformermodel.train_step((X_train, y_train, sw_train), Time=Time)\n",
    "      \n",
    "      if GarbageCollect:\n",
    "        if SymbolicWindows:\n",
    "          X_train_withSeq = None\n",
    "        X_train = None\n",
    "        y_train = None\n",
    "        sw_train = None\n",
    "        Spacetime_train = None\n",
    "\n",
    "        if garbagecollectcall > GarbageCollectionLimit:\n",
    "          garbagecollectcall = 0\n",
    "          gc.collect()\n",
    "        garbagecollectcall += 1\n",
    "\n",
    "      localloss = loss[\"loss\"].numpy()\n",
    "      train_lossoverbatch.append(localloss)\n",
    "\n",
    "      if batchperepoch:\n",
    "        qbar.update(Transformerbatch_size)\n",
    "        qbar.set_postfix(Loss = localloss, Epoch = e)\n",
    "      bbar.update(Transformerbatch_size)\n",
    "      bbar.set_postfix(Loss = localloss, Epoch = e)\n",
    "\n",
    "  # Training Batch Loop has ended\n",
    "    if GarbageCollect:\n",
    "      train_dataset = None\n",
    "\n",
    "  # Start Validation Loop over batches       \n",
    "    if UsedTransformervalidationfrac > 0.001:\n",
    "      Time = None\n",
    "      OuterBatchDimension = Total_val\n",
    "      if LocationBasedValidation:\n",
    "        Nloc_sample = UsedValidationNloc\n",
    "        OuterBatchDimension = Num_Seq\n",
    "      else:\n",
    "        Nloc_sample = Nloc\n",
    "      \n",
    "      Totaltodo = Nloc_sample*OuterBatchDimension\n",
    "      if IncreaseNloc_sample > 1:\n",
    "        Nloc_sample = int(Nloc_sample*IncreaseNloc_sample)\n",
    "      elif DecreaseNloc_sample > 1:\n",
    "        Nloc_sample = int(Nloc_sample/DecreaseNloc_sample)\n",
    "      OuterBatchDimension = int(Totaltodo/Nloc_sample)\n",
    "      if OuterBatchDimension * Nloc_sample != Totaltodo:\n",
    "        printexit('Inconsistent Nloc_sample ' + str(Nloc_sample))\n",
    "      d_sample = Tseq * Nloc_sample\n",
    "      max_d_sample = d_sample        \n",
    "\n",
    "      if TimeShufflingOnly:\n",
    "        X_val, y_val, sw_val, Spacetime_val = shuffleDLinput(\n",
    "            X_Transformerval, y_Transformerval, sw_Transformerval, Spacetime_Transformerval)\n",
    "      else:\n",
    "        X_val, y_val, sw_val, Spacetime_val = shuffleDLinput(\n",
    "            X_Transformervalflat1, y_Transformervalflat1, sw_Transformervalflat1, Spacetime_Transformervalflat1)\n",
    "        if SymbolicWindows:\n",
    "          X_val = np.reshape(X_val, (OuterBatchDimension, Nloc_sample))\n",
    "        else:\n",
    "          X_val = np.reshape(X_val, (OuterBatchDimension, d_sample, NpropperseqTOT))\n",
    "      y_val = np.reshape(y_val, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "      sw_val = np.reshape(sw_val, (OuterBatchDimension, Nloc_sample, NpredperseqTOT))\n",
    "      Spacetime_val = np.reshape(Spacetime_val, (OuterBatchDimension, Nloc_sample))\n",
    "\n",
    "      for Validationindex in range(0,OuterBatchDimension):\n",
    "        X_valbatch = X_val[Validationindex]\n",
    "        y_valbatch = y_val[Validationindex]\n",
    "        sw_valbatch = sw_val[Validationindex]\n",
    "        Spacetime_valbatch = Spacetime_val[Validationindex]\n",
    "        if SymbolicWindows:\n",
    "          X_valbatch = np.reshape(X_valbatch,[1,X_valbatch.shape[0]])\n",
    "        else:\n",
    "          X_valbatch = np.reshape(X_valbatch,[1,X_valbatch.shape[0],X_valbatch.shape[1]])\n",
    "        y_valbatch = np.reshape(y_valbatch,[1,y_valbatch.shape[0],y_valbatch.shape[1]])\n",
    "        sw_valbatch = np.reshape(sw_valbatch,[1,sw_valbatch.shape[0],sw_valbatch.shape[1]])\n",
    "        Numinbatch = X_valbatch.shape[0]\n",
    "        NuminAttention = X_valbatch.shape[1]\n",
    "        NumTOTAL = Numinbatch*NuminAttention\n",
    "\n",
    "        if SymbolicWindows:\n",
    "          StopWatch.start('label1')          \n",
    "          X_valbatch = np.reshape(X_valbatch,NumTOTAL)\n",
    "          iseqarray = np.right_shift(X_valbatch,16)\n",
    "          ilocarray = np.bitwise_and(X_valbatch, 0b1111111111111111)\n",
    "          StopWatch.stop('label1')\n",
    "          Ctime1 += StopWatch.get('label1', digits=4)\n",
    "          StopWatch.start('label3')\n",
    "          X_valbatch_withSeq = list()\n",
    "          for iloc in range(0,NumTOTAL):\n",
    "            X_valbatch_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "          X_valbatch_withSeq = np.array(X_valbatch_withSeq)\n",
    "          X_valbatch_withSeq = np.reshape(X_valbatch_withSeq,(Numinbatch, d_sample, NpropperseqTOT))\n",
    "          Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
    "          StopWatch.stop('label3')\n",
    "          Ctime3 += StopWatch.get('label3', digits=5)\n",
    "          StopWatch.start('label2')\n",
    "          loss = myScienceTransformermodel.test_step((X_valbatch_withSeq, y_valbatch, sw_valbatch), Time=Time)\n",
    "          StopWatch.stop('label2')\n",
    "          Ctime2 += StopWatch.get('label2', digits=4)\n",
    "\n",
    "        else:\n",
    "          Spacetime_valbatch = np.reshape(Spacetime_valbatch,-1)\n",
    "          iseqarray = np.right_shift(Spacetime_valbatch,16)\n",
    "          Time = SetSpacetime(np.reshape(iseqarray,[Numinbatch,-1]))\n",
    "          loss =   myScienceTransformermodel.test_step((X_valbatch, y_valbatch, sw_valbatch), Time=Time)\n",
    "        localval = loss[\"val_loss\"].numpy()\n",
    "        val_lossoverbatch.append(localval)\n",
    "\n",
    "        bbar.update(Transformerbatch_size)\n",
    "        bbar.set_postfix(Loss = localloss, Val_loss = localval, Epoch = e)\n",
    "\n",
    "      if GarbageCollect:\n",
    "        X_val = None\n",
    "        y_val = None\n",
    "        sw_val = None\n",
    "        X_valbatch = None\n",
    "        y_valbatch = None\n",
    "        sw_valbatch = None\n",
    "        if SymbolicWindows:\n",
    "          X_valbatch_withSeq = None\n",
    "        if garbagecollectcall > GarbageCollectionLimit:\n",
    "          garbagecollectcall = 0\n",
    "          gc.collect()\n",
    "        garbagecollectcall += 1\n",
    "\n",
    "    train_epoch = train_lossoverbatch[-1]\n",
    "    recordtrainloss.append(train_epoch)\n",
    "    mycheckpoint.tfrecordtrainloss = tf.Variable(recordtrainloss)\n",
    "\n",
    "    val_epoch = 0.0\n",
    "    if UsedTransformervalidationfrac > 0.001:\n",
    "      val_epoch = val_lossoverbatch[-1]\n",
    "      recordvalloss.append(val_epoch)\n",
    "      mycheckpoint.tfrecordvalloss = tf.Variable(recordvalloss)\n",
    "    \n",
    "    # End Epoch Processing\n",
    "    pbar.set_postfix(Loss = train_epoch, Val = val_epoch)\n",
    "    bbar.reset()\n",
    "    tfepochstep = tfepochstep + 1\n",
    "    mycheckpoint.tfepochstep.assign(tfepochstep)\n",
    "\n",
    "  # Decide on best fit\n",
    "    MonitorResult, train_epoch, val_epoch = TransformerTFMonitor.EpochEvaluate(e,train_epoch, val_epoch, tfepochstep, recordtrainloss, recordvalloss)\n",
    "    if MonitorResult==1:\n",
    "      tfepochstep, recordtrainloss, recordvalloss, train_epoch, val_epoch = TransformerTFMonitor.RestoreBestFit() # Restore Best Fit\n",
    "    else:\n",
    "      continue\n",
    "# *********************** End of Epoch Loop for custom training\n",
    "\n",
    "# Print Fit details\n",
    "  print(startbold + 'Times ' + str(round(Ctime1,5))  + ' ' + str(round(Ctime3,5)) + ' TF ' + str(round(Ctime2,5)) + resetfonts)\n",
    "  TransformerTFMonitor.PrintEndofFit(Transformerepochs)\n",
    "\n",
    "# Set Best Possible Fit\n",
    "  tfepochstep, recordtrainloss, recordvalloss, train_epoch, val_epoch = TransformerTFMonitor.BestPossibleFit()\n",
    "\n",
    "# Save Final State\n",
    "  if Checkpointfinalstate:\n",
    "    savepath = mycheckpoint.save(file_prefix=CHECKPOINTDIR + RunName)\n",
    "    print('Checkpoint at ' + savepath + ' from ' + CHECKPOINTDIR)\n",
    "  trainlen = len(recordtrainloss)\n",
    "  extrainfo = ''\n",
    "  if UsedTransformervalidationfrac > 0.001:\n",
    "    vallen = len(recordvalloss)\n",
    "    extrainfo = ' Val Epoch ' + str(vallen-1) + ' Val Loss ' + str(round(recordvalloss[vallen-1],7))\n",
    "  print('Train Epoch ' + str(trainlen-1) + ' Train Loss ' + str(round(recordtrainloss[trainlen-1],7)) + extrainfo)\n",
    "\n",
    "  SummarizeFullModel(myScienceTransformermodel)\n",
    "\n",
    "  finalanalysis(myScienceTransformermodel,recordtrainloss, recordvalloss,RunComment, False)\n",
    "  return myScienceTransformermodel\n",
    "\n",
    "def finalanalysis(myScienceTransformermodel,recordtrainloss, recordvalloss, LabelFit, summary):\n",
    "    \n",
    "    global UsedTransformervalidationfrac, Transformervalidationfrac, OuterBatchDimension, Nloc_sample, d_sample, max_d_sample\n",
    "    Nloc_sample = Nloc\n",
    "    OuterBatchDimension = Num_Seq\n",
    "    d_sample = Tseq * Nloc\n",
    "    max_d_sample = d_sample\n",
    "\n",
    "    if SymbolicWindows:\n",
    "      finalizeDL(myScienceTransformermodel,recordtrainloss,recordvalloss,UsedTransformervalidationfrac,\n",
    "                  ReshapedSequencesTOT, RawInputPredictionsTOT,1,LabelFit = LabelFit)\n",
    "    else:\n",
    "      finalizeDL(myScienceTransformermodel,recordtrainloss,recordvalloss,UsedTransformervalidationfrac,\n",
    "                  RawInputSequencesTOT, RawInputPredictionsTOT,1,LabelFit = LabelFit)\n",
    "    if summary:\n",
    "      SummarizeFullModel(myScienceTransformermodel)  \n",
    "    if SymbolicWindows:\n",
    "      DLprediction2D(ReshapedSequencesTOT, RawInputPredictionsTOT,myScienceTransformermodel)\n",
    "      DLprediction2B(ReshapedSequencesTOT, RawInputPredictionsTOT,myScienceTransformermodel)\n",
    "      if (not Hydrology):\n",
    "        DLprediction2(ReshapedSequencesTOT, RawInputPredictionsTOT,myScienceTransformermodel)\n",
    "    else:\n",
    "      DLprediction2D(RawInputSequencesTOT, RawInputPredictionsTOT,myScienceTransformermodel)\n",
    "      DLprediction2B(RawInputSequencesTOT, RawInputPredictionsTOT,myScienceTransformermodel)\n",
    "      if (not Hydrology):\n",
    "        DLprediction2(RawInputSequencesTOT, RawInputPredictionsTOT,myScienceTransformermodel)\n",
    "\n",
    "def SummarizeFullModel(DLmodel):\n",
    "  current_time = timenow()\n",
    "  print(startbold + startpurple + '\\n' + current_time + ' Model Weight Summary' + resetfonts)\n",
    "  DLmodel.Scienceencoder.summary()\n",
    "  DLmodel.Sciencemerge.summary()\n",
    "  DLmodel.fullLSTM.summary()\n",
    "  DLmodel.summary()\n",
    "  for ilayer in range(0,num_Encoderlayers):\n",
    "    print(startbold + startpurple +'Encoder Layer ' + str(ilayer) + resetfonts)\n",
    "    DLmodel.Scienceencoder.enc_layers[ilayer].summarize()\n",
    "  OutputDNNpics(DLmodel)\n",
    "  return\n",
    "\n",
    "def OutputDNNpics(DLmodel):\n",
    "  if not OutputNetworkPictures:\n",
    "    return  \n",
    "  outputpicture1 = APPLDIR +'/Outputs/Model_' +RunName + '1.png'\n",
    "  outputpicture2 = APPLDIR +'/Outputs/Model_' +RunName + '2.png'\n",
    "  outputpicture3 = APPLDIR +'/Outputs/Model_' +RunName + '3.png'\n",
    "  outputpicture4 = APPLDIR +'/Outputs/Model_' +RunName + '4.png'\n",
    "  global GlobalSpacetime\n",
    "  save = GlobalSpacetime\n",
    "  GlobalSpacetime = False\n",
    "  Nloc_sample = TrainingNloc\n",
    "  d_sample = Tseq * TrainingNloc\n",
    "  tf.keras.utils.plot_model(DLmodel.build_graph([d_sample,NpropperseqTOT]), \n",
    "                      show_shapes=True, to_file = outputpicture1,\n",
    "                      show_dtype=True, \n",
    "                      expand_nested=True)\n",
    "  tf.keras.utils.plot_model(DLmodel.fullLSTM.build_graph([Tseq,NpropperseqTOT]), \n",
    "                      show_shapes=True, to_file = outputpicture2,\n",
    "                      show_dtype=True, \n",
    "                      expand_nested=True)\n",
    "  tf.keras.utils.plot_model(DLmodel.Scienceencoder.build_graph([Tseq,NpropperseqTOT]), \n",
    "                      show_shapes=True, to_file = outputpicture3,\n",
    "                      show_dtype=True, \n",
    "                      expand_nested=True)\n",
    "  tf.keras.utils.plot_model(DLmodel.Sciencemerge.build_graph([Tseq,NpropperseqTOT]), \n",
    "                      show_shapes=True, to_file = outputpicture4,\n",
    "                      show_dtype=True, \n",
    "                      expand_nested=True)\n",
    "  GlobalSpacetime = save\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQTkE0G4l5jF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set Transformer Launching Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiUc9SgImCjQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Finally we can run the Science Transformer\n",
    "if UseScienceTransformerModel:\n",
    "    AnalysisOnly = True\n",
    "    Restorefromcheckpoint = False\n",
    "    Checkpointfinalstate = True\n",
    "    if AnalysisOnly:\n",
    "      Restorefromcheckpoint = True\n",
    "      Checkpointfinalstate = False\n",
    "    if Restorefromcheckpoint:\n",
    "      inputRunName = RunName\n",
    "    #  inputRunName =  'EARTHQC-Transformer11V'\n",
    "    #  inputCheckpointpostfix = 'MinLoss-110'\n",
    "      inputCheckpointpostfix ='-62'\n",
    "      inputCHECKPOINTDIR = APPLDIR + \"/checkpoints/\" + inputRunName + \"dir/\"\n",
    "\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "    SkipDL2B = False\n",
    "    SkipDL2 = False\n",
    "    SkipDL2D = False\n",
    "    SkipDL2E = False\n",
    "    SkipDL2F = True\n",
    "    Dumpoutkeyplotsaspics = False\n",
    "\n",
    "    SampleSize = 5\n",
    "    Transformerepochs = 100\n",
    "    PredictionTraining = False\n",
    "\n",
    "    batchperepoch = False # if True output a batch bar for each epoch\n",
    "\n",
    "    # Control Training\n",
    "    usecustomfit = True\n",
    "    if AnalysisOnly:\n",
    "      Restorefromcheckpoint = True\n",
    "      Checkpointfinalstate = False\n",
    "\n",
    "    # Repeat key parameters\n",
    "    FullSetValidation = False\n",
    "    TimeShufflingOnly = False\n",
    "    TransformerOnlyFullAttention = False\n",
    "    SpacewiseSecondAttention = True\n",
    "    SeparateHeads = True\n",
    "    MaskingOption = 1\n",
    "    IncreaseNloc_sample = 1\n",
    "    DecreaseNloc_sample = 1\n",
    "    # These are used in fits and DLPrediction2B (calling DLPrediction3) and DLPrediction2D\n",
    "\n",
    "    if MaskingOption == 0:\n",
    "      GlobalSpacetime = False\n",
    "    else: \n",
    "      GlobalSpacetime = True\n",
    "\n",
    "    ChopupMatrix = False\n",
    "    ChopupNumber = 1\n",
    "    ActivateAttention = False\n",
    "\n",
    "    # Values from DGX change sepaarateheads\n",
    "    # Repeat key parameters\n",
    "    FullSetValidation = False\n",
    "    TimeShufflingOnly = False\n",
    "    TransformerOnlyFullAttention = False\n",
    "    SpacewiseSecondAttention = False\n",
    "    SeparateHeads = True  # CHANGED\n",
    "    MaskingOption = 1\n",
    "    IncreaseNloc_sample = 1\n",
    "    DecreaseNloc_sample = 1\n",
    "    # These are used in fits and DLPrediction2B (calling DLPrediction3) and DLPrediction2D\n",
    "\n",
    "    if MaskingOption == 0:\n",
    "        GlobalSpacetime = False\n",
    "    else:\n",
    "        GlobalSpacetime = True\n",
    "\n",
    "    ChopupMatrix = False\n",
    "    ChopupNumber = 1\n",
    "    ActivateAttention = False\n",
    "\n",
    "    num_heads = 4\n",
    "    oldencoderversion = True\n",
    "    ReuseInputinEncoder = True\n",
    "    UseMappedinput = True\n",
    "    Takevasinput = True\n",
    "    d_model = 128\n",
    "    d_Attention = 2 * d_model\n",
    "    if TransformerOnlyFullAttention:\n",
    "        d_Attention = d_model\n",
    "    DoubleQKV = False\n",
    "    d_qk = d_model\n",
    "    d_intermediateqk = 2 * d_model\n",
    "    d_intermediatev = 2 * d_model\n",
    "    d_v = d_model\n",
    "    if DoubleQKV:\n",
    "        d_Attention = 2 * d_Attention\n",
    "        d_qk = 2 * d_model\n",
    "    depth = d_qk // num_heads\n",
    "    if not Takevasinput:\n",
    "        d_v = depth\n",
    "\n",
    "    num_Encoderlayers = 4\n",
    "    EncoderActivation = \"selu\"\n",
    "    d_EncoderLayer = d_Attention\n",
    "    d_ffn = 4 * d_model\n",
    "    d_merge = 2 * d_model\n",
    "    d_merge = d_model\n",
    "\n",
    "    LSTMSkipInitial = True\n",
    "    number_LSTMnodes = 64\n",
    "    LSTMFinalMLP = 128\n",
    "    LSTMInitialMLP = 128\n",
    "    LSTMThirdLayer = False\n",
    "\n",
    "    dropvalue = 0.2\n",
    "    LSTMdropout1 = dropvalue\n",
    "    LSTMrecurrent_dropout1 = dropvalue\n",
    "    LSTMdropout2 = dropvalue\n",
    "    LSTMrecurrent_dropout2 = dropvalue\n",
    "    EncoderDropout = dropvalue\n",
    "    processindex = 0\n",
    "    UsedTransformervalidationfrac = 0.0\n",
    "    Transformerbatch_size = 1\n",
    "\n",
    "    TransformerTFMonitor = TensorFlowTrainingMonitor()\n",
    "    if Hydrology:\n",
    "      TransformerTFMonitor.SetControlParms(SuccessLimit = 1,FailureLimit = 2)\n",
    "    if Earthquake:\n",
    "      TransformerTFMonitor.SetControlParms(SuccessLimit = 1,FailureLimit = 2)\n",
    "    if ReadJan2021Covid:\n",
    "      TransformerTFMonitor.SetControlParms(SuccessLimit = 5,FailureLimit = 2)\n",
    "    if ReadApril2021Covid:\n",
    "      TransformerTFMonitor.SetControlParms(SuccessLimit = 1,FailureLimit = 2)\n",
    "    Mymodel = RunScienceTransformer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEaGePMdYXwf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Output parameters used   this Science Transformer\n",
    "if UseScienceTransformerModel:\n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + current_time + ' ' +RunComment + ' ' + RunName + resetfonts)\n",
    "  print(\"TimeShufflingOnly: if False shuffle over space and Time each batch; True just shuffle over time \", str(TimeShufflingOnly))\n",
    "  print(\"Batch size of stochastic gradient descent Transformerbatch_size \", str(Transformerbatch_size))\n",
    "  if not LocationBasedValidation:\n",
    "    print(startbold  + startred + \"Fraction used for a validation dataset \", str(Transformervalidationfrac),\" No location based Validation\"+resetfonts)\n",
    "  print('FullSetValidation ' + str(FullSetValidation))\n",
    "  print('Sample size IncreaseNloc_sample ' + str(IncreaseNloc_sample) + ' DecreaseNloc_sample ' + str(DecreaseNloc_sample))\n",
    "  print(\"Dimension of value embedding for every input [Model1] d_model \", str(d_model))\n",
    "  print(\"Dimension of value embedding for input to Decoder (LSTM) d_merge \", str(d_merge))\n",
    "  print(\"Number of Attention Heads which must exactly divide d_model, num_heads \", str(num_heads))\n",
    "  print(\"Number of layers in Encoder stage num_Encoderlayers \", str(num_Encoderlayers))\n",
    "  print(\"Dropout in Encoder stage, EncoderDropout \", str(EncoderDropout))\n",
    "  print(\"Size of Encoder stage, d_EncoderLayer \", str(d_EncoderLayer))\n",
    "  print(\"Activation in Encoder Stage EncoderActivation \",str(EncoderActivation))\n",
    "  print(\"Size of feedforward network in each encoder layer. It appears to be 4 * d_model, d_ffn \", str(d_ffn))\n",
    "  print('d_Attention ' +str(d_Attention) + ' typically 2*d_model unless only one attention')\n",
    "  print('Size of Q K V d_qk ' + str(d_qk))\n",
    "  print('Double d_qk and d_attention ' + str(DoubleQKV))\n",
    "  print('d_intermediateqk ' + str(d_intermediateqk) + ' Typically 2 * d_model ')\n",
    "  print(startbold  + startred + \"Defines masking used; = 0 none; =1 mask the future MaskingOption \", str(MaskingOption) + resetfonts)\n",
    "  print(startbold + 'Nature of Attention ' + resetfonts)\n",
    "  print(' Are heads done sequentially SeparateHeads ' + str(SeparateHeads))\n",
    "  print(' TransformerOnlyFullAttention ' + str(TransformerOnlyFullAttention))\n",
    "  print(' SpacewiseSecondAttention First attention is Time in sequence Second is Space or Full ' + str(SpacewiseSecondAttention))\n",
    "  print(' ChopupMatrix ' + str(ChopupMatrix) + ' ChopupNumber ' +str(ChopupNumber))\n",
    "  print(' ActivateAttention ' + str(ActivateAttention))\n",
    "  print(' ReuseInputinEncoder ' + str(ReuseInputinEncoder))\n",
    "  print('\\n')\n",
    "  print(startbold + startred + current_time + ' ' + RunComment + ' ' + RunName + resetfonts)\n",
    "  PrintLSTMandBasicStuff()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMIeYApPybFA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TFT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkiL51xZ3XUr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Set up TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGBQUTmZgSSV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not UseTFTModel:\n",
    "  printexit('TFT Follows so Stopping')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqBfMQNwQj1z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Data and Input Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tUyF2ZhQ3-C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Type defintions\n",
    "import enum\n",
    "\n",
    "class DataTypes(enum.IntEnum):\n",
    "  \"\"\"Defines numerical types of each column.\"\"\"\n",
    "  REAL_VALUED = 0\n",
    "  CATEGORICAL = 1\n",
    "  DATE = 2\n",
    "  NULL = -1\n",
    "  STRING = 3\n",
    "  BOOL = 4\n",
    "\n",
    "class InputTypes(enum.IntEnum):\n",
    "  \"\"\"Defines input types of each column.\"\"\"\n",
    "  TARGET = 0 # Known before and after t for training\n",
    "  OBSERVED_INPUT = 1 # Known upto time t\n",
    "  KNOWN_INPUT = 2 # Known at all times\n",
    "  STATIC_INPUT = 3 # By definition known at all times\n",
    "  ID = 4  # Single column used as an entity identifier\n",
    "  TIME = 5  # Single column exclusively used as a time index\n",
    "  NULL = -1\n",
    "\n",
    "def checkdfNaN(label, AttributeSpec, y):\n",
    "  countNaN = 0\n",
    "  countnotNaN = 0\n",
    "  if y is None:\n",
    "    return\n",
    "  names = y.columns.tolist()\n",
    "  count = np.zeros(y.shape[1])\n",
    "  for j in range(0,y.shape[1]):\n",
    "    colname = names[j]\n",
    "    if AttributeSpec.loc[colname,'DataTypes'] != DataTypes.REAL_VALUED:\n",
    "      continue\n",
    "    for i in range(0,y.shape[0]):\n",
    "          if(np.math.isnan(y.iloc[i,j])):\n",
    "              countNaN += 1\n",
    "              count[j] += 1\n",
    "          else:\n",
    "              countnotNaN += 1\n",
    "\n",
    "  percent = (100.0*countNaN)/(countNaN + countnotNaN)\n",
    "  print(label + ' is NaN ',str(countNaN),' percent ',str(round(percent,2)),' not NaN ', str(countnotNaN))\n",
    "  for j in range(0,y.shape[1]):\n",
    "    if count[j] == 0:\n",
    "      continue\n",
    "    print(names[j] + ' has NaN ' + str(count[j]))\n",
    "\n",
    "excludeNULLtype = True\n",
    "TFTexcludedinputtypes = {InputTypes.ID, InputTypes.TIME}\n",
    "if excludeNULLtype:\n",
    "  TFTexcludedinputtypes = {InputTypes.ID, InputTypes.TIME, InputTypes.NULL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOXCqWirQxZb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Convert FFFFWNPF to TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98Wz3J3y3b2z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "if UseTFTModel:\n",
    "# Pick Values setting InputType\n",
    "# Currently ONLY pick from properties BUT\n",
    "# If PropPick = 0 (target) then these should be selected as predictions in FFFFWNPF and futured of length LengthFutures\n",
    "\n",
    "# Set Prediction Property mappings and calculations\n",
    "\n",
    "# PredictionTFTAction -2 a Future -1 Ignore 0 Futured Basic Prediction, 1 Nonfutured Simple Sum, 2 Nonfutured Energy Averaged Earthquake\n",
    "# CalculatedPredmaptoRaw is Raw Prediction on which Calculated Prediction based\n",
    "# PredictionCalcLength is >1 if Action=1,2 and says action based on this number of consequitive predictions\n",
    "# PredictionTFTnamemapping if a non trivial string it is that returned by TFT in output map; if ' ' it isd a special extra prediction\n",
    "\n",
    "  PredictionTFTnamemapping =np.full(NpredperseqTOT,' ',dtype=object)\n",
    "  PredictionTFTAction = np.full(NpredperseqTOT, -1, dtype = np.int32)\n",
    "  for ipred in range(0,NpredperseqTOT):\n",
    "    if ipred >= NumpredbasicperTime:\n",
    "      PredictionTFTAction[ipred] = -2\n",
    "    elif  FuturedPointer[ipred] >= 0:\n",
    "      PredictionTFTAction[ipred] = 0\n",
    "      # Default is -1 \n",
    "\n",
    "\n",
    "  CalculatedPredmaptoRaw = np.full(NpredperseqTOT, -1, dtype = np.int32)\n",
    "  PredictionCalcLength = np.full(NpredperseqTOT, 1, dtype = np.int32)\n",
    "  \n",
    "# TFT Pick flags\n",
    "# 0 Target and observed input\n",
    "# 1 Observed Input NOT predicted\n",
    "# 2 Known Input\n",
    "# 3 Static Observed Input\n",
    "#\n",
    "# Data Types 0 Float or Integer converted to Float\n",
    "\n",
    "  if ReadApril2021Covid:\n",
    "    if ReadNov2021Covid:\n",
    "      PropPick = [0,0,3,3,3,3,1,3,3,1,3,1,1,1,3,1,2,2,2,2,2,2,2,2]\n",
    "    else:\n",
    "      PropPick = [0,0,3,3,3,3,1,3,3,1,3,1,1,1,3,2,2,2,2,2,2,2,2]\n",
    "    PropDataType = [0] * NpropperseqTOT\n",
    "  if Earthquake: # Assuming Special non futured 6 months forward prediction defined but NOT directly predicted by TFT\n",
    "    PropPick = [3,3,3,3,0,1,1,1,1,1,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2]\n",
    "    PropDataType = [0] * NpropperseqTOT\n",
    "\n",
    "\n",
    "# Dataframe is overall label (real starting at 0), Location Name, Time Input Properties, Predicted Properties Nloc times Num_Time values\n",
    "# Row major order in Location-Time Space\n",
    "  Totalsize = (Num_Time + TFTExtraTimes) * Nloc\n",
    "  RawLabel = np.arange(0, Totalsize, dtype =np.float32)\n",
    "  LocationLabel = []\n",
    "  FFFFWNPFUniqueLabel = []\n",
    "  RawTime = np.empty([Nloc,Num_Time + TFTExtraTimes], dtype = np.float32)\n",
    "  RawTrain = np.full([Nloc,Num_Time + TFTExtraTimes], True, dtype = bool)\n",
    "  RawVal = np.full([Nloc,Num_Time + TFTExtraTimes], True, dtype = bool)\n",
    "#  print('Times ' + str(Num_Time) + ' ' + str(TFTExtraTimes))\n",
    "  ierror = 0\n",
    "  for ilocation in range(0,Nloc):\n",
    "#   locname = Locationstate[LookupLocations[ilocation]] + ' ' + Locationname[LookupLocations[ilocation]] \n",
    "    locname = Locationname[LookupLocations[ilocation]]  + ' ' + Locationstate[LookupLocations[ilocation]]\n",
    "    if locname == \"\":\n",
    "      printexit('Illegal null location name ' + str(ilocation))\n",
    "    for idupe in range(0,len(FFFFWNPFUniqueLabel)):\n",
    "      if locname == FFFFWNPFUniqueLabel[idupe]:\n",
    "        print(' Duplicate location name ' + str(ilocation) + ' ' + str(idupe) + ' ' + locname)\n",
    "        ierror += 1\n",
    "    FFFFWNPFUniqueLabel.append(locname)\n",
    "#    print(str(ilocation) + ' ' +locname)\n",
    "    for jtime in range(0,Num_Time + TFTExtraTimes):\n",
    "      RawTime[ilocation,jtime] = np.float32(jtime)\n",
    "      LocationLabel.append(locname)\n",
    "      if LocationBasedValidation:\n",
    "        if MappingtoTraining[ilocation] >= 0:\n",
    "          RawTrain[ilocation,jtime] = True\n",
    "        else:\n",
    "          RawTrain[ilocation,jtime] = False\n",
    "        if MappingtoValidation[ilocation] >= 0:\n",
    "          RawVal[ilocation,jtime] = True\n",
    "        else:\n",
    "          RawVal[ilocation,jtime] = False\n",
    "\n",
    "  if ierror > 0:\n",
    "    printexit(\" Duplicate Names \" + str(ierror))\n",
    "\n",
    "  RawTime = np.reshape(RawTime,-1)\n",
    "  RawTrain = np.reshape(RawTrain,-1)\n",
    "  RawVal = np.reshape(RawVal,-1)\n",
    "  TFTdf1 = pd.DataFrame(RawLabel, columns=['RawLabel'])\n",
    "  if LocationBasedValidation:\n",
    "    TFTdf2 = pd.DataFrame(RawTrain, columns=['TrainingSet'])\n",
    "    TFTdf3 = pd.DataFrame(RawVal, columns=['ValidationSet'])\n",
    "    TFTdf4 = pd.DataFrame(LocationLabel, columns=['Location'])\n",
    "    TFTdf5 = pd.DataFrame(RawTime, columns=['Time from Start'])\n",
    "    TFTdfTotal = pd.concat([TFTdf1,TFTdf2,TFTdf3,TFTdf4,TFTdf5], axis=1)\n",
    "  else:\n",
    "    TFTdf2 = pd.DataFrame(LocationLabel, columns=['Location'])\n",
    "    TFTdf3 = pd.DataFrame(RawTime, columns=['Time from Start'])\n",
    "    TFTdfTotal = pd.concat([TFTdf1,TFTdf2,TFTdf3], axis=1)\n",
    "  TFTdfTotalSpec = pd.DataFrame([['RawLabel', DataTypes.REAL_VALUED, InputTypes.NULL]], columns=['AttributeName', 'DataTypes', 'InputTypes'])\n",
    "  if LocationBasedValidation:\n",
    "    TFTdfTotalSpec.loc[len(TFTdfTotalSpec.index)] = ['TrainingSet', DataTypes.BOOL, InputTypes.NULL]\n",
    "    TFTdfTotalSpec.loc[len(TFTdfTotalSpec.index)] = ['ValidationSet', DataTypes.BOOL, InputTypes.NULL]\n",
    "  TFTdfTotalSpec.loc[len(TFTdfTotalSpec.index)] = ['Location', DataTypes.STRING, InputTypes.ID]\n",
    "  TFTdfTotalSpec.loc[len(TFTdfTotalSpec.index)]  = ['Time from Start', DataTypes.REAL_VALUED, InputTypes.TIME] \n",
    "  \n",
    "\n",
    "  ColumnsProp=[]\n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    line = str(iprop) + ' ' + InputPropertyNames[PropertyNameIndex[iprop]]  \n",
    "    jprop = PropertyAverageValuesPointer[iprop]\n",
    "    if QuantityTakeroot[jprop] > 1:\n",
    "      line += ' Root ' + str(QuantityTakeroot[jprop])\n",
    "    ColumnsProp.append(line)\n",
    "\n",
    "  QuantityStatisticsNames = ['Min','Max','Norm','Mean','Std','Normed Mean','Normed Std']\n",
    "  TFTInputSequences = np.reshape(ReshapedSequencesTOT,(-1,NpropperseqTOT))\n",
    "  TFTPropertyChoice = np.full(NpropperseqTOT, -1, dtype = np.int32)\n",
    "  TFTNumberTargets = 0\n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    if PropPick[iprop] >= 0:\n",
    "      if PropPick[iprop] == 0:\n",
    "        TFTNumberTargets += 1\n",
    "      nextcol = TFTInputSequences[:,iprop]\n",
    "      dftemp = pd.DataFrame(nextcol, columns=[ColumnsProp[iprop]])\n",
    "      TFTdfTotal = pd.concat([TFTdfTotal,dftemp], axis=1)\n",
    "      jprop = TFTdfTotal.columns.get_loc(ColumnsProp[iprop])\n",
    "      print('Property column ' + str(jprop) + ' ' + ColumnsProp[iprop])\n",
    "      TFTPropertyChoice[iprop] = jprop\n",
    "      TFTdfTotalSpec.loc[len(TFTdfTotalSpec.index)] = [ColumnsProp[iprop], PropDataType[iprop], PropPick[iprop]]\n",
    "  FFFFWNPFNumberTargets = TFTNumberTargets\n",
    "\n",
    "  ReshapedPredictionsTOT = np.transpose(RawInputPredictionsTOT,(1,0,2))\n",
    "\n",
    "  TFTdfTotalSpec = TFTdfTotalSpec.set_index('AttributeName', drop= False)\n",
    "  TFTdfTotalshape = TFTdfTotal.shape\n",
    "  TFTdfTotalcols = TFTdfTotal.columns\n",
    "  print('TFTdfTotalSpec.shape ' + str(TFTdfTotalSpec.shape))\n",
    "  print('TFTdfTotal.shape ' + str(TFTdfTotalshape))\n",
    "  print('TFTdfTotal.columns ' + str(TFTdfTotalcols))\n",
    "  print('TFTdfTotalSpec.shape ' + str(TFTdfTotalSpec.shape))\n",
    "  print('TFTdfTotalSpec.columns ' + str(TFTdfTotalSpec.columns))\n",
    "  pd.set_option('display.max_rows', 100)\n",
    "  print('Display TFTdfTotalSpec')\n",
    "  display(TFTdfTotalSpec)\n",
    "\n",
    "\n",
    "  print('Prediction mapping')\n",
    "\n",
    "  ifuture = 0\n",
    "  itarget = 0\n",
    "  for ipred in range(0,NpredperseqTOT):\n",
    "    predstatus = PredictionTFTAction[ipred]\n",
    "    if (predstatus == -1) or (predstatus > 0):\n",
    "      PredictionTFTnamemapping[ipred] = ' ' \n",
    "      text = 'NOT PREDICTED DIRECTLY'  \n",
    "    elif (predstatus == -2) or (predstatus == 0):\n",
    "      text = 't+{}-Obs{}'.format(ifuture,itarget)\n",
    "      PredictionTFTnamemapping[ipred] = text\n",
    "      itarget += 1\n",
    "      if itarget >= TFTNumberTargets:\n",
    "        itarget = 0\n",
    "        ifuture += 1\n",
    "    fp = -2\n",
    "    if ipred < NumpredbasicperTime:\n",
    "      fp = FuturedPointer[ipred]\n",
    "    line = startbold + startpurple + str(ipred) + ' ' + Predictionname[PredictionNameIndex[ipred]] + ' ' + text + resetfonts + ' Futured ' +str(fp) + ' '\n",
    "    line += 'Action ' + str(predstatus) + ' Property ' + str(CalculatedPredmaptoRaw[ipred]) + ' Length ' + str(PredictionCalcLength[ipred])\n",
    "    jpred = PredictionAverageValuesPointer[ipred]\n",
    "    line += ' Processing Root ' + str(QuantityTakeroot[jpred])\n",
    "    for proppredval in range (0,7):\n",
    "      line += ' ' + QuantityStatisticsNames[proppredval] + ' ' + str(round(QuantityStatistics[jpred,proppredval],3))\n",
    "    print(wraptotext(line,size=150))\n",
    "\n",
    "# Rescaling done by that appropriate for properties and predictions\n",
    "  TFTdfTotalSpecshape = TFTdfTotalSpec.shape\n",
    "  TFTcolumn_definition = []\n",
    "  print(' ')\n",
    "  print('LIST TFTcolumn_definition')\n",
    "  for i in range(0,TFTdfTotalSpecshape[0]):\n",
    "    TFTcolumn_definition.append((TFTdfTotalSpec.iloc[i,0],TFTdfTotalSpec.iloc[i,1],TFTdfTotalSpec.iloc[i,2]))\n",
    "    print(TFTcolumn_definition[i])\n",
    "  print(' ')\n",
    "  print('Names of Columns of TFTdfTotalSpec i.e. TFTdfTotalSpec.columns')\n",
    "  print(TFTdfTotalSpec.columns)\n",
    "  print(' ')\n",
    "  print('TFTdfTotalSpec.index')\n",
    "  print(TFTdfTotalSpec.index)\n",
    "\n",
    "# Set Futures to be calculated\n",
    "  if Earthquake:\n",
    "    PlotFutures = np.full(1+LengthFutures,False, dtype=np.bool)\n",
    "    PlotFutures[0] = True\n",
    "    PlotFutures[6] = True\n",
    "    PlotFutures[12] = True\n",
    "    PlotFutures[25] = True\n",
    "    PredictedQuantity = -NumpredbasicperTime\n",
    "    for ifuture in range (0,1+LengthFutures):\n",
    "      increment = NumpredbasicperTime\n",
    "      if ifuture > 1:\n",
    "        increment = NumpredFuturedperTime\n",
    "      PredictedQuantity += increment\n",
    "      for j in range(0,increment):\n",
    "        PlotPredictions[PredictedQuantity+j] = PlotFutures[ifuture]\n",
    "        CalculateNNSE[PredictedQuantity+j] = PlotFutures[ifuture]\n",
    "  else:\n",
    "    PlotFutures = np.full(1+LengthFutures,True, dtype=np.bool)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbNT-soy5zkY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###TFT Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVkN0Mn9526c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DLAnalysisOnly = False\n",
    "DLRestorefromcheckpoint = False\n",
    "DLinputRunName = RunName\n",
    "DLinputRunName = 'EARTHQ-newTFTv28'\n",
    "DLinputCheckpointpostfix = '-67'\n",
    "\n",
    "TFTdropout_rate = 0.1\n",
    "TFTTransformerbatch_size = 64\n",
    "TFTTransformerepochs = 40\n",
    "TFTd_model = 160\n",
    "TFTTransformertestvalbatch_size = max(128,TFTTransformerbatch_size)\n",
    "TFThidden_layer_size = TFTd_model\n",
    "\n",
    "number_LSTMnodes = TFTd_model\n",
    "LSTMactivationvalue = 'tanh'\n",
    "LSTMrecurrent_activation = 'sigmoid'\n",
    "LSTMdropout1 = 0.0\n",
    "LSTMrecurrent_dropout1 = 0.0\n",
    "\n",
    "TFTLSTMEncoderInitialMLP = 0\n",
    "TFTLSTMDecoderInitialMLP = 0\n",
    "TFTLSTMEncoderrecurrent_dropout1 = LSTMrecurrent_dropout1\n",
    "TFTLSTMDecoderrecurrent_dropout1 = LSTMrecurrent_dropout1\n",
    "TFTLSTMEncoderdropout1 = LSTMdropout1\n",
    "TFTLSTMDecoderdropout1 = LSTMdropout1\n",
    "TFTLSTMEncoderrecurrent_activation = LSTMrecurrent_activation\n",
    "TFTLSTMDecoderrecurrent_activation = LSTMrecurrent_activation\n",
    "TFTLSTMEncoderactivationvalue = LSTMactivationvalue\n",
    "TFTLSTMDecoderactivationvalue = LSTMactivationvalue\n",
    "TFTLSTMEncoderSecondLayer = True\n",
    "TFTLSTMDecoderSecondLayer = True\n",
    "TFTLSTMEncoderThirdLayer = False\n",
    "TFTLSTMDecoderThirdLayer = False\n",
    "TFTLSTMEncoderFinalMLP = 0\n",
    "TFTLSTMDecoderFinalMLP = 0\n",
    "\n",
    "TFTnum_heads = 4\n",
    "TFTnum_AttentionLayers = 2\n",
    "\n",
    "# For default TFT\n",
    "TFTuseCUDALSTM = True\n",
    "TFTdefaultLSTM = False\n",
    "if TFTdefaultLSTM:\n",
    "  TFTuseCUDALSTM = True\n",
    "  TFTLSTMEncoderFinalMLP = 0\n",
    "  TFTLSTMDecoderFinalMLP = 0\n",
    "  TFTLSTMEncoderrecurrent_dropout1 = 0.0\n",
    "  TFTLSTMDecoderrecurrent_dropout1 = 0.0\n",
    "  TFTLSTMEncoderdropout1 = 0.0\n",
    "  TFTLSTMDecoderdropout1 = 0.0\n",
    "  TFTLSTMEncoderSecondLayer = False\n",
    "  TFTLSTMDecoderSecondLayer = False\n",
    "\n",
    "TFTFutures = 0\n",
    "if ReadApril2021Covid:\n",
    "  TFTFutures = 1 + LengthFutures\n",
    "if Earthquake:\n",
    "  TFTFutures = 1 + LengthFutures\n",
    "if TFTFutures == 0:\n",
    "  printexit('No TFT Futures defined')\n",
    "\n",
    "TFTSingleQuantity = True\n",
    "TFTLossFlag = 0\n",
    "HuberLosscut = 0.01\n",
    "\n",
    "if TFTSingleQuantity:\n",
    "  TFTQuantiles =[1.0]\n",
    "  TFTQuantilenames = ['MSE']\n",
    "  TFTPrimaryQuantileIndex = 0\n",
    "else:\n",
    "  TFTQuantiles = [0.1,0.5,0.9]\n",
    "  TFTQuantilenames = ['p10','p50','p90']\n",
    "  TFTPrimaryQuantileIndex = 1 \n",
    "if TFTLossFlag == 11:\n",
    "  TFTQuantilenames = ['MAE']\n",
    "if TFTLossFlag == 12:\n",
    "  TFTQuantilenames = ['Huber']\n",
    "TFTfixed_params = {\n",
    "        'total_time_steps': Tseq + TFTFutures,\n",
    "        'num_encoder_steps': Tseq,\n",
    "        'num_epochs': TFTTransformerepochs,\n",
    "        'early_stopping_patience': 60,\n",
    "        'multiprocessing_workers': 12,\n",
    "        'optimizer': 'adam',\n",
    "        'lossflag': TFTLossFlag,\n",
    "        'HuberLosscut': HuberLosscut,\n",
    "        'AnalysisOnly': DLAnalysisOnly,\n",
    "        'inputRunName': DLinputRunName,\n",
    "        'Restorefromcheckpoint': DLRestorefromcheckpoint,\n",
    "        'inputCheckpointpostfix': DLinputCheckpointpostfix,\n",
    "        'maxibatch_size': TFTTransformertestvalbatch_size,\n",
    "        'TFTuseCUDALSTM':TFTuseCUDALSTM,\n",
    "        'TFTdefaultLSTM':TFTdefaultLSTM,\n",
    "}\n",
    "\n",
    "TFTmodel_params = {\n",
    "        'dropout_rate': TFTdropout_rate,\n",
    "        'hidden_layer_size': TFTd_model,\n",
    "        'learning_rate': 0.00001,\n",
    "        'minibatch_size': TFTTransformerbatch_size,\n",
    "        'max_gradient_norm': 0.01,\n",
    "        'num_heads': TFTnum_heads,\n",
    "        'stack_size': TFTnum_AttentionLayers,\n",
    "}\n",
    "\n",
    "TFTSymbolicWindows = False\n",
    "TFTFinalGatingOption = 1\n",
    "TFTMultivariate = True\n",
    "\n",
    "TFTuse_testing_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBMkPML6MXY7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Base Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWusLYsSMdX9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class GenericDataFormatter(abc.ABC):\n",
    "  \"\"\"Abstract base class for all data formatters.\n",
    "\n",
    "  User can implement the abstract methods below to perform dataset-specific\n",
    "  manipulations.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def set_scalers(self, df):\n",
    "    \"\"\"Calibrates scalers using the data supplied.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def transform_inputs(self, df):\n",
    "    \"\"\"Performs feature transformation.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def format_predictions(self, df):\n",
    "    \"\"\"Reverts any normalisation to give predictions in original scale.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def split_data(self, df):\n",
    "    \"\"\"Performs the default train, validation and test splits.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @property\n",
    "  @abc.abstractmethod\n",
    "  def _column_definition(self):\n",
    "    \"\"\"Defines order, input type and data type of each column.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def get_fixed_params(self):\n",
    "    \"\"\"Defines the fixed parameters used by the model for training.\n",
    "\n",
    "    Requires the following keys:\n",
    "      'total_time_steps': Defines the total number of time steps used by TFT\n",
    "      'num_encoder_steps': Determines length of LSTM encoder (i.e. history)\n",
    "      'num_epochs': Maximum number of epochs for training\n",
    "      'early_stopping_patience': Early stopping param for keras\n",
    "      'multiprocessing_workers': # of cpus for data processing\n",
    "\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of fixed parameters, e.g.:\n",
    "\n",
    "      fixed_params = {\n",
    "          'total_time_steps': 252 + 5,\n",
    "          'num_encoder_steps': 252,\n",
    "          'num_epochs': 100,\n",
    "          'early_stopping_patience': 5,\n",
    "          'multiprocessing_workers': 5,\n",
    "      }\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "  # Shared functions across data-formatters\n",
    "  @property\n",
    "  def num_classes_per_cat_input(self):\n",
    "    \"\"\"Returns number of categories per relevant input.\n",
    "\n",
    "    This is seqeuently required for keras embedding layers.\n",
    "    \"\"\"\n",
    "    return self._num_classes_per_cat_input\n",
    "\n",
    "  def get_num_samples_for_calibration(self):\n",
    "    \"\"\"Gets the default number of training and validation samples.\n",
    "\n",
    "    Use to sub-sample the data for network calibration and a value of -1 uses\n",
    "    all available samples.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (training samples, validation samples)\n",
    "    \"\"\"\n",
    "    return -1, -1\n",
    "\n",
    "  def get_column_definition(self):\n",
    "    \"\"\"\"Returns formatted column definition in order expected by the TFT.\"\"\"\n",
    "\n",
    "    column_definition = self._column_definition\n",
    "\n",
    "    # Sanity checks first.\n",
    "    # Ensure only one ID and time column exist\n",
    "    def _check_single_column(input_type):\n",
    "\n",
    "      length = len([tup for tup in column_definition if tup[2] == input_type])\n",
    "\n",
    "      if length != 1:\n",
    "        raise ValueError('Illegal number of inputs ({}) of type {}'.format(\n",
    "            length, input_type))\n",
    "\n",
    "    _check_single_column(InputTypes.ID)\n",
    "    _check_single_column(InputTypes.TIME)\n",
    "\n",
    "    identifier = [tup for tup in column_definition if tup[2] == InputTypes.ID]\n",
    "    time = [tup for tup in column_definition if tup[2] == InputTypes.TIME]\n",
    "    real_inputs = [\n",
    "        tup for tup in column_definition if tup[1] == DataTypes.REAL_VALUED and\n",
    "        tup[2] not in TFTexcludedinputtypes\n",
    "    ]\n",
    "    categorical_inputs = [\n",
    "        tup for tup in column_definition if tup[1] == DataTypes.CATEGORICAL and\n",
    "        tup[2] not in TFTexcludedinputtypes\n",
    "    ]\n",
    "\n",
    "    return identifier + time + real_inputs + categorical_inputs\n",
    "#  XXX Looks important in reordering\n",
    "\n",
    "  def _get_input_columns(self):\n",
    "    \"\"\"Returns names of all input columns.\"\"\"\n",
    "    return [\n",
    "        tup[0]\n",
    "        for tup in self.get_column_definition()\n",
    "        if tup[2] not in TFTexcludedinputtypes\n",
    "    ]\n",
    "\n",
    "  def _get_tft_input_indices(self):\n",
    "    \"\"\"Returns the relevant indexes and input sizes required by TFT.\"\"\"\n",
    "\n",
    "    # Functions\n",
    "    def _extract_tuples_from_data_type(data_type, defn):\n",
    "      return [\n",
    "          tup for tup in defn if tup[1] == data_type and\n",
    "          tup[2] not in TFTexcludedinputtypes\n",
    "      ]\n",
    "\n",
    "    def _get_locations(input_types, defn):\n",
    "      return [i for i, tup in enumerate(defn) if tup[2] in input_types]\n",
    "\n",
    "    # Start extraction\n",
    "    column_definition = [\n",
    "        tup for tup in self.get_column_definition()\n",
    "        if tup[2] not in TFTexcludedinputtypes\n",
    "    ]\n",
    "    print('column_definition ' + str(column_definition))\n",
    "\n",
    "    categorical_inputs = _extract_tuples_from_data_type(DataTypes.CATEGORICAL,\n",
    "                                                        column_definition)\n",
    "    real_inputs = _extract_tuples_from_data_type(DataTypes.REAL_VALUED,\n",
    "                                                 column_definition)\n",
    "\n",
    "    locations = {\n",
    "        'input_size':\n",
    "            len(self._get_input_columns()),\n",
    "        'output_size':\n",
    "            len(_get_locations({InputTypes.TARGET}, column_definition)),\n",
    "        'category_counts':\n",
    "            self.num_classes_per_cat_input,\n",
    "        'input_obs_loc':\n",
    "            _get_locations({InputTypes.TARGET}, column_definition),\n",
    "        'static_input_loc':\n",
    "            _get_locations({InputTypes.STATIC_INPUT}, column_definition),\n",
    "        'known_regular_inputs':\n",
    "            _get_locations({InputTypes.STATIC_INPUT, InputTypes.KNOWN_INPUT},\n",
    "                           real_inputs),\n",
    "        'known_categorical_inputs':\n",
    "            _get_locations({InputTypes.STATIC_INPUT, InputTypes.KNOWN_INPUT},\n",
    "                           categorical_inputs),\n",
    "    }\n",
    "    print('locations Dictionary')\n",
    "    for key, value in locations.items():\n",
    "      print(key, ' : ', value)\n",
    "\n",
    "    return locations\n",
    "\n",
    "  def get_experiment_params(self):\n",
    "    \"\"\"Returns fixed model parameters for experiments.\"\"\"\n",
    "\n",
    "    required_keys = [\n",
    "        'total_time_steps', 'num_encoder_steps', 'num_epochs',\n",
    "        'early_stopping_patience', 'multiprocessing_workers'\n",
    "    ]\n",
    "\n",
    "    fixed_params = self.get_fixed_params()\n",
    "\n",
    "    for k in required_keys:\n",
    "      if k not in fixed_params:\n",
    "        raise ValueError('Field {}'.format(k) +\n",
    "                         ' missing from fixed parameter definitions!')\n",
    "\n",
    "    fixed_params['column_definition'] = self.get_column_definition()\n",
    "\n",
    "    fixed_params.update(self._get_tft_input_indices())\n",
    "\n",
    "    return fixed_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-k-se9TA9M2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###TFT FFFFWNPF Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRZ5qaEdBKzm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Custom formatting functions for FFFFWNPF datasets.\n",
    "\n",
    "#GenericDataFormatter = data_formatters.base.GenericDataFormatter\n",
    "#DataTypes = data_formatters.base.DataTypes\n",
    "#InputTypes = data_formatters.base.InputTypes\n",
    "\n",
    "\n",
    "class FFFFWNPFFormatter(GenericDataFormatter):\n",
    "  \"\"\"\n",
    "  Defines and formats data for the Covid April 21 dataset.\n",
    "  Attributes:\n",
    "    column_definition: Defines input and data type of column used in the\n",
    "      experiment.\n",
    "    identifiers: Entity identifiers used in experiments.\n",
    "  \"\"\"\n",
    "\n",
    "  _column_definition = TFTcolumn_definition\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"Initialises formatter.\"\"\"\n",
    "\n",
    "    self.identifiers = None\n",
    "    self._real_scalers = None\n",
    "    self._cat_scalers = None\n",
    "    self._target_scaler = None\n",
    "    self._num_classes_per_cat_input = []\n",
    "    self._time_steps = self.get_fixed_params()['total_time_steps']\n",
    "\n",
    "\n",
    "  def split_data(self, df, valid_boundary=-1, test_boundary=-1):\n",
    "    \"\"\"Splits data frame into training-validation-test data frames.\n",
    "\n",
    "    This also calibrates scaling object, and transforms data for each split.\n",
    "\n",
    "    Args:\n",
    "      df: Source data frame to split.\n",
    "      valid_boundary: Starting time for validation data\n",
    "      test_boundary: Starting time for test data\n",
    "\n",
    "    Returns:\n",
    "      Tuple of transformed (train, valid, test) data.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Formatting train-valid-test splits.')\n",
    "\n",
    "    if LocationBasedValidation:\n",
    "      index = df['TrainingSet']\n",
    "      train = df[index == True]\n",
    "      index = df['ValidationSet']\n",
    "      valid = df[index == True]\n",
    "      index = train['Time from Start']\n",
    "      train = train[index<(Num_Time-0.5)]\n",
    "      index = valid['Time from Start']\n",
    "      valid = valid[index<(Num_Time-0.5)]\n",
    "      if test_boundary == -1:\n",
    "        test = df\n",
    "#      train.drop('TrainingSet', axis=1, inplace=True)\n",
    "#      train.drop('ValidationSet', axis=1, inplace=True)\n",
    "#     valid.drop('TrainingSet', axis=1, inplace=True)\n",
    "#     valid.drop('ValidationSet', axis=1, inplace=True)\n",
    "    else:\n",
    "      index = df['Time from Start']\n",
    "      train = df[index<(Num_Time-0.5)]\n",
    "      valid = df[index<(Num_Time-0.5)]\n",
    "      if test_boundary == -1:\n",
    "        test = df\n",
    "\n",
    "    if valid_boundary > 0:\n",
    "      train = df.loc[index < valid_boundary]\n",
    "      if test_boundary > 0:\n",
    "        valid = df.loc[(index >= valid_boundary - 7) & (index < test_boundary)]\n",
    "      else:\n",
    "        valid = df.loc[(index >= valid_boundary - 7)]\n",
    "    if test_boundary > 0:\n",
    "      test = df.loc[index >= test_boundary - 7]\n",
    "\n",
    "    self.set_scalers(train)\n",
    "\n",
    "    Trainshape = train.shape\n",
    "    Traincols = train.columns\n",
    "    print(' Train Shape ' + str(Trainshape))\n",
    "    print(Traincols)\n",
    "    Validshape = valid.shape\n",
    "    Validcols = valid.columns\n",
    "    print(' Validation Shape ' + str(Validshape))\n",
    "    print(Validcols)\n",
    "    if test_boundary >= -1:\n",
    "      return (self.transform_inputs(data) for data in [train, valid, test])\n",
    "    else:\n",
    "      return [train, valid]\n",
    "\n",
    "  def set_scalers(self, df):\n",
    "    \"\"\"Calibrates scalers using the data supplied.\n",
    "\n",
    "    Args:\n",
    "      df: Data to use to calibrate scalers.\n",
    "    \"\"\"\n",
    "    print('Setting scalers with training data...')\n",
    "\n",
    "    column_definitions = self.get_column_definition()\n",
    "#    print(column_definitions)\n",
    "#    print(InputTypes.TARGET)\n",
    "    id_column = myTFTTools.utilsget_single_col_by_input_type(InputTypes.ID,\n",
    "                                                   column_definitions, TFTMultivariate)\n",
    "    target_column = myTFTTools.utilsget_single_col_by_input_type(InputTypes.TARGET,\n",
    "                                                       column_definitions, TFTMultivariate)\n",
    "\n",
    "    # Format real scalers\n",
    "    real_inputs = myTFTTools.extract_cols_from_data_type(\n",
    "        DataTypes.REAL_VALUED, column_definitions,\n",
    "        TFTexcludedinputtypes)\n",
    "\n",
    "    # Initialise scaler caches\n",
    "    self._real_scalers = {}\n",
    "    self._target_scaler = {}\n",
    "    identifiers = []\n",
    "    for identifier, sliced in df.groupby(id_column):\n",
    "\n",
    "      data = sliced[real_inputs].values\n",
    "      if TFTMultivariate == True:\n",
    "        targets = sliced[target_column].values\n",
    "      else:\n",
    "        targets = sliced[target_column].values\n",
    "#      self._real_scalers[identifier] = sklearn.preprocessing.StandardScaler().fit(data)\n",
    "\n",
    "#      self._target_scaler[identifier] = sklearn.preprocessing.StandardScaler().fit(targets)\n",
    "      identifiers.append(identifier)\n",
    "\n",
    "    # Format categorical scalers\n",
    "    categorical_inputs = myTFTTools.extract_cols_from_data_type(\n",
    "        DataTypes.CATEGORICAL, column_definitions,\n",
    "        TFTexcludedinputtypes)\n",
    "\n",
    "    categorical_scalers = {}\n",
    "    num_classes = []\n",
    "\n",
    "    # Set categorical scaler outputs\n",
    "    self._cat_scalers = categorical_scalers\n",
    "    self._num_classes_per_cat_input = num_classes\n",
    "\n",
    "    # Extract identifiers in case required\n",
    "    self.identifiers = identifiers\n",
    "\n",
    "  def transform_inputs(self, df):\n",
    "    \"\"\"Performs feature transformations.\n",
    "\n",
    "    This includes both feature engineering, preprocessing and normalisation.\n",
    "\n",
    "    Args:\n",
    "      df: Data frame to transform.\n",
    "\n",
    "    Returns:\n",
    "      Transformed data frame.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return df\n",
    "\n",
    "  def format_predictions(self, predictions):\n",
    "    \"\"\"Reverts any normalisation to give predictions in original scale.\n",
    "\n",
    "    Args:\n",
    "      predictions: Dataframe of model predictions.\n",
    "\n",
    "    Returns:\n",
    "      Data frame of unnormalised predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    return predictions\n",
    "\n",
    "  # Default params\n",
    "  def get_fixed_params(self):\n",
    "    \"\"\"Returns fixed model parameters for experiments.\"\"\"\n",
    "\n",
    "    fixed_params = TFTfixed_params\n",
    "\n",
    "    return fixed_params\n",
    "\n",
    "  def get_default_model_params(self):\n",
    "    \"\"\"Returns default optimised model parameters.\"\"\"\n",
    "\n",
    "    model_params = TFTmodel_params\n",
    "\n",
    "    return model_params\n",
    "\n",
    "  def get_num_samples_for_calibration(self):\n",
    "    \"\"\"Gets the default number of training and validation samples.\n",
    "\n",
    "    Use to sub-sample the data for network calibration and a value of -1 uses\n",
    "    all available samples.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (training samples, validation samples)\n",
    "    \"\"\"\n",
    "    numtrain = TFTdfTotalshape[0]\n",
    "    numvalid = TFTdfTotalshape[0]\n",
    "    return numtrain, numvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yywSY1y0_XIE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Set TFT Parameter Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTG5j-nVIDSX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def setTFTparameters(data_formatter):\n",
    "# Sets up default params\n",
    "  fixed_params = data_formatter.get_experiment_params()\n",
    "  params = data_formatter.get_default_model_params()\n",
    "  \n",
    "  params[\"model_folder\"] = TFTmodel_folder\n",
    "  params['optimizer'] = Transformeroptimizer\n",
    "  fixed_params[\"quantiles\"] = TFTQuantiles\n",
    "  fixed_params[\"quantilenames\"] = TFTQuantilenames\n",
    "  fixed_params[\"quantileindex\"] = TFTPrimaryQuantileIndex\n",
    "  fixed_params[\"TFTLSTMEncoderFinalMLP\"] = TFTLSTMEncoderFinalMLP\n",
    "  fixed_params[\"TFTLSTMDecoderFinalMLP\"] = TFTLSTMDecoderFinalMLP\n",
    "  fixed_params[\"TFTLSTMEncoderrecurrent_dropout1\"] = TFTLSTMEncoderrecurrent_dropout1\n",
    "  fixed_params[\"TFTLSTMDecoderrecurrent_dropout1\"] = TFTLSTMDecoderrecurrent_dropout1\n",
    "  fixed_params[\"TFTLSTMEncoderdropout1\"] = TFTLSTMEncoderdropout1\n",
    "  fixed_params[\"TFTLSTMDecoderdropout1\"] = TFTLSTMDecoderdropout1\n",
    "  fixed_params[\"TFTLSTMEncoderSecondLayer\"] = TFTLSTMEncoderSecondLayer\n",
    "  fixed_params[\"TFTLSTMDecoderSecondLayer\"] = TFTLSTMDecoderSecondLayer\n",
    "  fixed_params[\"TFTLSTMEncoderThirdLayer\"] = TFTLSTMEncoderThirdLayer\n",
    "  fixed_params[\"TFTLSTMDecoderThirdLayer\"] = TFTLSTMDecoderThirdLayer\n",
    "  fixed_params[\"TFTLSTMEncoderrecurrent_activation\"] = TFTLSTMEncoderrecurrent_activation\n",
    "  fixed_params[\"TFTLSTMDecoderrecurrent_activation\"] = TFTLSTMDecoderrecurrent_activation\n",
    "  fixed_params[\"TFTLSTMEncoderactivationvalue\"] = TFTLSTMEncoderactivationvalue\n",
    "  fixed_params[\"TFTLSTMDecoderactivationvalue\"] = TFTLSTMDecoderactivationvalue\n",
    "  fixed_params[\"TFTLSTMEncoderInitialMLP\"] = TFTLSTMEncoderInitialMLP\n",
    "  fixed_params[\"TFTLSTMDecoderInitialMLP\"] = TFTLSTMDecoderInitialMLP\n",
    "  fixed_params['number_LSTMnodes'] = number_LSTMnodes\n",
    "  fixed_params[\"TFTOption1\"] = 1\n",
    "  fixed_params[\"TFTOption2\"] = 0\n",
    "  fixed_params['TFTMultivariate'] = TFTMultivariate\n",
    "\n",
    "  fixed_params['TFTFinalGatingOption'] = TFTFinalGatingOption\n",
    "  fixed_params['TFTSymbolicWindows'] = TFTSymbolicWindows\n",
    "  fixed_params['name'] = 'TemporalFusionTransformer'\n",
    "  fixed_params['nameFFF'] = TFTexperimentname\n",
    "  fixed_params['runname'] = TFTRunName\n",
    "  fixed_params['runcomment'] = TFTRunComment\n",
    "  fixed_params['data_formatter'] = data_formatter\n",
    "  fixed_params['Validation'] = LocationBasedValidation\n",
    "\n",
    "  # Parameter overrides for testing only! Small sizes used to speed up script.\n",
    "  if TFTuse_testing_mode:\n",
    "    fixed_params[\"num_epochs\"] = 1\n",
    "    params[\"hidden_layer_size\"] = 5\n",
    "#    train_samples, valid_samples = 100, 10 is applied later\n",
    "\n",
    "# Load all parameters -- fixed and model\n",
    "  for k in fixed_params:\n",
    "    params[k] = fixed_params[k]\n",
    "\n",
    "  return params\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g95F8DIAGR2e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###TFTTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwzZRu1RGZlp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TFTTools(object):\n",
    "  def __init__(self, params, **kwargs):\n",
    "# Args: params: Parameters to define TFT\n",
    "\n",
    "    self.name = params['name']\n",
    "    self.experimentname = params['nameFFF']\n",
    "    self.runname = params['runname']\n",
    "    self.runcomment = params['runcomment'] \n",
    "    self.data_formatter = params['data_formatter']\n",
    "    self.lossflag = params['lossflag']\n",
    "    self.HuberLosscut = params['HuberLosscut']\n",
    "    self.optimizer = params['optimizer']\n",
    "    self.validation = params['Validation']\n",
    "    self.AnalysisOnly = params['AnalysisOnly']\n",
    "    self.Restorefromcheckpoint = params['Restorefromcheckpoint']\n",
    "    self.inputRunName = params['inputRunName']\n",
    "    self.inputCheckpointpostfix = params['inputCheckpointpostfix']\n",
    "    self.excludeNULLtype = excludeNULLtype\n",
    "    self.TFTexcludedinputtypes = TFTexcludedinputtypes\n",
    "\n",
    "    # Data parameters\n",
    "    self.time_steps = int(params['total_time_steps'])\n",
    "    self.input_size = int(params['input_size'])\n",
    "    self.output_size = int(params['output_size'])\n",
    "    self.category_counts = json.loads(str(params['category_counts']))\n",
    "    self.n_multiprocessing_workers = int(params['multiprocessing_workers'])\n",
    "\n",
    "    # Relevant indices for TFT\n",
    "    self._input_obs_loc = json.loads(str(params['input_obs_loc']))\n",
    "    self._static_input_loc = json.loads(str(params['static_input_loc']))\n",
    "    self._known_regular_input_idx = json.loads(\n",
    "        str(params['known_regular_inputs']))\n",
    "    self._known_categorical_input_idx = json.loads(\n",
    "        str(params['known_categorical_inputs']))\n",
    "\n",
    "    self.column_definition = params['column_definition']\n",
    "\n",
    "    # Network params\n",
    "    # self.quantiles = [0.1, 0.5, 0.9]\n",
    "    self.quantiles = params['quantiles']\n",
    "    self.NumberQuantiles = len(self.quantiles)\n",
    "    self.Quantilenames = params['quantilenames']\n",
    "    self.PrimaryQuantileIndex = int(params['quantileindex'])\n",
    "    self.useMSE = False\n",
    "    if self.NumberQuantiles == 1 and self.Quantilenames[0] == 'MSE':\n",
    "        self.useMSE = True\n",
    "    self.TFTOption1 = params['TFTOption1']\n",
    "    self.TFTOption2 = params['TFTOption2']\n",
    "    self.TFTMultivariate = params['TFTMultivariate']\n",
    "    \n",
    "    self.TFTuseCUDALSTM = params['TFTuseCUDALSTM']\n",
    "    self.TFTdefaultLSTM = params['TFTdefaultLSTM']\n",
    "    self.number_LSTMnodes = params['number_LSTMnodes']\n",
    "    self.TFTLSTMEncoderInitialMLP = params[\"TFTLSTMEncoderInitialMLP\"]\n",
    "    self.TFTLSTMDecoderInitialMLP = params[\"TFTLSTMDecoderInitialMLP\"]\n",
    "    self.TFTLSTMEncoderFinalMLP = params['TFTLSTMEncoderFinalMLP']\n",
    "    self.TFTLSTMDecoderFinalMLP = params['TFTLSTMDecoderFinalMLP']\n",
    "    self.TFTLSTMEncoderrecurrent_dropout1 = params[\"TFTLSTMEncoderrecurrent_dropout1\"]\n",
    "    self.TFTLSTMDecoderrecurrent_dropout1 = params[\"TFTLSTMDecoderrecurrent_dropout1\"]\n",
    "    self.TFTLSTMEncoderdropout1 = params[\"TFTLSTMEncoderdropout1\"]\n",
    "    self.TFTLSTMDecoderdropout1 = params[\"TFTLSTMDecoderdropout1\"]\n",
    "    self.TFTLSTMEncoderrecurrent_activation = params[\"TFTLSTMEncoderrecurrent_activation\"]\n",
    "    self.TFTLSTMDecoderrecurrent_activation = params[\"TFTLSTMDecoderrecurrent_activation\"]\n",
    "    self.TFTLSTMEncoderactivationvalue = params[\"TFTLSTMEncoderactivationvalue\"]\n",
    "    self.TFTLSTMDecoderactivationvalue = params[\"TFTLSTMDecoderactivationvalue\"]\n",
    "    self.TFTLSTMEncoderSecondLayer = params[\"TFTLSTMEncoderSecondLayer\"]\n",
    "    self.TFTLSTMDecoderSecondLayer = params[\"TFTLSTMDecoderSecondLayer\"]\n",
    "    self.TFTLSTMEncoderThirdLayer = params[\"TFTLSTMEncoderThirdLayer\"]\n",
    "    self.TFTLSTMDecoderThirdLayer = params[\"TFTLSTMDecoderThirdLayer\"]\n",
    "    self.TFTFinalGatingOption = params['TFTFinalGatingOption']\n",
    "    self.TFTSymbolicWindows = params['TFTSymbolicWindows']\n",
    "\n",
    "    self.FinalLoopSize = 1\n",
    "    if (self.output_size == 1) and (self.NumberQuantiles == 1):\n",
    "        self.TFTFinalGatingOption = 0\n",
    "    if self.TFTFinalGatingOption > 0:\n",
    "        self.TFTLSTMFinalMLP = 0\n",
    "        self.FinalLoopSize = self.output_size * self.NumberQuantiles\n",
    "\n",
    "    self.hidden_layer_size = int(params['hidden_layer_size'])\n",
    "    self.dropout_rate = float(params['dropout_rate'])\n",
    "    self.max_gradient_norm = float(params['max_gradient_norm'])\n",
    "    self.learning_rate = float(params['learning_rate'])\n",
    "    self.minibatch_size = int(params['minibatch_size'])\n",
    "    self.maxibatch_size = int(params['maxibatch_size'])\n",
    "    self.num_epochs = int(params['num_epochs'])\n",
    "    self.early_stopping_patience = int(params['early_stopping_patience'])\n",
    "\n",
    "    self.num_encoder_steps = int(params['num_encoder_steps'])\n",
    "    self.num_stacks = int(params['stack_size'])\n",
    "    self.num_heads = int(params['num_heads'])\n",
    "\n",
    "    # Serialisation options\n",
    "# XXX\n",
    "#    self._temp_folder = os.path.join(params['model_folder'], 'tmp')\n",
    "#    self.reset_temp_folder()\n",
    "\n",
    "    # Extra components to store Tensorflow nodes for attention computations\n",
    "# XXX\n",
    "#    self._input_placeholder = None\n",
    "#    self._attention_components = None\n",
    "#    self._prediction_parts = None\n",
    "\n",
    "    self.TFTSeq = 0\n",
    "    self.TFTNloc = 0\n",
    "    self.UniqueLocations = []\n",
    "\n",
    "  def utilsget_single_col_by_input_type(self, input_type, column_definition, TFTMultivariate):\n",
    "    \"\"\"Returns name of single or multiple column.\n",
    "\n",
    "    Args:\n",
    "      input_type: Input type of column to extract\n",
    "      column_definition: Column definition list for experiment\n",
    "    \"\"\"\n",
    "\n",
    "    columnname = [tup[0] for tup in column_definition if tup[2] == input_type]\n",
    "\n",
    "# allow multiple targets\n",
    "    if TFTMultivariate and (input_type == 0):\n",
    "      return columnname\n",
    "    else:\n",
    "      if len(columnname) != 1:\n",
    "        printexit('Invalid number of columns for Type {}'.format(input_type))\n",
    "      return columnname[0]\n",
    "\n",
    "  def _get_single_col_by_type(self, input_type):\n",
    "      return self.utilsget_single_col_by_input_type(input_type, self.column_definition, self.TFTMultivariate)\n",
    "\n",
    "  def extract_cols_from_data_type(self, data_type, column_definition,\n",
    "                                  excluded_input_types):\n",
    "    \"\"\"Extracts the names of columns that correspond to a define data_type.\n",
    "\n",
    "    Args:\n",
    "      data_type: DataType of columns to extract.\n",
    "      column_definition: Column definition to use.\n",
    "      excluded_input_types: Set of input types to exclude\n",
    "\n",
    "    Returns:\n",
    "      List of names for columns with data type specified.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        tup[0]\n",
    "        for tup in column_definition\n",
    "        if tup[1] == data_type and tup[2] not in excluded_input_types\n",
    "    ]\n",
    "\n",
    "\n",
    "# Quantile Loss functions.\n",
    "  def tensorflow_quantile_loss(self, y, y_pred, quantile):\n",
    "    \"\"\"Computes quantile loss for tensorflow.\n",
    "\n",
    "    Standard quantile loss as defined in the \"Training Procedure\" section of\n",
    "    the main TFT paper\n",
    "\n",
    "    Args:\n",
    "      y: Targets\n",
    "      y_pred: Predictions\n",
    "      quantile: Quantile to use for loss calculations (between 0 & 1)\n",
    "\n",
    "    Returns:\n",
    "      Tensor for quantile loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Checks quantile\n",
    "    if quantile < 0 or quantile > 1:\n",
    "      printexit(\n",
    "          'Illegal quantile value={}! Values should be between 0 and 1.'.format(\n",
    "              quantile))\n",
    "\n",
    "    prediction_underflow = y - y_pred\n",
    "    q_loss = quantile * tf.maximum(prediction_underflow, 0.) + (\n",
    "        1. - quantile) * tf.maximum(-prediction_underflow, 0.)\n",
    "\n",
    "    return tf.reduce_sum(q_loss, axis=-1)\n",
    "\n",
    "  def PrintTitle(self, extrawords):\n",
    "    current_time = timenow()\n",
    "    line = self.name + ' ' + self.experimentname + ' ' + self.runname + ' ' + self.runcomment\n",
    "    beginwords = ''\n",
    "    if extrawords != '':\n",
    "      beginwords = extrawords + ' '\n",
    "    print(wraptotext(startbold + startred + beginwords + current_time + ' '  + line + resetfonts))\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zKOTiH3smSk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Setup  Classic TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i1xIk-Gz3Cl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "%cd \"/content/gdrive/MyDrive/Colab Datasets/TFToriginal/\"\n",
    "%ls\n",
    "%cd TFTCode/\n",
    "TFTexperimentname= \"FFFFWNPF\"\n",
    "output_folder = \"../TFTData\" # Please don't change this path\n",
    "Rootmodel_folder = os.path.join(output_folder, 'saved_models', TFTexperimentname)\n",
    "TFTmodel_folder = os.path.join(Rootmodel_folder, \"fixed\" + RunName)\n",
    "'''\n",
    "TFTexperimentname= \"FFFFWNPF\"\n",
    "TFTmodel_folder=\"Notused\"\n",
    "TFTRunName = RunName\n",
    "TFTRunComment = RunComment\n",
    "print('Set up useful columns in needed classes')\n",
    "\n",
    "print('These will be used in selecting items to convert from dataframe to numpy')\n",
    "if TFTexperimentname == 'FFFFWNPF':\n",
    "  formatter = FFFFWNPFFormatter()\n",
    "\n",
    "# Save data frames\n",
    "# TFTdfTotalSpec.to_csv('TFTdfTotalSpec.csv')\n",
    "# TFTdfTotal.to_csv('TFTdfTotal.csv')\n",
    "else:\n",
    "  import expt_settings.configs\n",
    "  ExperimentConfig = expt_settings.configs.ExperimentConfig\n",
    "  config = ExperimentConfig(name, output_folder)\n",
    "  formatter = config.make_data_formatter()\n",
    "\n",
    "TFTparams = setTFTparameters(formatter)\n",
    "myTFTTools = TFTTools(TFTparams)\n",
    "print(' ')\n",
    "myTFTTools.PrintTitle('Start TFT')\n",
    "\n",
    "\n",
    "for k in TFTparams:\n",
    "  print('# {} = {}'.format(k, TFTparams[k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2F6ZPgybDtJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Read TFT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJNPrs26bL7N",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def printLIST(label,outputlist):\n",
    "  if len(outputlist) < 5:\n",
    "    print(label + ' ' + str(outputlist))\n",
    "    return\n",
    "  print(label)\n",
    "  for start in range(0,len(outputlist),4):\n",
    "    endindex = min(start+4,len(outputlist) )\n",
    "    line = ''\n",
    "    for i in range(start,endindex):\n",
    "      line += str(outputlist[i]) + ', '\n",
    "    print(line)\n",
    "  return\n",
    "\n",
    "class TFTDataCache(object):\n",
    "  \"\"\"Caches data for the TFT.\n",
    "  This is a class and has no instances so uses cls not self \n",
    "  It just sets and uses a dictionary to record batched data locations\"\"\"\n",
    "\n",
    "  _data_cache = {}\n",
    "\n",
    "  @classmethod\n",
    "  def update(cls, data,  key):\n",
    "    \"\"\"Updates cached data.\n",
    "\n",
    "    Args:\n",
    "      data: Source to update\n",
    "      key: Key to dictionary location\n",
    "    \"\"\"\n",
    "    cls._data_cache[key] = data\n",
    "\n",
    "  @classmethod\n",
    "  def get(cls, key):\n",
    "    \"\"\"Returns data stored at key location.\"\"\"\n",
    "    return cls._data_cache[key]\n",
    "\n",
    "  @classmethod\n",
    "  def contains(cls, key):\n",
    "    \"\"\"Retuns boolean indicating whether key is present in cache.\"\"\"\n",
    "\n",
    "    return key in cls._data_cache\n",
    "\n",
    "class TFTdatasetup(object):\n",
    "\n",
    "  def __init__(self, **kwargs): \n",
    "    super(TFTdatasetup, self).__init__(**kwargs) \n",
    "\n",
    "    self.TFTNloc = 0\n",
    "\n",
    "# XXX TFTNloc bad\n",
    "    if myTFTTools.TFTSymbolicWindows:\n",
    "    # Set up Symbolic maps allowing location order to differ (due to possible sorting in TFT)\n",
    "        id_col = myTFTTools._get_single_col_by_type(InputTypes.ID)\n",
    "        time_col = myTFTTools._get_single_col_by_type(InputTypes.TIME)\n",
    "        target_col = myTFTTools._get_single_col_by_type(InputTypes.TARGET)\n",
    "        input_cols = [\n",
    "             tup[0]\n",
    "             for tup in myTFTTools.column_definition\n",
    "             if tup[2] not in TFTexcludedinputtypes\n",
    "        ]\n",
    "\n",
    "\n",
    "        self.UniqueLocations = TFTdfTotal[id_col].unique()\n",
    "        self.TFTNloc = len(self.UniqueLocations)\n",
    "        self.LocationLookup ={}\n",
    "        for i,locationname in enumerate(self.UniqueLocations):\n",
    "            self.LocationLookup[locationname] = i # maps name to TFT master location number\n",
    "\n",
    "\n",
    "        self.TFTnum_entries = 0 # Number of time values per location\n",
    "        for identifier, df in TFTdfTotal.groupby(id_col):\n",
    "            localnum_entries = len(df)\n",
    "            if self.TFTnum_entries == 0:\n",
    "                self.TFTnum_entries = localnum_entries\n",
    "            else:\n",
    "                if self.TFTnum_entries != localnum_entries:\n",
    "                    printexit('Incorrect length in time for ' + identifier + ' ' + str(localnum_entries))\n",
    "        self.Lookupinputs = np.zeros((self.TFTNloc, self.TFTnum_entries, myTFTTools.input_size))\n",
    "        for identifier, df in TFTdfTotal.groupby(id_col):\n",
    "            location = self.LocationLookup[identifier]\n",
    "            self.Lookupinputs[location,:,:] = df[input_cols].to_numpy(dtype=np.float32,copy=True)\n",
    "\n",
    "  def __call__(self, data, Dataset_key, num_samples=-1):\n",
    "    \"\"\"Batches Dataset for training, Validation.\n",
    "    Testing not Batched\n",
    "\n",
    "    Args:\n",
    "      data: Data to batch \n",
    "      Dataset_key: Key used for cache\n",
    "      num_samples: Maximum number of samples to extract (-1 to use all data)\n",
    "    \"\"\"\n",
    "    max_samples = num_samples\n",
    "    if max_samples < 0:\n",
    "      max_samples = data.shape[0]\n",
    "    sampleddata = self._sampled_data(data, Dataset_key, max_samples=max_samples)\n",
    "    TFTDataCache.update(sampleddata, Dataset_key)\n",
    "\n",
    "    print('Cached data \"{}\" updated'.format(Dataset_key))\n",
    "    return sampleddata\n",
    "\n",
    "  def _sampled_data(self, data, Dataset_key, max_samples):\n",
    "    \"\"\"Samples segments into a compatible format.\n",
    "\n",
    "    Args:\n",
    "      data: Sources data to sample and batch\n",
    "      max_samples: Maximum number of samples in batch\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of batched data with the maximum samples specified.\n",
    "    \"\"\"\n",
    "\n",
    "    if (max_samples < 1) and (max_samples != -1):\n",
    "      raise ValueError(\n",
    "          'Illegal number of samples specified! samples={}'.format(max_samples))\n",
    "\n",
    "    id_col = myTFTTools._get_single_col_by_type(InputTypes.ID)\n",
    "    time_col = myTFTTools._get_single_col_by_type(InputTypes.TIME)\n",
    "\n",
    "    #data.sort_values(by=[id_col, time_col], inplace=True)  # gives warning message\n",
    "\n",
    "    print('Getting legal sampling locations.')\n",
    "    valid_sampling_locations = []\n",
    "    split_data_map = {}\n",
    "    self.TFTSeq = 0\n",
    "\n",
    "    for identifier, df in data.groupby(id_col):\n",
    "      self.TFTnum_entries = len(df)\n",
    "      self.TFTSeq = max(self.TFTSeq, self.TFTnum_entries-myTFTTools.time_steps+1)\n",
    "      if self.TFTnum_entries >= myTFTTools.time_steps:\n",
    "        valid_sampling_locations += [\n",
    "            (identifier, myTFTTools.time_steps + i)\n",
    "            for i in range(self.TFTnum_entries - myTFTTools.time_steps + 1)\n",
    "        ]\n",
    "      split_data_map[identifier] = df\n",
    "    print(Dataset_key + ' max samples ' + str(max_samples) + ' actual ' + str(len(valid_sampling_locations)))\n",
    "\n",
    "    actual_samples = min(max_samples, len(valid_sampling_locations))\n",
    "    if max_samples > 0 and len(valid_sampling_locations) > max_samples:\n",
    "        print('Extracting {} samples...'.format(max_samples))\n",
    "        ranges = [\n",
    "            valid_sampling_locations[i] for i in np.random.choice(\n",
    "                len(valid_sampling_locations), max_samples, replace=False)\n",
    "        ]\n",
    "    else:\n",
    "        print('Max samples={} exceeds # available segments={}'.format(\n",
    "            max_samples, len(valid_sampling_locations)))\n",
    "        ranges = valid_sampling_locations\n",
    "\n",
    "    id_col = myTFTTools._get_single_col_by_type(InputTypes.ID)\n",
    "    time_col = myTFTTools._get_single_col_by_type(InputTypes.TIME)\n",
    "    target_col = myTFTTools._get_single_col_by_type(InputTypes.TARGET)\n",
    "    input_cols = [\n",
    "        tup[0]\n",
    "        for tup in myTFTTools.column_definition\n",
    "        if tup[2] not in TFTexcludedinputtypes\n",
    "    ]\n",
    "    print('\\nid_col ' + str(id_col))\n",
    "    print('time_col ' + str(time_col))\n",
    "    print(' ')\n",
    "    printLIST('target_col(s) LIST ',target_col)\n",
    "    print(' ')\n",
    "    printLIST('input_cols LIST', input_cols)\n",
    "\n",
    "    if myTFTTools.TFTSymbolicWindows:\n",
    "\n",
    "        inputs = np.zeros((actual_samples), dtype = np.int32)\n",
    "        outputs = np.zeros((actual_samples, myTFTTools.time_steps, myTFTTools.output_size))\n",
    "        time = np.empty((actual_samples, myTFTTools.time_steps, 1), dtype=object)\n",
    "        identifiers = np.empty((actual_samples, myTFTTools.time_steps, 1), dtype=object)\n",
    "\n",
    "\n",
    "        oldlocationnumber = -1\n",
    "        storedlocation = np.zeros(self.TFTNloc, dtype = np.int32)\n",
    "        for i, tup in enumerate(ranges):\n",
    "          identifier, start_idx = tup\n",
    "          newlocationnumber = self.LocationLookup[identifier]\n",
    "          if newlocationnumber != oldlocationnumber:\n",
    "              oldlocationnumber = newlocationnumber\n",
    "              if storedlocation[newlocationnumber] == 0:\n",
    "                  storedlocation[newlocationnumber] = 1\n",
    "          sliced = split_data_map[identifier].iloc[start_idx -\n",
    "                                                   myTFTTools.time_steps:start_idx]\n",
    "#          inputs[i, :, :] = sliced[input_cols]\n",
    "          inputs[i] = np.left_shift(start_idx,16) + newlocationnumber\n",
    "#         Sequence runs from start_idx - myTFTTools.time_steps to start_idx i.e. start_idx is label of FINAL time step in position start_idx - 1\n",
    "          if myTFTTools.TFTMultivariate:\n",
    "              outputs[i, :, :] = sliced[target_col]\n",
    "          else:\n",
    "              outputs[i, :, :] = sliced[[target_col]]\n",
    "          time[i, :, 0] = sliced[time_col]\n",
    "          identifiers[i, :, 0] = sliced[id_col]\n",
    "        inputs = inputs.reshape(-1,1,1)\n",
    "        sampled_data = {\n",
    "            'inputs': inputs,\n",
    "            'outputs': outputs[:, myTFTTools.num_encoder_steps:, :],\n",
    "            'active_entries': np.ones_like(outputs[:, self.num_encoder_steps:, :]),\n",
    "            'time': time,\n",
    "            'identifier': identifiers\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        inputs = np.zeros((actual_samples, myTFTTools.time_steps, myTFTTools.input_size), dtype=np.float32)\n",
    "        outputs = np.zeros((actual_samples, myTFTTools.time_steps, myTFTTools.output_size), dtype=np.float32)\n",
    "        time = np.empty((actual_samples, myTFTTools.time_steps, 1), dtype=object)\n",
    "        identifiers = np.empty((actual_samples, myTFTTools.time_steps, 1), dtype=object)\n",
    "\n",
    "        for i, tup in enumerate(ranges):\n",
    "            identifier, start_idx = tup\n",
    "            sliced = split_data_map[identifier].iloc[start_idx -\n",
    "                                                     myTFTTools.time_steps:start_idx]\n",
    "            inputs[i, :, :] = sliced[input_cols]\n",
    "            if myTFTTools.TFTMultivariate:\n",
    "                outputs[i, :, :] = sliced[target_col]\n",
    "            else:\n",
    "                outputs[i, :, :] = sliced[[target_col]]\n",
    "            time[i, :, 0] = sliced[time_col]\n",
    "            identifiers[i, :, 0] = sliced[id_col]\n",
    "\n",
    "        sampled_data = {\n",
    "            'inputs': inputs,\n",
    "            'outputs': outputs[:, myTFTTools.num_encoder_steps:, :],\n",
    "            'active_entries': np.ones_like(outputs[:, myTFTTools.num_encoder_steps:, :], dtype=np.float32),\n",
    "            'time': time,\n",
    "            'identifier': identifiers\n",
    "        }\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "def dothedatasetup():\n",
    "\n",
    "  myTFTTools.PrintTitle(\"Loading & splitting data...\")\n",
    "\n",
    "  if myTFTTools.experimentname == 'FFFFWNPF':\n",
    "    raw_data = TFTdfTotal\n",
    "  else:  \n",
    "    printexit('Currently only FFFWNPF supported')\n",
    "#    raw_data = pd.read_csv(TFTdfTotal, index_col=0)\n",
    "\n",
    "# XXX don't use test Could simplify \n",
    "  train, valid, test = myTFTTools.data_formatter.split_data(raw_data, test_boundary = -1)\n",
    "  train_samples, valid_samples = myTFTTools.data_formatter.get_num_samples_for_calibration()\n",
    "  test_samples = -1\n",
    "  if TFTuse_testing_mode:\n",
    "    train_samples, valid_samples,test_samples = 100, 10, 100\n",
    "\n",
    "  myTFTReader = TFTdatasetup() \n",
    "  train_data = myTFTReader(train, \"train\", num_samples=train_samples)\n",
    "  val_data = None\n",
    "  if valid_samples > 0:\n",
    "    val_data = myTFTReader(valid, \"valid\", num_samples=valid_samples)\n",
    "  test_data = myTFTReader(test, \"test\", num_samples=test_samples)\n",
    "  return train_data, val_data, test_data\n",
    "\n",
    "TFTtrain_datacollection, TFTval_datacollection, TFTtest_datacollection = dothedatasetup()\n",
    "TFToutput_map = None # holder for final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YcUwbvg0hsU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Predict TFT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3gAe0WaTYCS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Visualize TFT\n",
    "\n",
    "Called from finalizeDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8URv0ll0hsU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TFTSaveandInterpret():\n",
    "\n",
    "  def __init__(self, currentTFTmodel, currentoutput_map, ReshapedPredictionsTOT):\n",
    "# output_map is a dictionary pointing to dataframes\n",
    "# output_map[\"targets\"]) targets are called outputs on input\n",
    "# output_map[\"p10\"] is   p10 quantile forecast\n",
    "# output_map[\"p50\"] is   p10 quantile forecast\n",
    "# output_map[\"p90\"] is   p10 quantile forecast\n",
    "#  Labelled by last real time in sequence (t-1) which starts at time Tseq-1 going up to Num_Time-1\n",
    "\n",
    "# order of Dataframe columns is 'forecast_time', 'identifier', \n",
    "#'t+0-Obs0', 't+0-Obs1', 't+1-Obs0', 't+1-Obs1', 't+2-Obs0', 't+2-Obs1', 't+3-Obs0', 't+3-Obs1', \n",
    "#'t+4-Obs0', 't+4-Obs1', 't+5-Obs0', 't+5-Obs1', 't+6-Obs0', 't+6-Obs1', 't+7-Obs0', 't+7-Obs1', \n",
    "#'t+8-Obs0', 't+8-Obs1', 't+9-Obs0', 't+9-Obs1', 't+10-Obs0', 't+10-Obs1', 't+11-Obs0', 't+11-Obs1', \n",
    "#'t+12-Obs0', 't+12-Obs1', 't+13-Obs0', 't+13-Obs1', 't+14-Obs0', 't+14-Obs1''\n",
    "\n",
    "# First time is FFFFWNPF Sequence # + Tseq-1\n",
    "# Rows of data frame are ilocation*(Num_Seq+1) + FFFFWNPF Sequence #\n",
    "# ilocation runs from 0 ... Nloc-1 in same order in both TFT and FFFFWNPF\n",
    "\n",
    "    self.ScaledProperty = -1\n",
    "    self.Scaled = False\n",
    "    self.savedcolumn = []\n",
    "    self.currentoutput_map = currentoutput_map\n",
    "    self.currentTFTmodel = currentTFTmodel\n",
    "    Sizes = self.currentoutput_map[TFTQuantilenames[TFTPrimaryQuantileIndex]].shape\n",
    "    self.Numx = Sizes[0]\n",
    "    self.Numy = Sizes[1]\n",
    "    self.Num_Seq1 = 1 + Num_Seq\n",
    "    self.MaxTFTSeq = self.Num_Seq1-1\n",
    "    expectednumx = self.Num_Seq1*Nloc\n",
    "    if expectednumx != self.Numx:\n",
    "      printexit(' Wrong sizes of TFT compared to FFFFWNPF ' + str(expectednumx) + ' ' + str(self.Numx))\n",
    "    self.ReshapedPredictionsTOT = ReshapedPredictionsTOT\n",
    "    myTFTTools.PrintTitle('Set up TFTSaveandInterpret')\n",
    "    return\n",
    "\n",
    "  def setFFFFmapping(self):\n",
    "    myTFTTools.PrintTitle('Set up setFFFFmapping')\n",
    "    self.FFFFWNPFresults = np.zeros((self.Numx, NpredperseqTOT,3), dtype=np.float32)\n",
    "\n",
    "    mapFFFFtoTFT = np.empty(Nloc, dtype = np.int32)\n",
    "    TFTLoc = self.currentoutput_map[TFTQuantilenames[TFTPrimaryQuantileIndex]]['identifier'].unique()\n",
    "    FFFFWNPFLocLookup = {}\n",
    "    for i,locname in enumerate(FFFFWNPFUniqueLabel):\n",
    "      FFFFWNPFLocLookup[locname] = i\n",
    "    TFTLocLookup = {}\n",
    "    for i,locname in enumerate(TFTLoc):\n",
    "      TFTLocLookup[locname] = i\n",
    "      if FFFFWNPFLocLookup[locname] is None:\n",
    "        printexit('Missing TFT Location '+locname)\n",
    "    for i,locname in enumerate(FFFFWNPFUniqueLabel):\n",
    "      j = TFTLocLookup[locname] \n",
    "      if j is None:\n",
    "        printexit('Missing FFFFWNPF Location '+ locname)\n",
    "      mapFFFFtoTFT[i] = j\n",
    "\n",
    "\n",
    "    indexposition = np.empty(NpredperseqTOT, dtype=int)\n",
    "    output_mapcolumns = self.currentoutput_map[TFTQuantilenames[TFTPrimaryQuantileIndex]].columns\n",
    "    numcols = len(output_mapcolumns)\n",
    "    for ipred in range(0, NpredperseqTOT):\n",
    "      predstatus = PredictionTFTAction[ipred]\n",
    "      if predstatus > 0:\n",
    "        indexposition[ipred]= -1\n",
    "        continue\n",
    "      label = PredictionTFTnamemapping[ipred]\n",
    "      if label == ' ':\n",
    "        indexposition[ipred]=ipred\n",
    "      else:\n",
    "        findpos = -1\n",
    "        for i in range(0,numcols):\n",
    "          if label == output_mapcolumns[i]:\n",
    "            findpos = i\n",
    "        if findpos < 0:\n",
    "          printexit('Missing Output ' +str(ipred) + ' ' +label)    \n",
    "        indexposition[ipred] = findpos\n",
    "\n",
    "    for iquantile in range(0,myTFTTools.NumberQuantiles):\n",
    "      for ilocation in range(0,Nloc):\n",
    "        for seqnumber in range(0,self.Num_Seq1):\n",
    "          \n",
    "          for ipred in range(0,NpredperseqTOT):\n",
    "            predstatus = PredictionTFTAction[ipred]\n",
    "            if predstatus > 0:\n",
    "              continue\n",
    "            label = PredictionTFTnamemapping[ipred]\n",
    "            if label == ' ': # NOT calculated by TFT\n",
    "              if seqnumber >= Num_Seq:\n",
    "                value = 0.0\n",
    "              else:\n",
    "                value = self.ReshapedPredictionsTOT[ilocation, seqnumber, ipred]\n",
    "\n",
    "            else:\n",
    "              ActualTFTSeq = seqnumber\n",
    "              if ActualTFTSeq <= self.MaxTFTSeq:\n",
    "                ipos = indexposition[ipred]\n",
    "                dfindex = self.Num_Seq1*mapFFFFtoTFT[ilocation] + ActualTFTSeq\n",
    "                value = self.currentoutput_map[TFTQuantilenames[iquantile]].iloc[dfindex,ipos] \n",
    "              else:\n",
    "                dfindex = self.Num_Seq1*mapFFFFtoTFT[ilocation] + self.MaxTFTSeq\n",
    "                ifuture = int(ipred/FFFFWNPFNumberTargets)\n",
    "                jfuture = ActualTFTSeq - self.MaxTFTSeq + ifuture\n",
    "                if jfuture <= LengthFutures:\n",
    "                    jpred = ipred + (jfuture-ifuture)*FFFFWNPFNumberTargets\n",
    "                    value = self.currentoutput_map[TFTQuantilenames[iquantile]].iloc[dfindex,indexposition[jpred]]\n",
    "                else:\n",
    "                  value = 0.0\n",
    "            \n",
    "            FFFFdfindex = self.Num_Seq1*ilocation + seqnumber\n",
    "            self.FFFFWNPFresults[FFFFdfindex,ipred,iquantile] = value   \n",
    "\n",
    "          # Set Calculated Quantities as previous ipred loop has set base values\n",
    "          for ipred in range(0,NpredperseqTOT):\n",
    "            predstatus = PredictionTFTAction[ipred]\n",
    "            if predstatus <= 0:\n",
    "              continue\n",
    "            Basedonprediction = CalculatedPredmaptoRaw[ipred] \n",
    "            predaveragevaluespointer = PredictionAverageValuesPointer[Basedonprediction]\n",
    "            rootflag = QuantityTakeroot[predaveragevaluespointer]\n",
    "            rawdata = np.empty(PredictionCalcLength[ipred],dtype =np.float32)  \n",
    "            ActualTFTSeq = seqnumber\n",
    "\n",
    "            if ActualTFTSeq <= self.MaxTFTSeq:\n",
    "              for ifuture in range(0,PredictionCalcLength[ipred]):\n",
    "                if ifuture == 0:\n",
    "                  kpred = Basedonprediction\n",
    "                else:\n",
    "                  jfuture = NumpredbasicperTime + NumpredFuturedperTime*(ifuture-1)\n",
    "                  kpred = jfuture + FuturedPointer[Basedonprediction]\n",
    "                if predstatus == 3:\n",
    "                  newvalue = self.ReshapedPredictionsTOT[ilocation, ActualTFTSeq, kpred]/ QuantityStatistics[predaveragevaluespointer,2] + QuantityStatistics[predaveragevaluespointer,0]\n",
    "                else:\n",
    "                  kpos = indexposition[kpred]\n",
    "                  dfindex = self.Num_Seq1*mapFFFFtoTFT[ilocation] + ActualTFTSeq\n",
    "                  newvalue = self.currentoutput_map[TFTQuantilenames[iquantile]].iloc[dfindex,kpos] / QuantityStatistics[predaveragevaluespointer,2] + QuantityStatistics[predaveragevaluespointer,0]\n",
    "\n",
    "                if rootflag == 2:\n",
    "                    newvalue = newvalue**2\n",
    "                if rootflag == 3:\n",
    "                    newvalue = newvalue**3\n",
    "                rawdata[ifuture] = newvalue\n",
    "              \n",
    "              # Form collective quantity\n",
    "              if (predstatus == 1):\n",
    "                value = rawdata.sum()\n",
    "              elif predstatus >= 2:\n",
    "                value = log_energy(rawdata, sumaxis=0)\n",
    "              else:\n",
    "                value = 0.0\n",
    "              value = SetTakeroot(value,QuantityTakeroot[ipred])\n",
    "              actualpredaveragevaluespointer = PredictionAverageValuesPointer[ipred]\n",
    "              value = (value-QuantityStatistics[actualpredaveragevaluespointer,0])*QuantityStatistics[actualpredaveragevaluespointer,2]\n",
    "\n",
    "            else:  # Sequence out of range\n",
    "              value = 0.0\n",
    "            \n",
    "            FFFFdfindex = self.Num_Seq1*ilocation + seqnumber\n",
    "            self.FFFFWNPFresults[FFFFdfindex,ipred,iquantile] = value\n",
    "    myTFTTools.PrintTitle('End up setFFFFmapping')\n",
    "    return\n",
    "\n",
    "  # Default returns the median (50% quantile)\n",
    "  def __call__(self, InputVector, Time= None, training = False, Quantile = None):\n",
    "    lenvector = InputVector.shape[0]\n",
    "    result = np.empty((lenvector,NpredperseqTOT), dtype=np.float32)\n",
    "    if Quantile is None:\n",
    "      Quantile = TFTPrimaryQuantileIndex\n",
    "    for ivector in range(0,lenvector):\n",
    "      dfindex = self.Num_Seq1*InputVector[ivector,0] + InputVector[ivector,1]\n",
    "      result[ivector,:] = self.FFFFWNPFresults[dfindex, :, Quantile]\n",
    "\n",
    "    return result\n",
    "\n",
    "  def CheckProperty(self, iprop):\n",
    "  # Return true if property defined for TFT\n",
    "  # set ScaledProperty to be column to be changed\n",
    "    if (iprop < 0) or (iprop >= NpropperseqTOT):\n",
    "      return False\n",
    "    jprop = TFTPropertyChoice[iprop]\n",
    "    if jprop >= 0:\n",
    "      return True\n",
    "    return False\n",
    "  \n",
    "  def SetupProperty(self, iprop):\n",
    "    if self.Scaled:\n",
    "      self.ResetProperty()    \n",
    "    if (iprop < 0) or (iprop >= NpropperseqTOT):\n",
    "      return False\n",
    "    jprop = TFTPropertyChoice[iprop]\n",
    "    if jprop >= 0:\n",
    "      self.ScaledProperty = jprop\n",
    "      self.savedcolumn = TFTdfTotal.iloc[:,jprop].copy()\n",
    "      return True\n",
    "    return False\n",
    "  \n",
    "  def ScaleProperty(self, ScalingFactor):\n",
    "    jprop = self.ScaledProperty\n",
    "    TFTdfTotal.iloc[:,jprop] = ScalingFactor*self.savedcolumn\n",
    "    self.Scaled = True\n",
    "    return\n",
    "\n",
    "  def ResetProperty(self):\n",
    "    jprop = self.ScaledProperty\n",
    "    if jprop >= 0:\n",
    "      TFTdfTotal.iloc[:,jprop] = self.savedcolumn\n",
    "    self.Scaled = False\n",
    "    self.ScaledProperty = -1\n",
    "    return\n",
    "\n",
    "  def MakeMapping(self):\n",
    "    IncreaseNloc_sample = 1\n",
    "    DecreaseNloc_sample = 1\n",
    "    TFTtrain_notused, TFTval_notused, TFTtest_changed = dothedatasetup()\n",
    "    self.currentoutput_map = TFTTestpredict(self.currentTFTmodel, TFTtest_changed)\n",
    "    self.setFFFFmapping()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i_tRpVRaQ4x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "  def VisualizeTFT(TFTmodel, output_map):\n",
    "    myTFTTools.PrintTitle('Start VisualizeTFT')\n",
    "    MyFFFFWNPFLink = TFTSaveandInterpret(TFTmodel, output_map, ReshapedPredictionsTOT)\n",
    "    MyFFFFWNPFLink.setFFFFmapping()\n",
    "    modelflag = 2\n",
    "    myTFTTools.PrintTitle('Invoke DLPrediction')\n",
    "    FitPredictions = DLprediction(ReshapedSequencesTOT, RawInputPredictionsTOT, MyFFFFWNPFLink, modelflag, LabelFit ='TFT')\n",
    "    myTFTTools.PrintTitle('End VisualizeTFT')\n",
    "    # Input Predictions RawInputPredictionsTOT for DLPrediction are ordered Sequence #, Location but\n",
    "    # Input Predictions ReshapedPredictionsTOT for TFTSaveandInterpret are ordered Location, Sequence#\n",
    "    # Note TFT maximum Sequence # is one larger than FFFFWNPF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GDZmm4c6Wmn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##TFT Routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZXP6vYo4oGu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GLUplusskip: Gated Linear unit plus add and norm with Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSTW9UXV4wng",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# GLU with time distribution  optional\n",
    "# Dropout on input dropout_rate\n",
    "# Linear layer with hidden_layer_size and activation\n",
    "# Linear layer with hidden_layer_size and sigmoid\n",
    "# Follow with an add and norm\n",
    "class GLUplusskip(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, hidden_layer_size,\n",
    "                    dropout_rate=None,\n",
    "                    use_time_distributed=True,\n",
    "                    activation=None,\n",
    "                    GLUname = 'Default',\n",
    "                      **kwargs):\n",
    "    \"\"\"Applies a Gated Linear Unit (GLU) to an input.\n",
    "    Follow with an add and norm\n",
    "\n",
    "    Args:\n",
    "      hidden_layer_size: Dimension of GLU\n",
    "      dropout_rate: Dropout rate to apply if any\n",
    "      use_time_distributed: Whether to apply across time (index 1)\n",
    "      activation: Activation function to apply to the linear feature transform if necessary\n",
    "\n",
    "    Returns:\n",
    "      Tuple of tensors for: (GLU output, gate)\n",
    "    \"\"\"\n",
    "    super(GLUplusskip, self).__init__(**kwargs)\n",
    "    self.Gatehidden_layer_size = hidden_layer_size\n",
    "    self.Gatedropout_rate = dropout_rate\n",
    "    self.Gateuse_time_distributed = use_time_distributed\n",
    "    self.Gateactivation = activation\n",
    "\n",
    "    if self.Gatedropout_rate is not None:\n",
    "      n1 = 'GLUSkip' + 'dropout' + GLUname\n",
    "      self.FirstDropout = tf.keras.layers.Dropout(self.Gatedropout_rate, name = n1)\n",
    "\n",
    "    n3 = 'GLUSkip' + 'DenseAct1' + GLUname\n",
    "    n5 = 'GLUSkip' + 'DenseAct2' + GLUname\n",
    "    if self.Gateuse_time_distributed:\n",
    "      n2 = 'GLUSkip' + 'TD1' + GLUname   \n",
    "      self.Gateactivation_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.Gatehidden_layer_size, activation=self.Gateactivation, name=n3), name=n2)\n",
    "      n4 = 'GLUSkip' + 'TD2' + GLUname\n",
    "      self.Gategated_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.Gatehidden_layer_size, activation='sigmoid', name=n5), name=n4)\n",
    "    else:\n",
    "      self.Gateactivation_layer = tf.keras.layers.Dense(self.Gatehidden_layer_size, activation=self.Gateactivation, name=n3)\n",
    "      self.Gategated_layer = tf.keras.layers.Dense(self.Gatehidden_layer_size, activation='sigmoid', name=n5)\n",
    "\n",
    "    n6 = 'GLUSkip' + 'Mul' + GLUname\n",
    "    self.GateMultiply = tf.keras.layers.Multiply(name = n6)\n",
    "\n",
    "    n7 = 'GLUSkip'+ 'Add' + GLUname \n",
    "    n8 = 'GLUSkip' + 'Norm' + GLUname\n",
    "    self.GateAdd = tf.keras.layers.Add(name = n7)\n",
    "    self.GateNormalization = tf.keras.layers.LayerNormalization(name = n8)\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def call(self, Gateinput, Skipinput, training=None):\n",
    "  # Args:\n",
    "  # Gateinput: Input to gating layer\n",
    "  # Skipinput: Input to add and norm\n",
    "\n",
    "    if self.Gatedropout_rate is not None:\n",
    "      x = self.FirstDropout(Gateinput)\n",
    "    else:\n",
    "      x = Gateinput\n",
    "\n",
    "    activation_layer = self.Gateactivation_layer(x)\n",
    "    gated_layer = self.Gategated_layer(x)\n",
    "\n",
    "  # Formal end of GLU\n",
    "    GLUoutput = self.GateMultiply([activation_layer, gated_layer])\n",
    "\n",
    "  # Applies skip connection followed by layer normalisation to get GluSkip.\n",
    "    GLUSkipoutput = self.GateAdd([Skipinput,GLUoutput])\n",
    "    GLUSkipoutput = self.GateNormalization(GLUSkipoutput)\n",
    "\n",
    "    return GLUSkipoutput,gated_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7efbEn8kBlQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Linear Layer (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeuDkoxDkG0C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Layer utility functions.\n",
    "# Single layer size activation with bias and time distribution  optional\n",
    "\n",
    "def TFTlinear_layer(size,\n",
    "                 activation=None,\n",
    "                 use_time_distributed=False,\n",
    "                 use_bias=True,\n",
    "                 LLname = 'Default'):\n",
    "  \"\"\"Returns simple Keras linear layer.\n",
    "\n",
    "    Args:\n",
    "      size: Output size\n",
    "      activation: Activation function to apply if required\n",
    "      use_time_distributed: Whether to apply layer across time\n",
    "      use_bias: Whether bias should be included in layer\n",
    "  \"\"\"\n",
    "  n1 = 'LL'+'Dense'+LLname\n",
    "  linear = tf.keras.layers.Dense(size, activation=activation, use_bias=use_bias,name=n1)\n",
    "  if use_time_distributed:\n",
    "    n2 = 'LL'+'TD'+LLname\n",
    "    linear = tf.keras.layers.TimeDistributed(linear,name=n2)\n",
    "  return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3-kq7hAvJpw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Apply MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70vYILKAvM2M",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class apply_mlp(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, hidden_layer_size, output_size, output_activation=None, hidden_activation='tanh', use_time_distributed=False, MLPname='Default', **kwargs):\n",
    "    \"\"\"Applies simple feed-forward network to an input.\n",
    "\n",
    "    Args:\n",
    "        hidden_layer_size: Hidden state size\n",
    "      output_size: Output size of MLP\n",
    "      output_activation: Activation function to apply on output\n",
    "      hidden_activation: Activation function to apply on input\n",
    "      use_time_distributed: Whether to apply across time\n",
    "\n",
    "    Returns:\n",
    "      Tensor for MLP outputs.\n",
    "    \"\"\"\n",
    "    super(apply_mlp, self).__init__(**kwargs)\n",
    "    self.MLPhidden_layer_size = hidden_layer_size\n",
    "    self.MLPoutput_size = output_size\n",
    "    self.MLPoutput_activation = output_activation\n",
    "    self.MLPhidden_activation = hidden_activation\n",
    "    self.MLPuse_time_distributed = use_time_distributed\n",
    "    n1 = 'MLPDense1' + MLPname\n",
    "    n2 = 'MLPDense2' + MLPname\n",
    "    if self.MLPuse_time_distributed:\n",
    "      n3 = 'MLPTD1' + MLPname\n",
    "      n4 = 'MLPTD2' + MLPname\n",
    "      MLPFirstLayer = tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(self.MLPhidden_layer_size, activation=self.MLPhidden_activation, name = n1), name = n3)\n",
    "      MLPSecondLayer = tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(self.MLPoutput_size, activation=self.MLPoutput_activation, name = n2),name = n4)\n",
    "    else:\n",
    "      MLPFirstLayer = tf.keras.layers.Dense(self.MLPhidden_layer_size, activation=self.MLPhidden_activation, name = n1)\n",
    "      MLPSecondLayer = tf.keras.layers.Dense(self.MLPoutput_size, activation=self.MLPoutput_activation, name = n2)\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def call(self, inputs):\n",
    "  #    inputs: MLP inputs\n",
    "\n",
    "    hidden = MLPFirstLayer(inputs)\n",
    "    return MLPSecondLayer(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBZx6JdpQgn5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###GRN Gated Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSlsALLnQsaQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# GRN Gated Residual Network\n",
    "class GRN(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, hidden_layer_size, output_size=None, dropout_rate=None,\n",
    "               use_additionalcontext = False, use_time_distributed=True, GRNname='Default', **kwargs):\n",
    "    \"\"\"Applies the gated residual network (GRN) as defined in paper.\n",
    "\n",
    "    Args:\n",
    "      hidden_layer_size: Internal state size\n",
    "      output_size: Size of output layer\n",
    "      dropout_rate: Dropout rate if dropout is applied\n",
    "      use_time_distributed: Whether to apply network across time dimension\n",
    "    Returns:\n",
    "      Tuple of tensors for: (GRN output, GLU gate)\n",
    "    \"\"\"\n",
    "\n",
    "    super(GRN, self).__init__(**kwargs)\n",
    "    self.GRNhidden_layer_size = hidden_layer_size\n",
    "    self.GRNoutput_size = output_size\n",
    "    if self.GRNoutput_size is None:\n",
    "      self.GRNusedoutput_size = self.GRNhidden_layer_size\n",
    "    else:\n",
    "      self.GRNusedoutput_size = self.GRNoutput_size\n",
    "\n",
    "    self.GRNdropout_rate = dropout_rate\n",
    "    self.GRNuse_time_distributed = use_time_distributed\n",
    "    self.use_additionalcontext = use_additionalcontext\n",
    "\n",
    "    if self.GRNoutput_size is not None:\n",
    "      n1 = 'GRN'+'Dense4' + GRNname\n",
    "      if self.GRNuse_time_distributed:\n",
    "        n2 = 'GRN'+'TD4' + GRNname\n",
    "        self.GRNDense4 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.GRNusedoutput_size,name=n1),name=n2)\n",
    "      else:\n",
    "        self.GRNDense4 = tf.keras.layers.Dense(self.GRNusedoutput_size,name=n1)\n",
    "\n",
    "    n3 = 'GRNDense1' + GRNname\n",
    "    self.GRNDense1 = TFTlinear_layer(\n",
    "      self.GRNhidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=self.GRNuse_time_distributed,\n",
    "      LLname=n3)\n",
    "    \n",
    "    if self.use_additionalcontext:\n",
    "      n4 = 'GRNDense2' + GRNname\n",
    "      self.GRNDense2= TFTlinear_layer(\n",
    "        self.GRNhidden_layer_size,\n",
    "        activation=None,\n",
    "        use_time_distributed=self.GRNuse_time_distributed,\n",
    "        use_bias=False,\n",
    "        LLname=n4)\n",
    "    \n",
    "    n5 = 'GRNAct' + GRNname\n",
    "    self.GRNActivation = tf.keras.layers.Activation('elu',name=n5)\n",
    "\n",
    "    n6 = 'GRNDense3' + GRNname\n",
    "    self.GRNDense3 = TFTlinear_layer(\n",
    "      self.GRNhidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=self.GRNuse_time_distributed,\n",
    "      LLname =n6)\n",
    "\n",
    "    n7 = 'GRNGLU' + GRNname  \n",
    "    self.GRNGLUplusskip = GLUplusskip(hidden_layer_size = self.GRNusedoutput_size, dropout_rate=self.GRNdropout_rate, \n",
    "      use_time_distributed= self.GRNuse_time_distributed, GLUname=n7)\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def call(self, x, additional_context=None, return_gate=False, training=None):\n",
    "    \"\"\"Args:\n",
    "        x: Network inputs\n",
    "        additional_context: Additional context vector to use if relevant\n",
    "        return_gate: Whether to return GLU gate for diagnostic purposes\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup skip connection of given size\n",
    "    if self.GRNoutput_size is None:\n",
    "      skip = x\n",
    "    else:\n",
    "      skip = self.GRNDense4(x)\n",
    "\n",
    "    # Apply feedforward network\n",
    "    hidden = self.GRNDense1(x)\n",
    "    if additional_context is not None:\n",
    "      if not self.use_additionalcontext:\n",
    "        printexit('Inconsistent context in GRN')\n",
    "      hidden = hidden + self.GRNDense2(additional_context)\n",
    "    else:\n",
    "      if self.use_additionalcontext:\n",
    "        printexit('Inconsistent context in GRN')\n",
    "    hidden = self.GRNActivation(hidden)\n",
    "    hidden = self.GRNDense3(hidden)\n",
    "\n",
    "    gating_layer, gate = self.GRNGLUplusskip(hidden,skip)\n",
    "    if return_gate:\n",
    "      return gating_layer, gate\n",
    "    else:\n",
    "      return gating_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrQ7fndaXFfO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Process Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdAOB1s2XJ8K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Process Static inputs in TFT Style\n",
    "# TFTScaledStaticInputs[Location,0...NumTrueStaticVariables]\n",
    "\n",
    "class ProcessStaticInput(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, hidden_layer_size, dropout_rate, num_staticproperties, **kwargs):\n",
    "    super(ProcessStaticInput, self).__init__(**kwargs)\n",
    "    self.hidden_layer_size = hidden_layer_size\n",
    "    self.num_staticproperties = num_staticproperties\n",
    "    self.dropout_rate = dropout_rate\n",
    "    print('num_staticproperties ' + str(num_staticproperties))\n",
    "    \n",
    "    n4 = 'ProcStaticFlat'\n",
    "    self.Flatten = tf.keras.layers.Flatten(name=n4)\n",
    "    n5 = 'ProcStaticG1'\n",
    "    n7 = 'ProcStaticSoftmax'\n",
    "    n8 = 'ProcStaticMul'\n",
    "    self.StaticInputGRN1 = GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, \n",
    "      output_size=self.num_staticproperties, use_time_distributed=False, GRNname=n5)\n",
    "    \n",
    "    self.StaticInputGRN2 = []\n",
    "    for i in range(0,self.num_staticproperties):\n",
    "      n6 = 'ProcStaticG2-'+str(i)\n",
    "      self.StaticInputGRN2.append(GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, \n",
    "          use_time_distributed=False, GRNname = n6))\n",
    "    self.StaticInputsoftmax = tf.keras.layers.Activation('softmax', name= n7)\n",
    "    self.StaticMultiply = tf.keras.layers.Multiply(name = n8)\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def call(self, static_inputs, training=None):\n",
    "\n",
    "# Embed Static Inputs\n",
    "    num_static = static_inputs.shape[1] \n",
    "    if num_static != self.num_staticproperties:\n",
    "      printexit('Incorrect number of static variables')\n",
    "    if num_static == 0:\n",
    "      return None, None\n",
    "\n",
    "# static_inputs is [Batch, Static variable, TFTd_model] converted to\n",
    "# flatten is [Batch, Static variable*TFTd_model]\n",
    "    flatten = self.Flatten(static_inputs)\n",
    "\n",
    "    # Nonlinear transformation with gated residual network.\n",
    "    mlp_outputs = self.StaticInputGRN1(flatten)\n",
    "    sparse_weights = self.StaticInputsoftmax(mlp_outputs)\n",
    "    sparse_weights = tf.expand_dims(sparse_weights, axis=-1)\n",
    "\n",
    "    trans_emb_list = []\n",
    "    for i in range(num_static):\n",
    "      e = self.StaticInputGRN2[i](static_inputs[:,i:i+1,:])\n",
    "      trans_emb_list.append(e)\n",
    "    transformed_embedding = tf.concat(trans_emb_list, axis=1)\n",
    "\n",
    "    combined = self.StaticMultiply([sparse_weights, transformed_embedding])\n",
    "    static_encoder = tf.math.reduce_sum(combined, axis=1)\n",
    "    \n",
    "    return static_encoder, sparse_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-CC_kwDcT55",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Process Dynamic Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L79o4ClUcbLo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Process Initial Dynamic inputs in TFT Style\n",
    "# ScaledDynamicInputs[Location, time_steps,0...NumDynamicVariables]\n",
    "\n",
    "class ProcessDynamicInput(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self, hidden_layer_size, dropout_rate, NumDynamicVariables, PDIname='Default', **kwargs):\n",
    "    super(ProcessDynamicInput, self).__init__(**kwargs)\n",
    "\n",
    "    self.hidden_layer_size = hidden_layer_size\n",
    "    self.NumDynamicVariables = NumDynamicVariables\n",
    "    self.dropout_rate = dropout_rate\n",
    "    print('NumDynamicVariables ' + str(NumDynamicVariables))\n",
    "    \n",
    "    n6 = PDIname + 'ProcDynG1'\n",
    "    n8 = PDIname + 'ProcDynSoftmax'\n",
    "    n9 = PDIname + 'ProcDynMul'\n",
    "    self.DynamicVariablesGRN1 = GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate,\n",
    "      output_size=self.NumDynamicVariables, use_additionalcontext = True, use_time_distributed=True, GRNname = n6)\n",
    "    self.DynamicVariablesGRN2 = []\n",
    "    for i in range(0,self.NumDynamicVariables):\n",
    "      n7 = PDIname + 'ProcDynG2-'+str(i)\n",
    "      self.DynamicVariablesGRN2.append(GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, \n",
    "        use_additionalcontext = False, use_time_distributed=True, name = n7))\n",
    "    self.DynamicVariablessoftmax = tf.keras.layers.Activation('softmax', name = n8)\n",
    "    self.DynamicVariablesMultiply = tf.keras.layers.Multiply(name = n9)\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def call(self, dynamic_variables, static_context_variable_selection=None, training=None):\n",
    "\n",
    "# Add time window index to static context\n",
    "    if static_context_variable_selection is None:\n",
    "      self.expanded_static_context = None\n",
    "    else:\n",
    "      self.expanded_static_context = tf.expand_dims(static_context_variable_selection, axis=1)\n",
    "\n",
    "# Test Dynamic Variables\n",
    "    num_dynamic = dynamic_variables.shape[-1] \n",
    "    if num_dynamic != self.NumDynamicVariables:\n",
    "      printexit('Incorrect number of Dynamic Inputs ' + str(num_dynamic) + ' ' + str(self.NumDynamicVariables))\n",
    "    if num_dynamic == 0:\n",
    "      return None, None, None\n",
    "\n",
    "# dynamic_variables is [Batch, Time window index, Dynamic variable, TFTd_model] converted to\n",
    "# flatten is [Batch, Time window index, Dynamic variable,*TFTd_model]\n",
    "    _,time_steps,embedding_dimension,num_inputs = dynamic_variables.get_shape().as_list()\n",
    "    flatten = tf.reshape(dynamic_variables, [-1,time_steps,embedding_dimension * num_inputs])\n",
    "\n",
    "# Nonlinear transformation with gated residual network.\n",
    "    mlp_outputs, static_gate = self.DynamicVariablesGRN1(flatten, additional_context=self.expanded_static_context, return_gate=True)\n",
    "    sparse_weights = self.DynamicVariablessoftmax(mlp_outputs)\n",
    "    sparse_weights = tf.expand_dims(sparse_weights, axis=2)\n",
    "\n",
    "    trans_emb_list = []\n",
    "    for i in range(num_dynamic):\n",
    "      e = self.DynamicVariablesGRN2[i](dynamic_variables[Ellipsis,i], additional_context=None)\n",
    "      trans_emb_list.append(e)\n",
    "    transformed_embedding = tf.stack(trans_emb_list, axis=-1)\n",
    "\n",
    "    combined = self.DynamicVariablesMultiply([sparse_weights, transformed_embedding])\n",
    "    temporal_ctx = tf.math.reduce_sum(combined, axis=-1)\n",
    "\n",
    "    return temporal_ctx, sparse_weights, static_gate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4IlhggIYhVZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###TFT LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "td9kgEmYoG3F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class TFTLSTMLayer(tf.keras.Model):\n",
    "# Class for TFT Encoder multiple layer LSTM with possible FCN at start and end\n",
    "# All parameters defined externally\n",
    "\n",
    "  def __init__(self,  TFTLSTMSecondLayer, TFTLSTMThirdLayer, \n",
    "               TFTLSTMInitialMLP, TFTLSTMFinalMLP, \n",
    "               TFTnumber_LSTMnodes, TFTLSTMd_model, \n",
    "               TFTLSTMactivationvalue, TFTLSTMrecurrent_activation,\n",
    "               TFTLSTMdropout1, TFTLSTMrecurrent_dropout1,\n",
    "               TFTreturn_state, LSTMname='Default', **kwargs):\n",
    "    super(TFTLSTMLayer, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    self.TFTLSTMSecondLayer = TFTLSTMSecondLayer\n",
    "    self.TFTLSTMThirdLayer = TFTLSTMThirdLayer\n",
    "    self.TFTLSTMInitialMLP = TFTLSTMInitialMLP\n",
    "    self.TFTLSTMFinalMLP = TFTLSTMFinalMLP\n",
    "    self.TFTLSTMd_model = TFTLSTMd_model\n",
    "    self.TFTnumber_LSTMnodes = TFTnumber_LSTMnodes\n",
    "    self.TFTLSTMactivationvalue = TFTLSTMactivationvalue\n",
    "    self.TFTLSTMdropout1 = TFTLSTMdropout1\n",
    "    self.TFTLSTMrecurrent_dropout1 = TFTLSTMrecurrent_dropout1\n",
    "    self.TFTLSTMrecurrent_activation = TFTLSTMrecurrent_activation\n",
    "  \n",
    "    self.TFTLSTMreturn_state = TFTreturn_state\n",
    "    self.first_return_state = self.TFTLSTMreturn_state\n",
    "    if self.TFTLSTMSecondLayer:\n",
    "      self.first_return_state = True\n",
    "    self.second_return_state = self.TFTLSTMreturn_state\n",
    "    if self.TFTLSTMThirdLayer:\n",
    "      self.second_return_state = True\n",
    "    self.third_return_state = self.TFTLSTMreturn_state\n",
    "\n",
    "    if(self.TFTLSTMInitialMLP > 0):\n",
    "      n1= LSTMname +'LSTMDense1'\n",
    "      self.dense_1 = tf.keras.layers.Dense(self.TFTLSTMInitialMLP, activation=self.TFTLSTMactivationvalue, name =n1)\n",
    "    n2= LSTMname +'LSTMLayer1'\n",
    "\n",
    "    if myTFTTools.TFTuseCUDALSTM:\n",
    "        self.LSTM_1 = tf.compat.v1.keras.layers.CuDNNLSTM(\n",
    "          self.TFTnumber_LSTMnodes,\n",
    "          return_sequences=True,\n",
    "          return_state=self.first_return_state,\n",
    "          stateful=False, name=n2)\n",
    "    else:\n",
    "      self.LSTM_1 =tf.keras.layers.LSTM(self.TFTnumber_LSTMnodes, recurrent_dropout= self.TFTLSTMrecurrent_dropout1, dropout = self.TFTLSTMdropout1,\n",
    "                    return_state = self.first_return_state, activation= self.TFTLSTMactivationvalue , return_sequences=True, \n",
    "                    recurrent_activation= self.TFTLSTMrecurrent_activation, name=n2)\n",
    "\n",
    "    if self.TFTLSTMSecondLayer:\n",
    "      n3= LSTMname +'LSTMLayer2'\n",
    "      if myTFTTools.TFTuseCUDALSTM:\n",
    "        self.LSTM_2 = tf.compat.v1.keras.layers.CuDNNLSTM(\n",
    "          self.TFTnumber_LSTMnodes,\n",
    "          return_sequences=True,\n",
    "          return_state=self.second_return_state,\n",
    "          stateful=False, name=n3)\n",
    "      else:\n",
    "        self.LSTM_2 =tf.keras.layers.LSTM(self.TFTnumber_LSTMnodes, recurrent_dropout= self.TFTLSTMrecurrent_dropout1, dropout = self.TFTLSTMdropout1,\n",
    "          return_state = self.second_return_state, activation= self.TFTLSTMactivationvalue , return_sequences=True, \n",
    "          recurrent_activation= self.TFTLSTMrecurrent_activation, name=n3)\n",
    "    if self.TFTLSTMThirdLayer:\n",
    "      n4= LSTMname +'LSTMLayer3'\n",
    "      if myTFTTools.TFTuseCUDALSTM:\n",
    "        self.LSTM_3 = tf.compat.v1.keras.layers.CuDNNLSTM(\n",
    "          self.TFTnumber_LSTMnodes,\n",
    "          return_sequences=True,\n",
    "          return_state=self.third_return_state,\n",
    "          stateful=False, name=n4)\n",
    "      else:\n",
    "        self.LSTM_3 =tf.keras.layers.LSTM(self.TFTnumber_LSTMnodes, recurrent_dropout= self.TFTLSTMrecurrent_dropout1, dropout = self.TFTLSTMdropout1,\n",
    "          return_state = self.third_return_state, activation= self.TFTLSTMactivationvalue , \n",
    "          return_sequences=True, recurrent_activation= self.TFTLSTMrecurrent_activation, name=n4)\n",
    "    if(self.TFTLSTMFinalMLP > 0):\n",
    "      n5= LSTMname +'LSTMDense2'\n",
    "      n6= LSTMname +'LSTMDense3'\n",
    "      self.dense_2 = tf.keras.layers.Dense(self.TFTLSTMFinalMLP, activation=self.TFTLSTMactivationvalue, name=n5)\n",
    "      self.dense_f = tf.keras.layers.Dense(self.TFTLSTMd_model, name= n6)\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def call(self, inputs, initial_state = None, training=None):\n",
    "    if initial_state is None:\n",
    "      printexit(' Missing context in LSTM ALL')\n",
    "    if initial_state[0] is None:\n",
    "      printexit(' Missing context in LSTM h')\n",
    "    if initial_state[1] is None:\n",
    "      printexit(' Missing context in LSTM c')\n",
    "    returnstate_h = None\n",
    "    returnstate_c = None\n",
    "    if(self.TFTLSTMInitialMLP > 0):\n",
    "      Runningdata = self.dense_1(inputs)\n",
    "    else:\n",
    "      Runningdata = inputs\n",
    "\n",
    "    if self.first_return_state:\n",
    "      Runningdata, returnstate_h, returnstate_c = self.LSTM_1(inputs, training=training, initial_state=initial_state)\n",
    "      if returnstate_h is None:\n",
    "        printexit('Missing context in LSTM returnstate_h')\n",
    "      if returnstate_c is None:\n",
    "        printexit('Missing context in LSTM returnstate_c')\n",
    "    else:\n",
    "      Runningdata = self.LSTM_1(inputs, training=training, initial_state=initial_state)\n",
    "\n",
    "    if self.TFTLSTMSecondLayer:\n",
    "      initial_statehc2 = None\n",
    "      if self.first_return_state:\n",
    "        initial_statehc2 = [returnstate_h, returnstate_c]\n",
    "      if self.second_return_state:\n",
    "        Runningdata, returnstate_h, returnstate_c = self.LSTM_2(Runningdata, training=training, initial_state=initial_statehc2)\n",
    "        if returnstate_h is None:\n",
    "          printexit('Missing context in LSTM returnstate_h2')\n",
    "        if returnstate_c is None:\n",
    "          printexit('Missing context in LSTM returnstate_c2')\n",
    "      else:\n",
    "        Runningdata = self.LSTM_2(Runningdata, training=training, initial_state=initial_statehc2)\n",
    "    if self.TFTLSTMThirdLayer:\n",
    "      initial_statehc3 = None\n",
    "      if self.first_return_state:\n",
    "        initial_statehc3 = [returnstate_h, returnstate_c]\n",
    "      if self.third_return_state:\n",
    "        Runningdata, returnstate_h, returnstate_c  = self.LSTM_3(Runningdata, training=training, initial_state=initial_statehc3)\n",
    "      else:\n",
    "        Runningdata = self.LSTM_3(Runningdata, training=training, initial_state=initial_statehc3)\n",
    "\n",
    "    if(self.TFTLSTMFinalMLP > 0):\n",
    "      Runningdata = self.dense_2(Runningdata)\n",
    "      Outputdata = self.dense_f(Runningdata)\n",
    "    else:\n",
    "      Outputdata = Runningdata\n",
    "\n",
    "    if self.TFTLSTMreturn_state:\n",
    "      return Outputdata, returnstate_h, returnstate_c\n",
    "    else:\n",
    "      return Outputdata\n",
    "\n",
    "  def build_graph(self, shapes):\n",
    "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0re4iqkmCuv4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###TFT Multihead Temporal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r01Yst0VJgbv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Attention Components.\n",
    "#EAGER@tf.function\n",
    "def TFTget_decoder_mask(self_attn_inputs):\n",
    "  \"\"\"Returns causal mask to apply for self-attention layer.\n",
    "\n",
    "  Args:\n",
    "    self_attn_inputs: Inputs to self attention layer to determine mask shape\n",
    "  \"\"\"\n",
    "  len_s = tf.shape(self_attn_inputs)[1]\n",
    "  bs = tf.shape(self_attn_inputs)[:1]\n",
    "  mask = tf.math.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "  return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3PNlQ5eJYq-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TFTScaledDotProductAttention(tf.keras.Model):\n",
    "  \"\"\"Defines scaled dot product attention layer for TFT\n",
    "\n",
    "  Attributes:\n",
    "    dropout: Dropout rate to use\n",
    "    activation: Normalisation function for scaled dot product attention (e.g.\n",
    "      softmax by default)\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, attn_dropout=0.0, SPDAname='Default', **kwargs):\n",
    "    super(TFTScaledDotProductAttention, self).__init__(**kwargs)\n",
    "    n1 = SPDAname + 'SPDADropout'\n",
    "    n2 = SPDAname + 'SPDASoftmax'\n",
    "    n3 = SPDAname + 'SPDAAdd'\n",
    "    self.dropoutlayer = tf.keras.layers.Dropout(attn_dropout, name= n1)\n",
    "    self.activationlayer =  tf.keras.layers.Activation('softmax', name= n2)\n",
    "    self.addlayer = tf.keras.layers.Add(name=n3)\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def call(self, q, k, v, mask):\n",
    "    \"\"\"Applies scaled dot product attention.\n",
    "\n",
    "    Args:\n",
    "      q: Queries\n",
    "      k: Keys\n",
    "      v: Values\n",
    "      mask: Masking if required -- sets softmax to very large value\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (layer outputs, attention weights)\n",
    "    \"\"\"\n",
    "    temper = tf.sqrt(tf.cast(tf.shape(k)[-1], dtype='float32'))\n",
    "    attn = tf.keras.layers.Lambda(lambda x: tf.keras.backend.batch_dot(x[0], x[1], axes=[2, 2]) / temper)(\n",
    "        [q, k])  # shape=(batch, q, k)\n",
    "    if mask is not None:\n",
    "      mmask = tf.keras.layers.Lambda(lambda x: (-1e+9) * (1. - tf.cast(x, 'float32')))( mask)  # setting to infinity\n",
    "      attn = self.addlayer([attn, mmask])\n",
    "    attn = self.activationlayer(attn)\n",
    "    attn = self.dropoutlayer(attn)\n",
    "    output = tf.keras.layers.Lambda(lambda x: tf.keras.backend.batch_dot(x[0], x[1]))([attn, v])\n",
    "    return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpkCmJw3C715",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TFTInterpretableMultiHeadAttention(tf.keras.Model):\n",
    "  \"\"\"Defines interpretable multi-head attention layer for time only.\n",
    "\n",
    "  Attributes:\n",
    "    n_head: Number of heads\n",
    "    d_k: Key/query dimensionality per head\n",
    "    d_v: Value dimensionality\n",
    "    dropout: Dropout rate to apply\n",
    "    qs_layers: List of queries across heads\n",
    "    ks_layers: List of keys across heads\n",
    "    vs_layers: List of values across heads\n",
    "    attention: Scaled dot product attention layer\n",
    "    w_o: Output weight matrix to project internal state to the original TFT\n",
    "      state size\n",
    "  \"\"\"\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def __init__(self, n_head, d_model, dropout, MHAname ='Default', **kwargs):\n",
    "    super(TFTInterpretableMultiHeadAttention, self).__init__(**kwargs)\n",
    "    \"\"\"Initialises layer.\n",
    "\n",
    "    Args:\n",
    "      n_head: Number of heads\n",
    "      d_model: TFT state dimensionality\n",
    "      dropout: Dropout discard rate\n",
    "    \"\"\"\n",
    "\n",
    "    self.n_head = n_head\n",
    "    self.d_k = self.d_v  = d_model // n_head\n",
    "    self.d_model = d_model\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.qs_layers = []\n",
    "    self.ks_layers = []\n",
    "    self.vs_layers = []\n",
    "\n",
    "    # Use same value layer to facilitate interp\n",
    "    n3= MHAname + 'MHAV'\n",
    "    vs_layer = tf.keras.layers.Dense(self.d_v, use_bias=False,name= n3)\n",
    "\n",
    "    self.Dropoutlayer1 =[]\n",
    "    for i_head in range(n_head):\n",
    "      n1= MHAname + 'MHAQ' + str(i)\n",
    "      n2= MHAname + 'MHAK' + str(i)\n",
    "      self.qs_layers.append(tf.keras.layers.Dense(self.d_k, use_bias=False, name = n1))\n",
    "      self.ks_layers.append(tf.keras.layers.Dense(self.d_k, use_bias=False, name = n2))\n",
    "      self.vs_layers.append(vs_layer)  # use same vs_layer\n",
    "      n4= MHAname + 'Dropout1-' + str(i)\n",
    "      self.Dropoutlayer1.append(tf.keras.layers.Dropout(self.dropout, name = n4))\n",
    "\n",
    "    self.attention = TFTScaledDotProductAttention(SPDAname = MHAname)\n",
    "\n",
    "    n5= MHAname + 'Dropout2'\n",
    "    n6= MHAname + 'w_olayer'\n",
    "    self.Dropoutlayer2 = tf.keras.layers.Dropout(self.dropout, name = n5)\n",
    "    self.w_olayer = tf.keras.layers.Dense(d_model, use_bias=False, name = n6)\n",
    "\n",
    "  #EAGER@tf.function\n",
    "  def call(self, q, k, v, mask=None):\n",
    "    \"\"\"Applies interpretable multihead attention.\n",
    "\n",
    "    Using T to denote the number of past + future time steps fed into the transformer.\n",
    "\n",
    "    Args:\n",
    "      q: Query tensor of shape=(?, T, d_model)\n",
    "      k: Key of shape=(?, T, d_model)\n",
    "      v: Values of shape=(?, T, d_model)\n",
    "      mask: Masking if required with shape=(?, T, T)\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (layer outputs, attention weights)\n",
    "    \"\"\"\n",
    "\n",
    "    heads = []\n",
    "    attns = []\n",
    "    for i in range(self.n_head):\n",
    "      qs = self.qs_layers[i](q)\n",
    "      ks = self.ks_layers[i](k)\n",
    "      vs = self.vs_layers[i](v)\n",
    "      head, attn = self.attention(qs, ks, vs, mask)\n",
    "      head_dropout = self.Dropoutlayer1[i](head)\n",
    "      heads.append(head_dropout)\n",
    "      attns.append(attn)\n",
    "\n",
    "    head = tf.stack(heads) if self.n_head > 1 else heads[0]\n",
    "    attn = tf.stack(attns)\n",
    "\n",
    "    outputs = tf.math.reduce_mean(head, axis=0) if self.n_head > 1 else head\n",
    "    outputs = self.w_olayer(outputs)\n",
    "    outputs = self.Dropoutlayer2(outputs)  # output dropout\n",
    "\n",
    "    return outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f04YOJfnF0Eb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###TFTFullNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd09IS9VF58i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TFTFullNetwork(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self,  **kwargs):\n",
    "    super(TFTFullNetwork, self).__init__(**kwargs)\n",
    "\n",
    "# XXX check TFTSeq TFTNloc UniqueLocations\n",
    "    self.TFTSeq = 0\n",
    "    self.TFTNloc = 0\n",
    "    self.UniqueLocations = []\n",
    "    self.hidden_layer_size = myTFTTools.hidden_layer_size\n",
    "    self.dropout_rate = myTFTTools.dropout_rate\n",
    "    self.num_heads = myTFTTools.num_heads\n",
    "\n",
    "# New parameters in this TFT version\n",
    "    self.num_static = len(myTFTTools._static_input_loc)\n",
    "    self.num_categorical_variables = len(myTFTTools.category_counts)\n",
    "    self.NumDynamicHistoryVariables = myTFTTools.input_size  - self.num_static # Note Future (targets) are also in history\n",
    "    self.num_regular_variables = myTFTTools.input_size - self.num_categorical_variables\n",
    "\n",
    "    self.NumDynamicFutureVariables  = 0\n",
    "    line = 'Future DF Locations '\n",
    "    for i in myTFTTools._known_regular_input_idx:\n",
    "      if i not in myTFTTools._static_input_loc:\n",
    "        self.NumDynamicFutureVariables += 1\n",
    "        line += str(i) + ' '\n",
    "    for i in myTFTTools._known_categorical_input_idx:\n",
    "      if i + self.num_regular_variables not in myTFTTools._static_input_loc:\n",
    "        self.NumDynamicFutureVariables += 1  \n",
    "        line += 'CAT ' + str(i) + ' '  \n",
    "    print(line)\n",
    "\n",
    "# Embed Categorical Variables\n",
    "    line = 'CATEGORICAL EMBEDDINGS '\n",
    "    self.CatVariablesembeddings = []\n",
    "    for i in range(0,self.num_categorical_variables):\n",
    "      numcat = self.category_counts[i]\n",
    "      line += str(i) + ' CT ' + str(numcat) + ' '\n",
    "      n1 = 'CatEmbed-'+str(i)\n",
    "      n2 = n1 + 'Input ' + str(numcat)\n",
    "      n3 = n1 + 'Map'\n",
    "      n1 = n1 +'Seq'\n",
    "      embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.InputLayer([myTFTTools.time_steps],name=n2),\n",
    "          tf.keras.layers.Embedding(\n",
    "              numcat,\n",
    "              self.hidden_layer_size,\n",
    "              input_length=myTFTTools.time_steps,\n",
    "              dtype=tf.float32,name=n3)\n",
    "      ],name=n1)\n",
    "      self.CatVariablesembeddings.append(embedding)\n",
    "    print(line)\n",
    "\n",
    "# Embed Static Variables\n",
    "    numstatic = 0\n",
    "    line = 'Static '\n",
    "    self.StaticInitialembeddings = []\n",
    "    for i in range(self.num_regular_variables):\n",
    "      if i in myTFTTools._static_input_loc:\n",
    "        n1 = 'StaticRegEmbed-'+str(numstatic)\n",
    "        embedding = tf.keras.layers.Dense(self.hidden_layer_size, name=n1)\n",
    "        line += str(i) + ' '\n",
    "        self.StaticInitialembeddings.append(embedding)\n",
    "        numstatic += 1\n",
    "    print(line)\n",
    "\n",
    "# Embed Targets _input_obs_loc - also included as part of Observed inputs\n",
    "    self.convert_obs_inputs = []\n",
    "    num_obs_inputs = 0\n",
    "    line = 'Observed Inputs '\n",
    "    for i in myTFTTools._input_obs_loc:\n",
    "        n1 = 'OBSINPEmbed-Dense-'+str(num_obs_inputs)\n",
    "        n2 = 'OBSINPEmbed-Time-'+str(num_obs_inputs)\n",
    "        embedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.hidden_layer_size,name=n1), name=n2)\n",
    "        num_obs_inputs += 1\n",
    "        self.convert_obs_inputs.append(embedding)\n",
    "        line += str(i) + ' '\n",
    "    print(line)\n",
    "\n",
    "# Embed unknown_inputs which are elsewhere called observed inputs\n",
    "    self.convert_unknown_inputs = []\n",
    "    num_unknown_inputs = 0\n",
    "    line =' Unknown Inputs '\n",
    "    for i in range(self.num_regular_variables):\n",
    "      if i not in myTFTTools._known_regular_input_idx and i not in myTFTTools._input_obs_loc:\n",
    "        line += str(i) + ' '\n",
    "        n1 = 'UNKINPEmbed-Dense-'+str(num_unknown_inputs)\n",
    "        n2 = 'UNKINPEmbed-Time-'+str(num_unknown_inputs)\n",
    "        embedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.hidden_layer_size,name=n1), name=n2)\n",
    "        num_unknown_inputs += 1\n",
    "        self.convert_unknown_inputs.append(embedding)\n",
    "    print(line)\n",
    "\n",
    "# Embed Known Inputs\n",
    "    self.convert_known_regular_inputs = []\n",
    "    line = 'Known Inputs '\n",
    "    num_known_regular_inputs = 0\n",
    "    for i in myTFTTools._known_regular_input_idx:\n",
    "      if i not in myTFTTools._static_input_loc:\n",
    "        n1 = 'KnownINPEmbed-Dense-'+str(num_known_regular_inputs)\n",
    "        n2 = 'KnownINPEmbed-Time-'+str(num_known_regular_inputs)\n",
    "        embedding = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.hidden_layer_size,name=n1), name=n2)\n",
    "        num_known_regular_inputs += 1\n",
    "        self.convert_known_regular_inputs.append(embedding)\n",
    "        line += str(i) + ' '\n",
    "    print(line)\n",
    "    \n",
    "# Select Input Static Variables\n",
    "    self.ControlProcessStaticInput = ProcessStaticInput(self.hidden_layer_size,self.dropout_rate, self.num_static)\n",
    "\n",
    "    self.StaticGRN1 = GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, use_time_distributed=False, GRNname = 'Control1')\n",
    "    self.StaticGRN2 = GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, use_time_distributed=False, GRNname = 'Control2')\n",
    "    self.StaticGRN3 = GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, use_time_distributed=False, GRNname = 'Control3')\n",
    "    self.StaticGRN4 = GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, use_time_distributed=False, GRNname = 'Control4')\n",
    "                          \n",
    "# Select Input Dynamic Variables\n",
    "    self.ControlProcessDynamicInput1 = ProcessDynamicInput(self.hidden_layer_size, self.dropout_rate, \n",
    "               self.NumDynamicHistoryVariables, PDIname='Control1') \n",
    "    \n",
    "    if myTFTTools.TFTdefaultLSTM:\n",
    "      self.TFTLSTMEncoder = tf.compat.v1.keras.layers.CuDNNLSTM(\n",
    "              self.hidden_layer_size,\n",
    "              return_sequences=True,\n",
    "              return_state=True,\n",
    "              stateful=False,\n",
    "          )\n",
    "      self.TFTLSTMDecoder = tf.compat.v1.keras.layers.CuDNNLSTM(\n",
    "              self.hidden_layer_size,\n",
    "              return_sequences=True,\n",
    "              return_state=False,\n",
    "              stateful=False,\n",
    "          )\n",
    "    else:\n",
    "      self.TFTLSTMEncoder = TFTLSTMLayer( myTFTTools.TFTLSTMEncoderSecondLayer, myTFTTools.TFTLSTMEncoderThirdLayer, \n",
    "                myTFTTools.TFTLSTMEncoderInitialMLP, myTFTTools.TFTLSTMEncoderFinalMLP, \n",
    "                myTFTTools.number_LSTMnodes, self.hidden_layer_size, \n",
    "                myTFTTools.TFTLSTMEncoderactivationvalue, myTFTTools.TFTLSTMEncoderrecurrent_activation,\n",
    "                myTFTTools.TFTLSTMEncoderdropout1, myTFTTools.TFTLSTMEncoderrecurrent_dropout1, TFTreturn_state = True, LSTMname='ControlEncoder')\n",
    "      self.TFTLSTMDecoder = TFTLSTMLayer(myTFTTools.TFTLSTMDecoderSecondLayer, myTFTTools.TFTLSTMDecoderThirdLayer, \n",
    "                myTFTTools.TFTLSTMDecoderInitialMLP, myTFTTools.TFTLSTMDecoderFinalMLP, \n",
    "                myTFTTools.number_LSTMnodes, self.hidden_layer_size, \n",
    "                myTFTTools.TFTLSTMDecoderactivationvalue, myTFTTools.TFTLSTMDecoderrecurrent_activation,\n",
    "                myTFTTools.TFTLSTMDecoderdropout1, myTFTTools.TFTLSTMDecoderrecurrent_dropout1, TFTreturn_state = False, LSTMname='ControlDecoder')\n",
    "\n",
    "    self.TFTFullLSTMGLUplusskip = GLUplusskip(self.hidden_layer_size, self.dropout_rate, activation=None, \n",
    "                                              use_time_distributed=True, GLUname='ControlLSTM')\n",
    "    self.TemporalGRN5 = GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, use_additionalcontext = True,\n",
    "                            use_time_distributed=True, GRNname = 'Control5')\n",
    "\n",
    "    self.ControlProcessDynamicInput2 = ProcessDynamicInput(self.hidden_layer_size, self.dropout_rate, \n",
    "                                                              self.NumDynamicFutureVariables, PDIname='Control2')\n",
    "    \n",
    "\n",
    "# Decoder self attention\n",
    "    self.TFTself_attn_layer = TFTInterpretableMultiHeadAttention(\n",
    "        self.num_heads, self.hidden_layer_size, self.dropout_rate)\n",
    "    \n",
    "# Set up for final prediction\n",
    "    self.FinalGLUplusskip2 = []\n",
    "    self.FinalGLUplusskip3 = []\n",
    "    self.FinalGRN6 = []\n",
    "    for FinalGatingLoop in range(0, myTFTTools.FinalLoopSize):   \n",
    "      self.FinalGLUplusskip2.append(GLUplusskip(self.hidden_layer_size, self.dropout_rate, activation=None, \n",
    "                                          use_time_distributed=True, GLUname='ControlFinal2-'+str(FinalGatingLoop)))\n",
    "      self.FinalGLUplusskip3.append(GLUplusskip(self.hidden_layer_size, self.dropout_rate, activation=None, \n",
    "                                          use_time_distributed=True, GLUname='ControlFinal3-'+str(FinalGatingLoop)))\n",
    "      self.FinalGRN6.append(GRN(self.hidden_layer_size, dropout_rate=self.dropout_rate, use_time_distributed=True, GRNname = 'Control6-'+str(FinalGatingLoop)))\n",
    "\n",
    "\n",
    "# Final Processing\n",
    "    if myTFTTools.TFTLSTMFinalMLP > 0:\n",
    "      self.FinalApplyMLP = apply_mlp(myTFTTools.TFTLSTMFinalMLP,  output_size = myTFTTools.output_size * myTFTTools.NumberQuantiles,\n",
    "            output_activation = None, hidden_activation = 'selu',\n",
    "            use_time_distributed = True, MLPname='Predict')\n",
    "           \n",
    "\n",
    "    else:\n",
    "      if myTFTTools.FinalLoopSize == 1:\n",
    "        n1 = 'FinalTD'\n",
    "        n2 = 'FinalDense'\n",
    "        self.FinalLayer =  tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(myTFTTools.output_size * myTFTTools.NumberQuantiles, name = n2), name =n1)\n",
    "      else:\n",
    "        self.FinalStack =[]\n",
    "        localloopsize = myTFTTools.output_size * myTFTTools.NumberQuantiles\n",
    "        for localloop in range(0,localloopsize):\n",
    "            self.FinalStack.append(tf.keras.layers.Dense(1))\n",
    "\n",
    "# Called with each batch as input\n",
    "  #EAGER@tf.function\n",
    "  def call(self, all_inputs,  ignoredtime, ignoredidentifiers, training=None):\n",
    "# ignoredtime, ignoredidentifiers not used\n",
    "          \n",
    "    time_steps = myTFTTools.time_steps\n",
    "    combined_input_size = myTFTTools.input_size\n",
    "    encoder_steps = myTFTTools.num_encoder_steps          \n",
    "\n",
    "# Sanity checks on inputs\n",
    "    for InputIndex in myTFTTools._known_regular_input_idx:\n",
    "      if InputIndex in myTFTTools._input_obs_loc:\n",
    "        printexit('Observation cannot be known a priori!' + str(InputIndex))\n",
    "    for InputIndex in myTFTTools._input_obs_loc:\n",
    "      if InputIndex in myTFTTools._static_input_loc:\n",
    "        printexit('Observation cannot be static!' + str(InputIndex))\n",
    "\n",
    "    Sizefrominputs = all_inputs.get_shape().as_list()[-1]\n",
    "    if Sizefrominputs != myTFTTools.input_size:\n",
    "      raise printexit('Illegal number of inputs! Inputs observed={}, expected={}'.format(\n",
    "              Sizefrominputs, myTFTTools.input_size))\n",
    "\n",
    "    regular_inputs, categorical_inputs = all_inputs[:, :, :self.num_regular_variables], all_inputs[:, :, self.num_regular_variables:]\n",
    "\n",
    "# Embed categories of all categorical variables -- static and Dynamic\n",
    "# categorical variables MUST be at end and reordering done in preprocessing (definition of train valid test)\n",
    "# XXX add reordering\n",
    "    categoricalembedded_inputs = []\n",
    "    for i in range(0,self.num_categorical_variables):\n",
    "      categoricalembedded_inputs.append( CatVariablesembeddings[i](categorical_inputs[Ellipsis, i]) )\n",
    "\n",
    "# Complete Static Variables -- whether categorical or regular -- they are essentially thought of as known inputs\n",
    "    if myTFTTools._static_input_loc:\n",
    "      static_inputs = []\n",
    "      numstatic = 0\n",
    "      for i in range(self.num_regular_variables):\n",
    "        if i in myTFTTools._static_input_loc:\n",
    "          static_inputs.append(self.StaticInitialembeddings[numstatic](regular_inputs[:, 0, i:i + 1]) )\n",
    "          numstatic += 1\n",
    "      static_inputs = static_inputs + [self.categoricalembedded_inputs[i][:, 0, :]\n",
    "             for i in range(self.num_categorical_variables)\n",
    "             if i + self.num_regular_variables in myTFTTools._static_input_loc]\n",
    "      static_inputs = tf.stack(static_inputs, axis=1)\n",
    "    else:\n",
    "      static_inputs = None\n",
    "\n",
    "# Targets misleadingly labelled obs_inputs. They are used as targets to predict and as observed inputs\n",
    "    obs_inputs = []\n",
    "    num_obs_inputs = 0\n",
    "    for i in myTFTTools._input_obs_loc:\n",
    "      e = self.convert_obs_inputs[num_obs_inputs](regular_inputs[Ellipsis, i:i + 1])\n",
    "      num_obs_inputs += 1\n",
    "      obs_inputs.append(e)\n",
    "    obs_inputs = tf.stack(obs_inputs, axis=-1)\n",
    "\n",
    "# Categorical Unknown inputs. Unknown + Target is complete Observed InputCategory \n",
    "    categorical_unknown_inputs = []\n",
    "    for i in range(self.num_categorical_variables):\n",
    "      if i not in myTFTTools._known_categorical_input_idx and i + self.num_regular_variables not in myTFTTools._input_obs_loc:\n",
    "        e = self.categoricalembedded_inputs[i]\n",
    "        categorical_unknown_inputs.append(e)\n",
    "\n",
    "# Regular Unknown inputs \n",
    "    unknown_inputs = []\n",
    "    num_unknown_inputs = 0\n",
    "    for i in range(self.num_regular_variables):\n",
    "      if i not in myTFTTools._known_regular_input_idx and i not in myTFTTools._input_obs_loc:\n",
    "        e = self.convert_unknown_inputs[num_unknown_inputs](regular_inputs[Ellipsis, i:i + 1])\n",
    "        num_unknown_inputs += 1\n",
    "        unknown_inputs.append(e)\n",
    "\n",
    "# Add in categorical_unknown_inputs into unknown_inputs\n",
    "    if unknown_inputs + categorical_unknown_inputs:\n",
    "      unknown_inputs = tf.stack(unknown_inputs + categorical_unknown_inputs, axis=-1)\n",
    "    else:\n",
    "      unknown_inputs = None\n",
    "\n",
    "# A priori known inputs\n",
    "    known_regular_inputs = []\n",
    "    num_known_regular_inputs = 0\n",
    "    for i in myTFTTools._known_regular_input_idx:\n",
    "      if i not in myTFTTools._static_input_loc:\n",
    "        e = self.convert_known_regular_inputs[num_known_regular_inputs](regular_inputs[Ellipsis, i:i + 1])\n",
    "        num_known_regular_inputs += 1\n",
    "        known_regular_inputs.append(e)\n",
    "\n",
    "    known_categorical_inputs = []\n",
    "    for i in myTFTTools._known_categorical_input_idx:\n",
    "      if i + self.num_regular_variables not in myTFTTools._static_input_loc:\n",
    "        e = categoricalembedded_inputs[i]\n",
    "        known_categorical_inputs.append(e)\n",
    "\n",
    "    known_combined_layer = tf.stack(known_regular_inputs + known_categorical_inputs, axis=-1)\n",
    "\n",
    " #  Now we know unknown_inputs, known_combined_layer, obs_inputs, static_inputs\n",
    "\n",
    "# Identify known and observed historical_inputs.\n",
    "    if unknown_inputs is not None:\n",
    "      historical_inputs = tf.concat([\n",
    "          unknown_inputs[:, :encoder_steps, :],\n",
    "          known_combined_layer[:, :encoder_steps, :],\n",
    "          obs_inputs[:, :encoder_steps, :]\n",
    "      ], axis=-1)\n",
    "    else:\n",
    "      historical_inputs = tf.concat([\n",
    "          known_combined_layer[:, :encoder_steps, :],\n",
    "          obs_inputs[:, :encoder_steps, :]\n",
    "      ], axis=-1)\n",
    "\n",
    "# Identify known future inputs.\n",
    "    future_inputs = known_combined_layer[:, encoder_steps:, :]\n",
    "\n",
    "# Process Static Variables\n",
    "    static_encoder, static_weights = self.ControlProcessStaticInput(static_inputs)\n",
    "    static_context_variable_selection = self.StaticGRN1(static_encoder)\n",
    "    static_context_enrichment = self.StaticGRN2(static_encoder)\n",
    "    static_context_state_h = self.StaticGRN3(static_encoder)\n",
    "    static_context_state_c = self.StaticGRN4(static_encoder)\n",
    "# End set up of static variables\n",
    "\n",
    "    historical_features, historical_flags, _ = self.ControlProcessDynamicInput1(historical_inputs, \n",
    "                static_context_variable_selection = static_context_variable_selection)\n",
    "    \n",
    "    history_lstm, state_h, state_c = self.TFTLSTMEncoder(historical_features, initial_state = [static_context_state_h, static_context_state_c])\n",
    "    \n",
    "    input_embeddings = historical_features\n",
    "    lstm_layer = history_lstm \n",
    "\n",
    "    future_features, future_flags, _ = self.ControlProcessDynamicInput2(future_inputs, static_context_variable_selection = static_context_variable_selection)\n",
    "    \n",
    "    future_lstm = self.TFTLSTMDecoder(future_features, initial_state= [state_h, state_c])\n",
    "    input_embeddings = tf.concat([historical_features, future_features], axis=1)\n",
    "    lstm_layer = tf.concat([history_lstm, future_lstm], axis=1)\n",
    "\n",
    "    temporal_feature_layer, _ = self.TFTFullLSTMGLUplusskip(lstm_layer, input_embeddings)\n",
    "    expanded_static_context = tf.expand_dims(static_context_enrichment, axis=1) # Add fake time axis\n",
    "    enriched = self.TemporalGRN5(temporal_feature_layer, additional_context=expanded_static_context, return_gate=False)\n",
    "\n",
    "# Calculate attention\n",
    "# mask does not use \"time\" as implicit in order of entries in window\n",
    "    mask = TFTget_decoder_mask(enriched)\n",
    "    x, self_att = self.TFTself_attn_layer(enriched, enriched, enriched, mask=mask)\n",
    "\n",
    "    if myTFTTools.FinalLoopSize > 1:\n",
    "        StackLayers = []\n",
    "\n",
    "    for FinalGatingLoop in range(0, myTFTTools.FinalLoopSize):\n",
    "      x, _ = self.FinalGLUplusskip2[FinalGatingLoop](x,enriched)\n",
    "\n",
    "      # Nonlinear processing on outputs\n",
    "      decoder = self.FinalGRN6[FinalGatingLoop](x)\n",
    "\n",
    "      # Final skip connection\n",
    "      transformer_layer, _ = self.FinalGLUplusskip3[FinalGatingLoop](decoder, temporal_feature_layer)\n",
    "      \n",
    "      if myTFTTools.FinalLoopSize > 1:\n",
    "            StackLayers.append(transformer_layer)\n",
    "# End Loop over FinalGatingLoop\n",
    "\n",
    "    if myTFTTools.FinalLoopSize > 1:\n",
    "      transformer_layer = tf.stack(StackLayers, axis=-1)\n",
    "\n",
    "    # Attention components for explainability IGNORED\n",
    "    attention_components = {\n",
    "        # Temporal attention weights\n",
    "        'decoder_self_attn': self_att,\n",
    "        # Static variable selection weights\n",
    "        'static_flags': static_weights[Ellipsis, 0],\n",
    "        # Variable selection weights of past inputs\n",
    "        'historical_flags': historical_flags[Ellipsis, 0, :],\n",
    "        # Variable selection weights of future inputs\n",
    "        'future_flags': future_flags[Ellipsis, 0, :]\n",
    "    }\n",
    "    self._attention_components = attention_components\n",
    "\n",
    "    # Original split procerssing here and did\n",
    "    # return transformer_layer, all_inputs, attention_components\n",
    "\n",
    "    if myTFTTools.TFTLSTMFinalMLP > 0:\n",
    "      outputs = self.FinalApplyMLP(transformer_layer[Ellipsis, encoder_steps:, :])\n",
    "\n",
    "    else:\n",
    "      if myTFTTools.FinalLoopSize == 1:\n",
    "        outputs = self.FinalLayer(transformer_layer[Ellipsis, encoder_steps:, :])\n",
    "      else:\n",
    "        outputstack =[]\n",
    "        localloopsize = myTFTTools.output_size * myTFTTools.NumberQuantiles\n",
    "        for localloop in range(0,localloopsize):\n",
    "          localoutput = self.FinalStack[localloop](transformer_layer[Ellipsis, encoder_steps:, :, localloop])\n",
    "          outputstack.append(localoutput)\n",
    "        outputs = tf.stack(outputstack, axis=-2)\n",
    "        outputs = tf.squeeze(outputs, axis=-1)\n",
    "\n",
    "    return outputs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckwZHK12xwCY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##TFT Run & Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV-u9bfz6OIL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###TFT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01KS6sEG6VSw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def TFTTestpredict(custommodel,datacollection):\n",
    "  \"\"\"Computes predictions for a given input dataset.\n",
    "\n",
    "  Args:\n",
    "    df: Input dataframe\n",
    "    return_targets: Whether to also return outputs aligned with predictions to\n",
    "      faciliate evaluation\n",
    "\n",
    "  Returns:\n",
    "    Input dataframe or tuple of (input dataframe, algined output dataframe).\n",
    "  \"\"\"\n",
    "  inputs = datacollection['inputs']\n",
    "  time = datacollection['time']\n",
    "  identifier = datacollection['identifier']\n",
    "  outputs = datacollection['outputs']\n",
    "  print(inputs.shape)\n",
    "  print(time.shape)\n",
    "  print(identifier.shape)\n",
    "  print(outputs.shape)\n",
    "\n",
    "  combined = None\n",
    "  myTFTTools.PrintTitle('Start TFTTestpredict')\n",
    "  OuterBatchDimension = inputs.shape[0]\n",
    "  batchsize = myTFTTools.maxibatch_size\n",
    "  numberoftestbatches = math.ceil(OuterBatchDimension/batchsize)\n",
    "  count1 = 0\n",
    "  for countbatches in range(0,numberoftestbatches):\n",
    "    count2 = min(OuterBatchDimension, count1+batchsize)\n",
    "    if count2 <= count1:\n",
    "      continue\n",
    "    samples = np.arange(count1,count2)\n",
    "    count1 += batchsize\n",
    "    X_test = inputs[samples,Ellipsis]\n",
    "\n",
    "    time_test = []\n",
    "    id_test =[]\n",
    "    Numinbatch = X_test.shape[0]\n",
    "    if myTFTTools.TFTSymbolicWindows:\n",
    "      X_test = X_test.numpy()          \n",
    "      X_test = np.reshape(X_test,Numinbatch)\n",
    "      iseqarray = np.right_shift(X_test,16)\n",
    "      ilocarray = np.bitwise_and(X_test, 0b1111111111111111)\n",
    "      X_testFull = list()\n",
    "      for iloc in range(0,Numinbatch):\n",
    "        X_testFull.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "      X_test = np.array(X_testFull)\n",
    "\n",
    "    batchprediction = custommodel(X_test, time_test, id_test, training=False).numpy()\n",
    "    if combined is None:\n",
    "      combined = batchprediction\n",
    "    else:\n",
    "      combined = np.concatenate((combined, batchprediction),axis=0)\n",
    "\n",
    "  # Extract predictions for each quantile into different entries\n",
    "  process_map = {\n",
    "      qname:\n",
    "      combined[Ellipsis, i * myTFTTools.output_size:(i + 1) * myTFTTools.output_size]\n",
    "      for i, qname in enumerate(myTFTTools.Quantilenames)\n",
    "  }\n",
    "  process_map['targets'] = outputs\n",
    "  myTFTTools.PrintTitle('End TFTTestpredict')\n",
    "\n",
    "  def format_outputs(prediction):\n",
    "    \"\"\"Returns formatted dataframes for prediction.\"\"\"\n",
    "\n",
    "    reshapedprediction = prediction.reshape(prediction.shape[0], -1)\n",
    "    flat_prediction = pd.DataFrame(\n",
    "        reshapedprediction[:, :],\n",
    "        columns=[\n",
    "            't+{}-Obs{}'.format(i, j)\n",
    "            for i in range(myTFTTools.time_steps - myTFTTools.num_encoder_steps)\n",
    "            for j in range(0, myTFTTools.output_size)\n",
    "        ])\n",
    "    cols = list(flat_prediction.columns)\n",
    "    flat_prediction['forecast_time'] = time[:,\n",
    "                                            myTFTTools.num_encoder_steps - 1, 0]\n",
    "    flat_prediction['identifier'] = identifier[:, 0, 0]\n",
    "\n",
    "# Arrange in order\n",
    "    return flat_prediction[['forecast_time', 'identifier'] + cols]\n",
    "    \n",
    "  return {k: format_outputs(process_map[k]) for k in process_map}\n",
    "\n",
    "  # Simple Plot of Loss from history\n",
    "def finalizeTFTDL(ActualModel, recordtrainloss, recordvalloss, validationfrac, test_datacollection, modelflag, LabelFit =''):\n",
    "\n",
    "# Ouput Loss v Epoch\n",
    "  histlen = len(recordtrainloss)\n",
    "  trainloss = recordtrainloss[histlen-1]\n",
    "  plt.rcParams[\"figure.figsize\"] = [8,6]\n",
    "  plt.plot(recordtrainloss)\n",
    "  if (validationfrac > 0.001) and len(recordvalloss) > 0:\n",
    "    valloss = recordvalloss[histlen-1]\n",
    "    plt.plot(recordvalloss)\n",
    "  else:\n",
    "    valloss = 0.0\n",
    "  \n",
    "  current_time = timenow()\n",
    "  print(startbold + startred + current_time + ' ' + RunName + ' finalizeDL ' + RunComment +resetfonts)\n",
    "  plt.title(LabelFit + ' ' + RunName+' model loss ' + str(round(trainloss,7)) + ' Val ' + str(round(valloss,7)))\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.yscale(\"log\")\n",
    "  plt.grid(True)\n",
    "  plt.legend(['train', 'val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "# Setup TFT  \n",
    "  if modelflag == 2:\n",
    "    global SkipDL2F, IncreaseNloc_sample, DecreaseNloc_sample\n",
    "    SkipDL2F = True\n",
    "    IncreaseNloc_sample = 1\n",
    "    DecreaseNloc_sample = 1\n",
    "    TFToutput_map = TFTTestpredict(ActualModel,test_datacollection)\n",
    "    VisualizeTFT(ActualModel, TFToutput_map)\n",
    "  else:\n",
    "    printexit(\"unsupported model \" +str(modelflag))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysYaHvNAuxFe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###TFTcustommodel\n",
    "\n",
    "Control Full TFT Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_kBmZ0xu3eW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TFTcustommodel(tf.keras.Model):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(TFTcustommodel, self).__init__(**kwargs)\n",
    "    self.myTFTFullNetwork = TFTFullNetwork()\n",
    "\n",
    "  def compile(self, optimizer,  loss):\n",
    "      super(TFTcustommodel, self).compile()\n",
    "      if optimizer == 'adam':\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=myTFTTools.learning_rate)\n",
    "      else:\n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "      Dictopt = self.optimizer.get_config()\n",
    "      print(startbold+startred + 'Optimizer ' + resetfonts, Dictopt)\n",
    "\n",
    "      if loss == 'MSE' or loss =='mse':\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError()\n",
    "      elif loss == 'MAE' or loss =='mae':\n",
    "        self.loss_object = tf.keras.losses.MeanAbsoluteError()        \n",
    "      else:\n",
    "        self.loss_object = loss\n",
    "      self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "      self.loss_tracker.reset_states()\n",
    "      self.val_tracker = tf.keras.metrics.Mean(name=\"val\")\n",
    "      self.val_tracker.reset_states()\n",
    "      return\n",
    "\n",
    "  def resetmetrics(self):\n",
    "      self.loss_tracker.reset_states()\n",
    "      self.val_tracker.reset_states()\n",
    "      return\n",
    "\n",
    "  def build_graph(self, shapes):\n",
    "    input = tf.keras.layers.Input(shape=shapes, name=\"Input\")\n",
    "    return tf.keras.models.Model(inputs=[input], outputs=[self.call(input)])\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "    if len(data) == 5:\n",
    "      X_train, y_train, sw_train, time_train, id_train = data\n",
    "    else:\n",
    "      X_train, y_train = data\n",
    "      sw_train = []\n",
    "      time_train = []\n",
    "      id_train = []\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = self(X_train, time_train, id_train, training=True)\n",
    "#      loss = self.loss_object(y_train, predictions, sw_train)\n",
    "      loss = self.loss_object(y_train, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "    self.loss_tracker.update_state(loss)\n",
    "    return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data):\n",
    "    if len(data) == 5:\n",
    "      X_val, y_val, sw_val, time_val, id_val = data\n",
    "    else:\n",
    "      X_val, y_val = data\n",
    "      sw_val = []\n",
    "      time_train = []\n",
    "      id_train = []\n",
    "\n",
    "    predictions = self(X_val, time_val, id_val, training=False)\n",
    "#    loss = self.loss_object(y_val, predictions, sw_val)\n",
    "    loss = self.loss_object(y_val, predictions)\n",
    "\n",
    "    self.val_tracker.update_state(loss)\n",
    "    return {\"val_loss\": self.val_tracker.result()}\n",
    "\n",
    "  #@tf.function\n",
    "  def call(self, inputs, time, identifier, training=None):  \n",
    "    predictions = self.myTFTFullNetwork(inputs, time, identifier, training=training)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfb6ttCt8EHI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TFT Overall Batch Training\n",
    "\n",
    "* TIME not set explicitly\n",
    "* Weights allowed or not\n",
    "* Assumes TFTFullNetwork is full Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqBnNKee8RYN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def RunTFTCustomVersion():\n",
    "\n",
    "  myTFTTools.PrintTitle(\"Start Tensorflow\")\n",
    "\n",
    "  global AnyOldValidation\n",
    "  UseClassweights = False\n",
    "  usecustomfit = True\n",
    "  AnyOldValidation = myTFTTools.validation\n",
    "\n",
    "  garbagecollectcall = 0\n",
    "\n",
    "\n",
    "# XXX InitializeDLforTimeSeries setSeparateDLinput NOT USED\n",
    "  tf.keras.backend.set_floatx('float32')\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "  myTFTcustommodel = TFTcustommodel(name ='myTFTcustommodel')\n",
    "  lossobject = 'MSE'\n",
    "  if myTFTTools.lossflag == 8:\n",
    "    lossobject = custom_lossGCF1\n",
    "  if myTFTTools.lossflag == 11:\n",
    "    lossobject = 'MAE'\n",
    "  if myTFTTools.lossflag == 12:\n",
    "    lossobject = tf.keras.losses.Huber(delta=myTFTTools.HuberLosscut)\n",
    "  myTFTcustommodel.compile(loss= lossobject, optimizer= myTFTTools.optimizer)\n",
    "\n",
    "  recordtrainloss = []\n",
    "  recordvalloss = []\n",
    "  tfrecordtrainloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "  tfrecordvalloss = tf.Variable([],  shape =tf.TensorShape(None), trainable = False)\n",
    "  tfepochstep = tf.Variable(0, trainable = False)\n",
    "\n",
    "# Set up checkpoints to read or write\n",
    "  mycheckpoint = tf.train.Checkpoint(optimizer=myTFTcustommodel.optimizer, \n",
    "                                     model=myTFTcustommodel, tfepochstep=tf.Variable(0),\n",
    "                                     tfrecordtrainloss=tfrecordtrainloss,tfrecordvalloss=tfrecordvalloss)\n",
    "    \n",
    "# This restores back up\n",
    "  if Restorefromcheckpoint:\n",
    "    save_path = inputCHECKPOINTDIR + inputRunName + inputCheckpointpostfix\n",
    "    mycheckpoint.restore(save_path=save_path).expect_partial()\n",
    "    tfepochstep  = mycheckpoint.tfepochstep \n",
    "    recordvalloss = mycheckpoint.tfrecordvalloss.numpy().tolist()\n",
    "    recordtrainloss = mycheckpoint.tfrecordtrainloss.numpy().tolist()\n",
    "    trainlen = len(recordtrainloss)\n",
    "    extrainfo = ''\n",
    "    vallen = len(recordvalloss)\n",
    "    SavedTrainLoss = recordtrainloss[trainlen-1]\n",
    "    SavedValLoss = 0.0\n",
    "    if vallen > 0:\n",
    "      extrainfo = ' Val Loss ' + str(round(recordvalloss[vallen-1],7))\n",
    "      SavedValLoss = recordvalloss[vallen-1]\n",
    "    print(startbold + 'Network restored from ' + save_path + '\\nLoss ' + str(round(recordtrainloss[trainlen-1],7)) \n",
    "      + extrainfo + ' Epochs ' + str(tfepochstep.numpy()) + resetfonts )\n",
    "    TFTTrainingMonitor.SetCheckpointParms(mycheckpoint,CHECKPOINTDIR,RunName = RunName,Restoredcheckpoint= True, \n",
    "            Restored_path = save_path,  ValidationFraction = AnyOldValidation, SavedTrainLoss = SavedTrainLoss, \n",
    "            SavedValLoss =SavedValLoss)\n",
    "  else:\n",
    "    TFTTrainingMonitor.SetCheckpointParms(mycheckpoint,CHECKPOINTDIR,RunName = RunName,Restoredcheckpoint= False, \n",
    "                                      ValidationFraction = AnyOldValidation)\n",
    "\n",
    "# This just does analysis      \n",
    "  if AnalysisOnly:\n",
    "    if OutputNetworkPictures:\n",
    "      outputpicture1 = APPLDIR +'/Outputs/Model_' +RunName + '1.png'\n",
    "      outputpicture2 = APPLDIR +'/Outputs/Model_' +RunName + '2.png'\n",
    "      tf.keras.utils.plot_model(myTFTcustommodel.build_graph([Tseq,NpropperseqTOT]), \n",
    "                          show_shapes=True, to_file = outputpicture1,\n",
    "                          show_dtype=True, \n",
    "                          expand_nested=True)\n",
    "      tf.keras.utils.plot_model(myTFTcustommodel.myTFTFullNetwork.build_graph([Tseq,NpropperseqTOT]), \n",
    "                          show_shapes=True, to_file = outputpicture2,\n",
    "                          show_dtype=True, \n",
    "                          expand_nested=True)\n",
    "    if myTFTTools.TFTSymbolicWindows:\n",
    "      finalizeTFTDL(myTFTcustommodel,recordtrainloss,recordvalloss,AnyOldValidation,TFTtest_datacollection,2, LabelFit = 'Custom TFT Fit')\n",
    "    else:\n",
    "      finalizeTFTDL(myTFTcustommodel,recordtrainloss,recordvalloss,AnyOldValidation,TFTtest_datacollection,2, LabelFit = 'Custom TFT Fit')\n",
    "      return\n",
    "\n",
    "# Initialize progress bars\n",
    "  epochsize = len(TFTtrain_datacollection[\"inputs\"])\n",
    "  if AnyOldValidation > 0.001:\n",
    "    epochsize += len(TFTval_datacollection[\"inputs\"])\n",
    "\n",
    "  pbar = notebook.trange(myTFTTools.num_epochs, desc='Training loop', unit ='epoch')\n",
    "  bbar = notebook.trange(epochsize,  desc='Batch    loop', unit  = 'sample')\n",
    "\n",
    "  train_epoch = 0.0 # Training Loss this epoch\n",
    "  val_epoch = 0.0 # Validation Loss this epoch\n",
    "\n",
    "  Ctime1 = 0.0\n",
    "  Ctime2 = 0.0\n",
    "  Ctime3 = 0.0\n",
    "  GarbageCollect = True\n",
    "\n",
    "#  train_dataset = tf.data.Dataset.from_tensor_slices((TFTtrain_datacollection['inputs'],TFTtrain_datacollection['outputs'],TFTtrain_datacollection['active_entries']))\n",
    "#  val_dataset = tf.data.Dataset.from_tensor_slices((TFTval_datacollection['inputs'],TFTval_datacollection['outputs'],TFTval_datacollection['active_entries']))\n",
    "  OuterTrainBatchDimension = TFTtrain_datacollection['inputs'].shape[0]\n",
    "  OuterValBatchDimension = TFTval_datacollection['inputs'].shape[0]\n",
    "  print('Samples to batch Train ' + str(OuterTrainBatchDimension) + ' Val ' + str(OuterValBatchDimension))\n",
    "#  train_dataset = train_dataset.shuffle(buffer_size = OuterBatchDimension, reshuffle_each_iteration=True).batch(myTFTTools.minibatch_size)\n",
    "#  val_dataset = val_dataset.batch(myTFTTools.maxibatch_size)\n",
    "  np.random.seed(int.from_bytes(os.urandom(4), byteorder='little'))\n",
    "\n",
    "  trainbatchsize = myTFTTools.minibatch_size\n",
    "  valbatchsize = myTFTTools.maxibatch_size\n",
    "  numberoftrainbatches = math.ceil(OuterTrainBatchDimension/trainbatchsize)\n",
    "  numberofvalbatches = math.ceil(OuterValBatchDimension/valbatchsize)\n",
    "\n",
    "  for e in pbar:\n",
    "    myTFTcustommodel.resetmetrics()\n",
    "    train_lossoverbatch=[]\n",
    "    val_lossoverbatch=[]\n",
    "    \n",
    "    if batchperepoch:\n",
    "      qbar = notebook.trange(epochsize, desc='Batch loop epoch ' +str(e))\n",
    "\n",
    "#   for batch, (X_train, y_train, sw_train) in enumerate(train_dataset.take(-1))\n",
    "    trainingorder = np.arange(0, OuterTrainBatchDimension)\n",
    "    np.random.shuffle(trainingorder)\n",
    "    count1 = 0\n",
    "\n",
    "    for countbatches in range(0,numberoftrainbatches):\n",
    "      count2 = min(OuterTrainBatchDimension, count1+trainbatchsize)\n",
    "      if count2 <= count1:\n",
    "        continue\n",
    "      samples = trainingorder[count1:count2]\n",
    "      count1 += trainbatchsize\n",
    "      X_train = TFTtrain_datacollection['inputs'][samples,Ellipsis]\n",
    "      y_train = TFTtrain_datacollection['outputs'][samples,Ellipsis]\n",
    "      sw_train = []\n",
    "      time_train = []\n",
    "      id_train = []\n",
    "      Numinbatch = X_train.shape[0]\n",
    "      # myTFTTools.TFTSymbolicWindows X_train is indexed by Batch index, 1(replace by Window), 1 (replace by properties)\n",
    "      if myTFTTools.TFTSymbolicWindows:\n",
    "        StopWatch.start('label1')\n",
    "        X_train = X_train.numpy()          \n",
    "        X_train = np.reshape(X_train,Numinbatch)\n",
    "        iseqarray = np.right_shift(X_train,16)\n",
    "        ilocarray = np.bitwise_and(X_train, 0b1111111111111111)\n",
    "        StopWatch.stop('label1')\n",
    "        Ctime1 += StopWatch.get('label1', digits=4)\n",
    "        StopWatch.start('label3')\n",
    "        X_train_withSeq = list()\n",
    "        for iloc in range(0,Numinbatch):\n",
    "          X_train_withSeq.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "#         X_train_withSeq=[ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq] for iloc in range(0,Numinbatch)]\n",
    "        StopWatch.stop('label3')\n",
    "        Ctime3 += StopWatch.get('label3', digits=5)\n",
    "        StopWatch.start('label2')\n",
    "        loss = myTFTcustommodel.train_step((np.array(X_train_withSeq), y_train, sw_train, time_train,id_train))\n",
    "        StopWatch.stop('label2')\n",
    "        Ctime2 += StopWatch.get('label2', digits=4)\n",
    "\n",
    "      else:\n",
    "        loss = myTFTcustommodel.train_step((X_train, y_train, sw_train, time_train, id_train))\n",
    "\n",
    "      GarbageCollect = False\n",
    "      if GarbageCollect:\n",
    "        if myTFTTools.TFTSymbolicWindows:\n",
    "          X_train_withSeq = None\n",
    "        X_train = None\n",
    "        y_train = None\n",
    "        sw_train = None\n",
    "        time_train = None\n",
    "        id_train = None\n",
    "        if garbagecollectcall > GarbageCollectionLimit:\n",
    "          garbagecollectcall = 0\n",
    "          gc.collect()\n",
    "        garbagecollectcall += 1\n",
    "\n",
    "      localloss = loss[\"loss\"].numpy()\n",
    "      train_lossoverbatch.append(localloss)\n",
    "\n",
    "      if batchperepoch:\n",
    "        qbar.update(LSTMbatch_size)\n",
    "        qbar.set_postfix(Loss = localloss, Epoch = e)\n",
    "      bbar.update(Numinbatch)\n",
    "      bbar.set_postfix(Loss = localloss, Epoch = e)\n",
    "# End Training step for one batch\n",
    "\n",
    "# Start Validation \n",
    "    if AnyOldValidation:\n",
    "      count1 = 0\n",
    "      for countbatches in range(0,numberofvalbatches):\n",
    "        count2 = min(OuterValBatchDimension, count1+valbatchsize)\n",
    "        if count2 <= count1:\n",
    "          continue\n",
    "        samples = np.arange(count1,count2)\n",
    "        count1 += valbatchsize\n",
    "        X_val = TFTval_datacollection['inputs'][samples,Ellipsis]\n",
    "        y_val = TFTval_datacollection['outputs'][samples,Ellipsis]\n",
    "        sw_val = []\n",
    "#      for batch, (X_val, y_val, sw_val) in enumerate(val_dataset.take(-1)):\n",
    "        time_val = []\n",
    "        id_val =[]\n",
    "        Numinbatch = X_val.shape[0]\n",
    "        # myTFTTools.TFTSymbolicWindows X_val is indexed by Batch index, 1(replace by Window), 1 (replace by properties)\n",
    "        if myTFTTools.TFTSymbolicWindows:\n",
    "          StopWatch.start('label1')\n",
    "          X_val = X_val.numpy()          \n",
    "          X_val = np.reshape(X_val,Numinbatch)\n",
    "          iseqarray = np.right_shift(X_val,16)\n",
    "          ilocarray = np.bitwise_and(X_val, 0b1111111111111111)\n",
    "          StopWatch.stop('label1')\n",
    "          Ctime1 += StopWatch.get('label1', digits=4)\n",
    "          StopWatch.start('label3')\n",
    "          X_valFull = list()\n",
    "          for iloc in range(0,Numinbatch):\n",
    "            X_valFull.append(ReshapedSequencesTOT[ilocarray[iloc],iseqarray[iloc]:iseqarray[iloc]+Tseq])\n",
    "          StopWatch.stop('label3')\n",
    "          Ctime3 += StopWatch.get('label3', digits=5)\n",
    "          StopWatch.start('label2')\n",
    "          loss = myTFTcustommodel.test_step((np.array(X_valFull), y_val, sw_val, time_val, id_val))\n",
    "          StopWatch.stop('label2')\n",
    "          Ctime2 += StopWatch.get('label2', digits=4)\n",
    "\n",
    "        else:\n",
    "          loss = myTFTcustommodel.test_step((X_val, y_val, sw_val, time_val, id_val))\n",
    "\n",
    "        localval = loss[\"val_loss\"].numpy()\n",
    "        val_lossoverbatch.append(localval)\n",
    "        \n",
    "        bbar.update(Numinbatch)\n",
    "        bbar.set_postfix(Val_loss = localval, Epoch = e)\n",
    "# End Batch\n",
    "\n",
    "    train_epoch = train_lossoverbatch[-1]\n",
    "    recordtrainloss.append(train_epoch)\n",
    "    mycheckpoint.tfrecordtrainloss = tf.Variable(recordtrainloss)\n",
    "    '''\n",
    "    line = 'Train ' + str(round(np.mean(train_lossoverbatch),5)) + ' '\n",
    "    count = 0\n",
    "    for x in train_lossoverbatch:\n",
    "      if count%100 == 0:\n",
    "        line = line + str(count) +':' + str(round(x,5)) + ' '\n",
    "      count += 1\n",
    "    print(wraptotext(line,size=180))\n",
    "    '''\n",
    "    val_epoch = 0.0\n",
    "    if AnyOldValidation > 0.001:\n",
    "      val_epoch = val_lossoverbatch[-1]\n",
    "      recordvalloss.append(val_epoch)\n",
    "      mycheckpoint.tfrecordvalloss = tf.Variable(recordvalloss)\n",
    "      '''\n",
    "      line = 'Val ' + str(round(np.mean(val_lossoverbatch),5)) + ' '\n",
    "      count = 0\n",
    "      for x in val_lossoverbatch:\n",
    "        if count%100 == 0:\n",
    "          line = line + str(count) +':' + str(round(x,5)) + ' '\n",
    "        count += 1\n",
    "      print(wraptotext(line,size=180))\n",
    "      '''\n",
    "\n",
    "    pbar.set_postfix(Loss = train_epoch, Val = val_epoch)\n",
    "    bbar.reset()\n",
    "    tfepochstep = tfepochstep + 1\n",
    "    mycheckpoint.tfepochstep.assign(tfepochstep)\n",
    "\n",
    "# Decide on best fit\n",
    "    MonitorResult, train_epoch, val_epoch = TFTTrainingMonitor.EpochEvaluate(e,train_epoch, val_epoch, \n",
    "        tfepochstep, recordtrainloss, recordvalloss)\n",
    "    if MonitorResult==1:\n",
    "      tfepochstep, recordtrainloss, recordvalloss, train_epoch, val_epoch = TFTTrainingMonitor.RestoreBestFit() # Restore Best Fit\n",
    "    else:\n",
    "      continue\n",
    "# *********************** End of Epoch Loop\n",
    "\n",
    "# Print Fit details\n",
    "  print(startbold + 'Times ' + str(round(Ctime1,5))  + ' ' + str(round(Ctime3,5)) + ' TF ' + str(round(Ctime2,5)) + resetfonts)\n",
    "  TFTTrainingMonitor.PrintEndofFit(TFTTransformerepochs)\n",
    "\n",
    "# Set Best Possible Fit\n",
    "  tfepochstep, recordtrainloss, recordvalloss, train_epoch, val_epoch = TFTTrainingMonitor.BestPossibleFit()\n",
    "\n",
    "  if Checkpointfinalstate:\n",
    "    savepath = mycheckpoint.save(file_prefix=CHECKPOINTDIR + RunName)\n",
    "    print('Checkpoint at ' + savepath + ' from ' + CHECKPOINTDIR)\n",
    "  trainlen = len(recordtrainloss)\n",
    "  extrainfo = ''\n",
    "  if AnyOldValidation > 0.001:\n",
    "    vallen = len(recordvalloss)\n",
    "    extrainfo = ' Val Epoch ' + str(vallen-1) + ' Val Loss ' + str(round(recordvalloss[vallen-1],7))\n",
    "  print('Train Epoch ' + str(trainlen-1) + ' Train Loss ' + str(round(recordtrainloss[trainlen-1],7)) + extrainfo)\n",
    "\n",
    " #\n",
    "  myTFTcustommodel.summary()\n",
    "  print('\\nmyTFTcustommodel.myTFTFullNetwork **************************************')\n",
    "  myTFTcustommodel.myTFTFullNetwork.summary()\n",
    "  print('\\nmyTFTcustommodel.myTFTFullNetwork.TFTLSTMEncoder **************************************')\n",
    "  if not myTFTTools.TFTdefaultLSTM:\n",
    "    myTFTcustommodel.myTFTFullNetwork.TFTLSTMEncoder.summary()\n",
    "    print('\\nmyTFTcustommodel.myTFTFullNetwork.TFTLSTMDecoder **************************************')\n",
    "    myTFTcustommodel.myTFTFullNetwork.TFTLSTMEncoder.summary()\n",
    "  print('\\nmyTFTcustommodel.myTFTFullNetwork.TFTself_attn_layer **************************************')\n",
    "  myTFTcustommodel.myTFTFullNetwork.TFTself_attn_layer.summary()\n",
    "  myTFTcustommodel.myTFTFullNetwork.TFTself_attn_layer.attention.summary()\n",
    "  \n",
    "  if OutputNetworkPictures:\n",
    "    outputpicture1 = APPLDIR +'/Outputs/Model_' +RunName + '1.png'\n",
    "    outputpicture2 = APPLDIR +'/Outputs/Model_' +RunName + '2.png'\n",
    "    tf.keras.utils.plot_model(myTFTcustommodel.build_graph([Tseq,NpropperseqTOT]), \n",
    "                        show_shapes=True, to_file = outputpicture1,\n",
    "                        show_dtype=True, \n",
    "                        expand_nested=True)\n",
    "    tf.keras.utils.plot_model(myTFTcustommodel.myTFTFullNetwork.build_graph([Tseq,NpropperseqTOT]), \n",
    "                        show_shapes=True, to_file = outputpicture2,\n",
    "                        show_dtype=True, \n",
    "                        expand_nested=True)\n",
    "  if myTFTTools.TFTSymbolicWindows:\n",
    "    finalizeTFTDL(myTFTcustommodel,recordtrainloss,recordvalloss,AnyOldValidation,TFTtest_datacollection,2, LabelFit = 'Custom TFT Fit')\n",
    "  else:\n",
    "    finalizeTFTDL(myTFTcustommodel,recordtrainloss,recordvalloss,AnyOldValidation,TFTtest_datacollection,2, LabelFit = 'Custom TFT Fit')\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdakV_4cz3Ck",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Run TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dD_cDFla_yV2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run TFT Only\n",
    "AnalysisOnly = myTFTTools.AnalysisOnly\n",
    "Dumpoutkeyplotsaspics = True\n",
    "Restorefromcheckpoint = myTFTTools.Restorefromcheckpoint\n",
    "Checkpointfinalstate = True\n",
    "if AnalysisOnly:\n",
    "  Restorefromcheckpoint = True\n",
    "  Checkpointfinalstate = False\n",
    "if Restorefromcheckpoint:\n",
    "  inputCHECKPOINTDIR = CHECKPOINTDIR\n",
    "  inputRunName = myTFTTools.inputRunName\n",
    "  inputCheckpointpostfix = myTFTTools.inputCheckpointpostfix\n",
    "  inputCHECKPOINTDIR = APPLDIR + \"/checkpoints/\" + inputRunName + \"dir/\"\n",
    "\n",
    "batchperepoch = False # if True output a batch bar for each epoch\n",
    "GlobalSpacetime = False\n",
    "IncreaseNloc_sample = 1\n",
    "DecreaseNloc_sample = 1\n",
    "SkipDL2F = True\n",
    "FullSetValidation = False\n",
    "  \n",
    "TFTTrainingMonitor = TensorFlowTrainingMonitor()\n",
    "if Hydrology:\n",
    "  TFTTrainingMonitor.SetControlParms(SuccessLimit = 1,FailureLimit = 2)\n",
    "if Earthquake:\n",
    "  TFTTrainingMonitor.SetControlParms(SuccessLimit = 1,FailureLimit = 2)\n",
    "if ReadJan2021Covid or ReadApril2021Covid:\n",
    "  TFTTrainingMonitor.SetControlParms(SuccessLimit = 1,FailureLimit = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZIby3dM_yV5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def PrintTFTBasicStuff():\n",
    "  myTFTTools.PrintTitle('Start TFT Deep Learning')\n",
    "  if myTFTTools.TFTSymbolicWindows:\n",
    "    print(startbold  + startred + 'Symbolic Windows used to save space'+resetfonts)\n",
    "  else:\n",
    "    print(startbold  + startred + 'Symbolic Windows NOT used'+resetfonts)\n",
    "  print('Training Locations ' + str(TrainingNloc) + ' Validation Locations ' + str(ValidationNloc) +\n",
    "        ' Sequences ' + str(Num_Seq))\n",
    "  if LocationBasedValidation:\n",
    "    print(startbold  + startred + \" Location Based Validation with fraction \" + str(LocationValidationFraction)+resetfonts)\n",
    "    if RestartLocationBasedValidation:\n",
    "      print(startbold  + startred + \" Using Validation set saved in \" + RestartValidationSetRunName+resetfonts)\n",
    "  print('\\nAre futures predicted ' + str(UseFutures) + ' Custom Loss Pointer ' + str(CustomLoss) + ' Class weights used ' + str(UseClassweights))\n",
    "  \n",
    "  print('\\nProperties per sequence ' + str(NpropperseqTOT))\n",
    "  print('\\n' + startbold +startpurple + 'Properties ' + resetfonts)\n",
    "  labelline = 'Name   '\n",
    "  for propval in range (0,7):\n",
    "    labelline += QuantityStatisticsNames[propval] + '    '\n",
    "  print('\\n' + startbold + labelline + resetfonts)\n",
    "  for iprop in range(0,NpropperseqTOT):\n",
    "    line = startbold + startpurple + str(iprop) + ' ' + InputPropertyNames[PropertyNameIndex[iprop]] + resetfonts  \n",
    "    jprop = PropertyAverageValuesPointer[iprop]\n",
    "    line += ' Root ' + str(QuantityTakeroot[jprop])\n",
    "    for proppredval in range (0,7):\n",
    "      line += ' ' + str(round(QuantityStatistics[jprop,proppredval],3))\n",
    "    print(line)\n",
    "\n",
    "  print('\\nPredictions per sequence ' + str(NpredperseqTOT))\n",
    "  print('\\n' + startbold +startpurple + 'Predictions ' + resetfonts)\n",
    "  print('\\n' + startbold + labelline + resetfonts)\n",
    "  for ipred in range(0,NpredperseqTOT):\n",
    "    line = startbold + startpurple + str(ipred) + ' ' + Predictionname[PredictionNameIndex[ipred]] + ' wgt ' + str(round(Predictionwgt[ipred],3)) + resetfonts + ' '\n",
    "    jpred = PredictionAverageValuesPointer[ipred]\n",
    "    line += ' Root ' + str(QuantityTakeroot[jpred])\n",
    "    for proppredval in range (0,7):\n",
    "      line += ' ' + str(round(QuantityStatistics[jpred,proppredval],3))\n",
    "    print(line)\n",
    "  print('\\n')\n",
    "\n",
    "  myTFTTools.PrintTitle('Start TFT Deep Learning')\n",
    "  for k in TFTparams:\n",
    "    print('# {} = {}'.format(k, TFTparams[k]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeEXX-lSuuhq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "runtype = ''\n",
    "if Restorefromcheckpoint:\n",
    "  runtype = 'Restarted '\n",
    "myTFTTools.PrintTitle(runtype)\n",
    "PrintTFTBasicStuff()\n",
    "\n",
    "RunTFTCustomVersion()\n",
    "myTFTTools.PrintTitle('TFT run completed')\n",
    "sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7sWeoGSNREO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#End modified TFT"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6CdCNdQ_yGWV"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}